Analysis and Optimization of
Convolutional Neural Networ k
Architectures
Master Thesis of
Mar tin Thoma
Depar tment of Computer Science
Institute for Anthropomatics
and
FZI Research Center for Infor mation Technology
Reviewer : Prof. Dr.ŒIng. R. Dillmann
Second reviewer : Prof. Dr.ŒIng. J. M. Zöllner
Advisor : Dipl.ŒInfor m. Michael Weber
Research Per iod: 03. May 2017 Œ 03. August 2017
KIT Œ University of the State of Baden-Wuer ttemberg and National Research Center of the Helmholtz Association
www.kit.edu
arXiv:1707.09725v1  [cs.CV]  31 Jul 2017
Analysis and Optimization of Convolutional Neural
Network Architectures
by
Mar tin Thoma
Master Thesis
August 2017
Master Thesis, FZI
Depar tment of Computer Science, 2017
Gutachter : Prof. Dr.ŒIng. R. Dillmann, Prof. Dr.ŒIng. J. M. Zöllner
Abteilung Technisch Kognitive Assistenzsysteme
FZI Research Center for Infor mation Technology
Afrmation
Ich versichere wahrheitsgemäÿ, die Arb eit selbstständig angefertigt, alle b enutzten Hilfs-
mittel vollständig und genau angegeb en und alles kenntlich gemacht zu hab en, was aus
Arb eiten anderer unverändert o der mit Abänderungen entnommen wurde.
Karlsruhe,
Martin Thoma
August 2017
v
Abstract
Convolutional Neural Networks (CNNs) dominate various computer vision tasks since
Alex Krizhevsky showed that they can b e trained e˙ectively and reduced the top-5 error
from
26
:
2 %
to
15
:
3 %
on the ImageNet large scale visual recognition challenge. Many
asp ects of CNNs are examined in various publications, but literature ab out the analysis
and construction of neural network architectures is rare. This work is one step to close this
gap. A comprehensive overview over existing techniques for CNN analysis and top ology
construction is provided. A novel way to visualize classi˝cation errors with confusion
matrices was develop ed. Based on this metho d, hierarchical classi˝ers are describ ed and
evaluated. Additionally, some results are con˝rmed and quanti˝ed for CIFAR-100. For
example, the p ositive impact of smaller batch sizes, averaging ensembles, data augmentation
and test-time transformations on the accuracy. Other results, such as the p ositive impact of
learned color transformation on the test accuracy could not b e con˝rmed. A mo del which
has only one million learned parameters for an input size of
32

32

3
and 100 classes and
which b eats the state of the art on the b enchmark dataset Asirra, GTSR B, HASYv2 and
STL-10 was develop ed.
vii
Zusammenfassung
Mo delle welche auf Convolutional Neural Networks (CNNs) basieren sind in verschiedenen
Aufgab en der Computer Vision dominant seit Alex Krizhevsky gezeigt hat dass diese
e˙ektiv trainiert werden können und er den Top-5 Fehler in dem ImageNet large scale visual
recognition challenge Benchmark von
26
:
2 %
auf
15
:
3 %
drücken konnte. Viele Asp ekte
von CNNs wurden in verschiedenen Publikationen untersucht, ab er es wurden vergleich-
sweise wenige Arb eiten üb er die Analyse und die Konstruktion von Neuronalen Netzen
geschrieb en. Diese Masterarb eit stellt einen Schritt dar um diese Lücke zu schlieÿen. Eine
umfassende Üb erblick üb er Analyseverfahren und Top ologielernverfahren wird gegeb en. Ein
neues Verfahren zur Visualisierung der Klassi˝kationsfehler mit Konfusionsmatrizen wurde
entwickelt. Basierend auf diesem Verfahren wurden hierarchische Klassi˝zierer eingeführt
und evaluiert. Zusätzlich wurden einige b ereits in der Literatur b eschrieb ene Beobachtun-
gen wie z.B. der p ositive Ein˛uss von kleinen Batch-Gröÿen, Ensembles, Erhöhung der
Trainingsdatenmenge durch künstliche Transformationen (Data Augmentation) und die In-
varianzbildung durch künstliche Transformationen zur Test-Zeit (Test-time transformations)
exp erimentell b estätigt. Andere Beobachtungen, wie b eispielsweise der p ositive Ein˛uss
gelernter Farbraumtransformationen konnten nicht b estätigt werden. Ein Mo dell welches
weniger als eine Millionen Parameter nutzt und auf den Benchmark-Datensätzen Asirra,
GTSRB, HASYv2 und STL-10 den Stand der Technik neu de˝niert wurde entwickelt.
Acknowledgment
I would like to thank Stephan Go cht and Marvin Teichmann for the many inspiring
conversations we had ab out various topics, including machine learning.
I also want to thank my father for the supp ort he gave me. He made it p ossible for me to
study without having to worry ab out anything b esides my studies. Thank you!
Finally, I want to thank Timothy Gebhard, Daniel Schütz and Yang Zhang for pro of-reading
my masters thesis and Stephan Go cht for giving me access to a GTX 1070.
ix
This work can b e cited the following way:
@MastersThe sis{Thoma:2017,
Title = {Analysis and Optimization of Convolutional Neura l Netwo rk
Architectur es},
Author = {Martin Thoma},
School = {Karlsruhe Instit ute of Techno logy},
Year = { 2017},
Address = {Kar lsruhe, Germany},
Month = jun,
Type = { Masters's Thesis},
Keywords = {machine learning; artificial neural networks ;
classificat ion; su pervised learning; CNNs},
Url = {https:/ /martin-thoma.com/msth esis/}
}
A DVD with a digital version of this master thesis and the source co de as well as the used
data is part of this work.
Contents
1 Introduction 1
2 Convolutional Neural Networks 3
2.1 Linear Image Filters
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2 CNN Layer Types
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2.1 Convolutional Layers
. . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2.2 Pooling Layers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2.3 Dropout
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2.4 Nor malization Layers
. . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.3 CNN Blocks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3.1 Residual Blocks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3.2 Aggregation Blocks
. . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.3.3 Dense Blocks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.4 Transition Layers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.5 Analysis Techniques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.5.1 Qualitative Analysis by Example
. . . . . . . . . . . . . . . . . . . .
15
2.5.2 Confusion Matr ices
. . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.5.3 Validation Cur ves: Accuracy, loss and other metr ics
. . . . . . . . .
16
2.5.4 Lear ning Cur ves
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.5.5 Input-feature based model explanations
. . . . . . . . . . . . . . . .
21
2.5.6 Argmax Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.5.7 Feature Map Reconstr uctions
. . . . . . . . . . . . . . . . . . . . . .
22
2.5.8 Filter compar ison
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.5.9 Weight update tracking
. . . . . . . . . . . . . . . . . . . . . . . . .
23
2.6 Accuracy boosting techniques
. . . . . . . . . . . . . . . . . . . . . . . . . .
24
3 Topology Learning 27
3.1 Growing approaches
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1.1 Cascade-Correlation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1.2 Meiosis Networ ks
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.1.3 Automatic Str ucture Optimization
. . . . . . . . . . . . . . . . . . . .
29
3.2 Pr uning approaches
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.3 Genetic approaches
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.4 Reinforcement Lear ning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
xi
3.5 Convolutional Neural Fabr ics
. . . . . . . . . . . . . . . . . . . . . . . . . .
31
4 Hierarchical Classication 33
4.1 Advantages of classier hierarchies
. . . . . . . . . . . . . . . . . . . . . .
34
4.2 Cluster ing classes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
5 Experimental Evaluation 37
5.1 Baseline Model and Training setup
. . . . . . . . . . . . . . . . . . . . . . .
38
5.1.1 Baseline Evaluation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
40
5.1.2 Weight distr ibution
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
5.1.3 Training behavior
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
5.2 Confusion Matr ix Order ing
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
5.3 Spectral Cluster ing vs CMO
. . . . . . . . . . . . . . . . . . . . . . . . . . .
51
5.4 Hierarchy of Classiers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
5.5 Increased width for faster lear ning
. . . . . . . . . . . . . . . . . . . . . . .
54
5.6 Weight updates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
5.7 Multiple narrow layers vs One wide layer
. . . . . . . . . . . . . . . . . . . .
56
5.8 Batch Nor malization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.9 Batch size
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
5.10 Bias
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
5.11 Lear ned Color Space Transfor mation
. . . . . . . . . . . . . . . . . . . . . .
60
5.12 Pooling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
5.13 Activation Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
5.14 Label smoothing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
5.15 Optimized Classier
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
5.16 Ear ly Stopping vs More Data
. . . . . . . . . . . . . . . . . . . . . . . . . .
68
5.17 Regular ization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
6 Conclusion and Outlook 71
A Figures, Tables and Algorithms 75
B Hyperparameters 79
B.1 Preprocessing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
B.2 Data augmentation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
B.3 Initialization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
B.4 Objective function
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
B.5 Optimization Techniques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
B.6 Networ k Design
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
B.7 Regular ization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
C Calculating Network Characteristics 87
C.1 Parameter Numbers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
C.2 FLOPs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
C.3 Memor y Footpr int
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
D Common Architectures 89
D.1 LeNet-5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
D.2 AlexNet
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
D.3 VGG-16 D
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
D.4 GoogleNet, Inception v2 and v3
. . . . . . . . . . . . . . . . . . . . . . . . .
94
D.5 Inception-v4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
E Datasets 97
F List of Tables 99
G List of Figures 101
H Bibliography 103
I Glossar y 119
1. Introduction
Computer vision is the academic ˝eld which aims to gain a high-level understanding of the
low-level information given by raw pixels from digital images.
Rob ots, search engines, self-driving cars, surveillance agencies and many others have
applications which include one of the following six problems in computer vision as sub-
problems:
‹
Classi˝cation
:
1
The algorithm is given an image and
k
p ossible classes. The task is
to decide which of the
k
classes the image b elongs to. For example, an image from
a self-driving cars on-b oard camera contains either
paved road
,
unpaved road
or
no road
: Which of those given three classes is in the image?
‹
Lo calization
: The algorithm is given an image and one class
k
. The task is to ˝nd
b ounding b oxes for all instances of
k
.
‹
Detection
: Given an image and
k
classes, ˝nd b ounding b oxes for all instances of
those classes.
‹
Semantic Segmentation
: Given an image and
k
classes, classify each pixel.
‹
Instance segmentation
: Given an image and
k
classes, classify each pixel as one of
the
k
classes, but distinguish di˙erent instances of the classes.
‹
Content-based Image Retrieval
: Given an image
x
and
n
images in a database,
˝nd the top
u
images which are most similar to
x
.
There are many techniques to approach those problems, but since AlexNet [
KSH12
] was
published, all of those problems have high-quality solutions which make use of Convolutional
Neural Networks (CNNs) [HZRS15a, LAE
+
16, RF B15, DHS16, SKP15].
To day, most neural networks are constructed by rules of thumb and gut feeling. The
architectures evolved and got deep er, more hyp erparameters were added. Although there
are metho ds for analyzing CNNs, those metho ds are not enough to determine all steps in
the development of network architectures without gut feeling. A detailed intro duction to
CNNs as well as nine metho ds for analysis of CNNs is given in Chapter 2.
1
Classi˝cation is also called
identi˝cation
if the classes are humans. Another name is
object recognition
,
although the classes can b e humans and animals as well.
1
1. Intro duction
Despite the fact that most researchers and develop ers do not use top ology learning, a couple
of algorithms have b een prop osed for this task. Five classes of top ology learning algorithms
are intro duced in Chapter 3.
When datasets and the numb er of classes are large, evaluating a single idea how to improve
the network can take several weeks just for the training. Hence the idea of building a
hierarchy of classi˝ers which allows to split the classi˝ cation task into various sub-tasks
that can easily b e combined is evaluated in Chapter 4.
Confusion Matrix Ordering (CMO), the hierarchical classi˝er, 9 typ es of hyp erparameters
and lab el smo othing are evaluated in Chapter 5.
This work fo cuses on classi˝cation problems to keep the presented ideas as pure and
simple as p ossible. The describ ed techniques are relevant to all six describ ed computer
vision problems due to the fact that Enco der-Deco der architectures are one comp onent of
state-of-the-art algorithms for all six of them.
2
2. Convolutional Neural Networks
In the following, it is assumed that the reader knows what a multilayer p erceptron (MLP)
is and how they are designed for classi˝cation problems, what activation functions are and
how gradient descent works. In case the reader needs a refresher on any of those topics, I
recommend chapter 4.3 and 4.4 of [Tho14a] as well as [LBH15].
This chapter intro duces linear image ˝ lters in Section 2.1, then standard layer typ es of
CNNs are explained in Section 2.2. The layer blo ck pattern is describ ed in Section 2.3,
transition layers in Section 2.4 and nine ways to analyze CNNs are describ ed in Section 2.5.
2.1. Linear Imag e Filters
A
linear image ˝lter
(also called a
˝lter bank
or a
kernel
) is an element
F
2
R
k
w

k
h

d
,
where
k
w
represents the ˝lter's width,
k
h
the ˝lter's height and
d
the numb er of input
channels. The ˝lter
F
is convolved with the image
I
2
R
w

h

d
to pro duce a new image
I
0
.
The output image
I
0
has only one channel. Each pixel
I
0
(
x; y
)
of the output image gets
calculated by p oint-wise multiplication of one ˝ lter element with one element of the original
image
I
:
I
0
(
x; y
) =
b
k
w
2
c
X
i
x
=1

k
w
2
e
b
k
h
2
c
X
i
y
=1

k
h
2
e
d
X
i
c
=1
I
(
x
+
i
x
; y
+
i
y
; i
c
)

F
(
i
x
; i
y
; i
c
)
This pro cedure is explained by Figure 2.1. It is essentially a discrete convolution.
I
2
R
7

7
Filter kernel
F
2
R
3

3
Result of p oint-wise
multipli cation
I
0
2
R
7

7
104
116
116
112
58
47
47
109
97
114
116
105
110
45
116
104
111
109
97
46
100
101
47
109
97
115
116
101
114
47
99
97
116
99
97
116
99
97
116
46
112
104
112
63
118
61
49
46
48
9
-3
-1
-6
5
3
2
-8
0
936
-333
-109
-282
545
291
94
-792
0
-4
-254
-498
-662
-849
-642
187
-520
45
240
211
388
215
-861
-340
559
-105
185
-138
-180
503
-718
429
350
173
251
268
-655
-567
-53
-75
80
571
-128
24
-408
596
-550
368
26
976
156
302
647
879
223
811
54
660
Figure 2.1.:
Visualization of the application of a linear
k

k

1
image ˝lter. For each pixel of the
output image,
k
2
multiplications and
k
2
additions of the pro d ucts have to b e calculated.
3
2. Convolutional Neural Networks
One imp ortant detail is how b oundaries are treated. There are four common ways of
b oundary treatment:
‹
don't compute
: The image
I
0
will b e smaller than the original image.
I
0
2
R
(
w

k
w
+1)

(
h

k
h
+1)

d
3
, to b e exact.
‹
zero padding
: The image
I
is padded by zeros where the ˝lter would access elements
which do not exist. This will result in edges b eing detected at the b order if the b order
pixels are not black, but do esn't need any computation.
‹
nearest
: Rep eat the pixel which is closest to the b oundary.
‹
re˛ect
: Re˛ect the image at the b oundaries.
Common tasks that can b e done with linear ˝lters include edge detection, corner detection,
smo othing, sharp ening, median ˝ltering, b ox ˝ltering. See Figure A.1 for ˝ve examples.
Please note that the result of a ˝ltering op eration is again an image. This means ˝lters
can b e applied successively. While each pixel after one ˝ltering op eration with a
3

3
˝lter got in˛uenced by
3

3 = 9
pixels of the original image, two successively applied
3

3
˝lters increase the area of the original image which in˛uenced the output. The output is
then in˛uenced by 25 pixel. This is called the
receptive ˝eld
. The kind of pattern which is
detected by a ˝lter is called a
feature
. The bigger the receptive ˝eld is, the more complex
can features get as they are able to consider more of the original image. Instead of taking
one
5

5
˝lter with 25 parameters, one might consider to take two successive
3

3
˝lters
with
2

(3

3) = 18
parameters. The
5

5
˝lter is a strict sup erset of p ossible ˝ltering
op erations compared to the two
3

3
˝lters, but the relevance of this technique will b ecome
clear in Section 2.2.
2.2. CNN Layer Types
While the idea b ehind deep MLPs is that
feature hierarchies
capture the imp ortant parts
of the input more easily, CNNs are inspired by the idea of
translational invariance
: Many
features in an image are translationally invariant. For example, if a car is develop ed, one
could try to detect it by its parts [
FGMR10
]. But then there are many p ositions at which
the wheels could b e. Combining those, it is desirable to capture low-level, translationally
invariant features at lower layers of an arti˝cial neural network (ANN) and in higher layers
high-level features which are combinations of the low-level features.
Also, mo dels should utilize the fact that the pixels of images are ordered. One way to use
this is by learning image ˝lters in so called
convolutional layers
.
While MLPs vectorize the input, the input of a layer in a CNN are
feature maps
. A feature
map is a matrix
m
2
R
w

h
, but typically the width equals the height (
w
=
h
). For an RGB
4
2.2. CNN Layer Typ es
input image, the numb er of feature maps is
d
= 3
. Each color channel is a feature map.
Since AlexNet [
KSH12
] almost halved the error in the ImageNet challenge, CNNs are
state-of-the-art in various computer vision tasks.
Traditional CNNs have three imp ortant building to ols:
‹
Convolutional layers with a non-linear activation function as describ ed in Section 2.2.1,
‹
p o oling layers as describ ed in Section 2.2.2 and
‹
normalization layers as describ ed in Section 2.2.4.
2.2.1. Convolutional Layers
Convolutional layers take several feature maps as input and pro duce
n
feature maps
1
as
output, where
n
is the numb er of ˝lters in the convolution layer. The ˝lter weights of
the linear convolutions are the parameters which are adapted to the training data. The
numb er
n
of ˝lters as well as the ˝lter's size
k
w

k
h
are hyp erparameters of convolutional
layers. Sometimes, it is denoted as
n
@
k
w

k
h
. Although the ˝lter depth is usually omitted
in the notation, the ˝lters are of dimension
k
w

k
h

d
(
i

1)
, where
d
(
i

1)
is the numb er of
feature maps of the input layer
(
i

1)
.
Another hyp erparameter of convolution layers is the stride
s
2
N

1
and the padding.
Padding (usually zero-padding [
SCL12
,
SEZ
+
13
,
HZRS15a
]) is used to make sure that the
size of the feature maps do esn't change.
The hyp erparameters of convolutional layers are
‹
the numb er of ˝lters
n
2
N

1
,
‹
k
w
; k
h
2
N

1
of the ˝lter size
k
w

k
h

d
(
i

1)
,
‹
the activation function of the layer (see Table B.3) and
‹
the stride
s
2
N

1
Typical choices are
n
2 f
32
;
64
;
128
g
,
k
w
=
k
h
=
k
2 f
1
;
3
;
5
;
11
g
such as in [
KSH12
,
SZ14, SLJ
+
15], recti˝ed linear unit (ReLU) activation and
s
= 1
.
The concept of weight sharing is crucial for CNNs. This concept was intro duced in [
WHH
+
89
].
With weight sharing, the ˝lters can b e learned with sto chastic gradient descent (SGD) just
like MLPs. In fact, every CNN has an equivalent MLP which computes the same function
if only the ˛attened output is compared.
1
also called
activation maps
or
channels
5
2. Convolutional Neural Networks
This is easier to see when the ˝ltering op eration is denoted formally:
o
(
i
)
(
x
) =
b
+
k
X
j
=1
w
ij

x
j
with
i
2 f
1
; : : : ; w
g  f
1
; : : : ; h
g  f
1
; : : : ; d
g
[2.1]
o
(
x;y ;z
)
(
I
) =
b
+
b
k
w
2
c
X
i
x
=1

k
w
2
e
b
k
h
2
c
X
i
y
=1

k
h
2
e
d
X
i
c
=1
F
z
(
i
x
; i
y
; i
c
)

I
(
x
+
i
x
; y
+
i
y
; i
c
)
[2.2]
with a bias
b
2
R
,
x
2 f
1
; : : : ; w
g
,
y
2 f
1
; : : : ; h
g
and
z
2 f
1
; : : : ; d
g
One can see that most weights of the equivalent MLP are zero and many weights are
equivalent. Hence the advantage of CNNs compared to MLPs is the reduction of parameters.
The e˙ect of fewer parameters is that less training data is necessary to get suitable
estimations for those. This means a MLP which is able to compute the same functions as a
CNN will likely have worse results on the same dataset, if a CNN architecture is suitable
for the dataset.
See Figure 2.2 for a visualization of the application of a convolutional layer.
3
feature maps
(e.g. RGB)
n
feature maps
n
˝lters of
size
k

k

3
width
w
width
w
height
h
height
h
neural
network
data
apply
. . .
. . .
. . .
. . .
. . .
. . .
Figure 2.2.:
Application of a single convolutional layer with
n
˝lters of size
k

k

3
with stride
s
= 1
to input data of size width

height with three ch an nels.
6
2.2. CNN Layer Typ es
A convolutional layer with
n
˝lters of size
k
w

k
h
and
SAME
padding after
d
(
i

1)
feature
maps of size
s
x

s
y
has
n

d
(
i

1)

(
k
w

k
h
)
parameters if no bias is used. In contrast, a fully
connected layer which pro duces the same output size and do es not use a bias would have
n

d
(
i

1)

(
s
x

s
y
)
2
parameters. This means a convolutional layer has drastically fewer
parameters. One the one hand, this means it can learn less complex decision b oundaries. On
the other hand, it means fewer parameters have to b e learned and hence the optimization
pro cedure needs fewer examples and the optimization ob jective is simpler.
It is particularly interesting to notice that even a convolutional layer of
1

1
˝lters do es
learn a linear combination of the
d
input feature maps. This can b e used for dimensionality
reduction, if there are fewer
1

1
˝lters in a convolutional layer than input feature maps.
Another insight recently got imp ortant: Every fully connected layer has an equivalent
convolutional layer which has the same weights.
2
This way, one can use the complete
classi˝cation network as a very complex non-linear image ˝lter which can b e used for
semantic segmentation.
A fully connected layer with
d
2
N

1
inputs and
n
2
N

1
no des can b e interpreted as a
convolutional layer with an input of shap e
1

1

d
and
n
˝lters of size
1

1
. This will
pro duce an output shap e
1

1

n
. Every single output is connected to all of the inputs.
When a convolutional layer is followed by a fully connected layer, it is necessary to vectorize
to feature maps. If the
1

1
convolutional ˝lter layer is applied to the vectorized output,
it is completely equivalent to a fully connected layer. However, the vectorization can b e
omitted if a convolution layer without padding and a ˝lter size equal to the feature maps
size is applied. This was used by [LSD15].
2.2.2. Pooling Layers
Po oling summarizes a
p

p
area of the input feature map. Just like convolutional layers,
p o oling can b e used with a stride of
s
2
N
>
1
. As
s

2
is the usual choice, p o oling layers
are sometimes also called
subsampling layers
. Typically,
p
2 f
2
;
3
;
4
;
5
g
and
s
= 2
such as
for AlexNet [KSH12] and VGG-16 [SZ14].
The typ e of summary for the set of activations
A
varies b etween the functions listed
in Table 2.1, spatial pyramid p o oling as intro duced in [
HZRS14
] and generalizing p o oling
functions as intro duced in [LGT16].
2
But convolutional layers only have equivalent fully connected layers if the output feature map is
1

1
7
2. Convolutional Neural Networks
Name De˝nition Used by
Max p o oling
max
f
a
2
A
g
[BPL10, KSH12]
Average / mean p o oling
1
j
A
j
P
a
2
A
a
LeNet-5 [LBBH98] and [KSlB
+
10]
`
2
p o oling
p
P
a
2
A
a
2
[Le13]
Sto chastic p o oling * [ZF13]
Table 2.1.: Po oling typ es for a set
A
of activations
a
2
R
.
(*) For sto chasti c p o oling, each of the
p

p
activation values
a
i
in the p o oling region gets
picked with probability
p
i
=
a
i
P
a
j
2
A
a
j
. This assumes the activations
a
i
are non-negative.
Po oling is applied for three reasons: To get lo cal translational invariance, to get invariance
against minor lo cal changes and, most imp ortant, for data reduction to
1
s
2
th of the data by
using strides of
s >
1
.
See Figure 2.3 for a visualization of max p o oling.
7
9
3
5
9
4
0
7
0
0
9
0
5
0
9
3
7
5
9
2
9
6
4
3
2

2
max p o oling
9
5
9
9
9
7
2
2
Figure 2.3.:
2

2
max p o oling applied to a feature map of size
6

4
with stride
s
= 2
and padding.
Average p o oling of
p

p
areas with stride
s
can b e replaced by a convolutional layer. If
the input of the p o oling layer are
d
(
i

1)
feature maps, the convolutional layer has to have
d
(
i

1)
˝lters of size
p

p
and stride
s
. The
i
th ˝lter has the values
0
B
B
@
1
p
2
: : :
1
p
2
:
:
:
:
:
:
:
:
:
1
p
2
: : :
1
p
2
1
C
C
A
for the dimension
i
and the zero matrix
0
B
B
@
0
: : :
0
:
:
:
:
:
:
:
:
:
0
: : :
0
1
C
C
A
for all other dimensions
i
= 1
; : : : ; d
(
i

1)
.
8
2.2. CNN Layer Typ es
2.2.3. Dropout
Drop out is a technique used to prevent over˝tting and co-adaptations of neurons by setting
the output of any neuron to zero with probability
p
. It was intro duced in [
HSK
+
12
] and is
well-describ ed in [SHK
+
14].
A Drop out layer can b e implemented as follows: For an input
in
of any shap e
s
, a tensor of
the same shap e
D
2 f
0
;
1
g
s
is sampled, where each element
d
i
is sampled indep endently
from a Bernoulli distribution. The results are element-wise multiplied to calculate the
output
out
of the Drop out layer:
out
=
D

in with
d
i
˘
B
(1
; p
)
where

is the Hadamard pro duct
(
A

B
)
i;j
:= (
A
)
i;j
(
B
)
i;j
Hence every value of the input gets set to zero with a drop out probability of
p
. Typically,
Drop out is used with
p
= 0
:
5
. Layers closer to the input usually have a lower drop out prob-
ability than later layers. In order to keep the exp ected output at the same value, the
output of a drop out layer is multiplied with
1
1

p
when drop out is enabled [
Las17
,
tf-16b
].
At inference time, drop out is disabled.
Drop out is usually only applied after fully connected layers, but not after convolutional
layers as it usually increases the test error as p ointed out in [GG16].
Mo dels which use Drop out can b e interpreted as an ensemble of mo dels with di˙erent
numb ers of neurons in each layer, but also with weight sharing.
Conceptually similar are DropConnect and networks with sto chastic depth. DropCon-
nect [
WZZ
+
13
] is a generalization of Drop out, which sets weights to zero in contrast to
setting the output of a neuron to zero. Networks with sto chastic depth as intro duced
in [
HSL
+
16
] drop out only complete layers. This can b e done by having Residual networks
which have one identity connection and one residual feature connection. Hence the residual
features can b e dropp ed out and the identity connection remains.
2.2.4. Normalization Layers
One problem when training deep neural networks is
internal covariate shift
: While the
parameters of layers close to the output are adapted to some input pro duced by lower layers,
those lower layers parameters are also adapted. This leads to the parameters in the upp er
layers b eing worse. A very low learning rate has to b e chosen to adjust for the fact that the
input features might drastically change over time.
9
2. Convolutional Neural Networks
One way to approach this problem is by normalizing mini-batches as describ ed in [
IS15
]. A
Batch Normalization layer with
d
-dimensional input
x
= (
x
(1)
; : : : ; x
(
d
)
)
is ˝rst normalized
p oint-wise to
^
x
(
k
)
=
x
(
k
)


x
(
k
)
p
s
0
[
x
(
k
)
]
2
+
"
with

x
(
k
)
=
1
m
P
m
i
=1
x
(
k
)
i
b eing the sample mean and
s
0
[
x
(
k
)
]
2
=
1
m
P
m
i
=1
(
x
(
k
)
i


x
(
k
)
)
the
sample variance where
m
2
N

1
is the numb er of training samples p er mini-batch,
" >
0
b eing a small constant to prevent division by zero and
x
(
k
)
i
is the activation of neuron
k
for
training sample
i
.
Additionally, for each activation
x
(
k
)
two parameters

(
k
)
; 
(
k
)
are intro duced which scale
and shift the feature:
y
(
k
)
=

(
k
)

^
x
(
k
)
+

(
k
)
In the case of fully connected layers, this is applied to the activation, b efore the non-linearity
is applied. If it is applied after the activation, it harms the training in early stages. For
convolution, only one

and one

is learned p er feature map.
One imp ortant sp ecial case is

(
k
)
=
p
s
0
[
x
(
k
)
]
2
+
"
and

(
k
)
=

x
(
k
)
, which would make the
Batch Normalization layer an identity layer.
During evaluation time,
3
the exp ected value and the variance are calculated once for the
complete dataset. An unbiased estimate of the empirical variance is used.
The question where Batch Normalization layers (BN) should b e applied and for which
reasons is still op en. For Drop out, it do esn't matter if it is applied b efore or after the
activation function. Considering this, the p ossible options for the order are:
1.
 CONV / FC
!
BN
!
activation function
!
Drop out
!
. . .
2.
 CONV / FC
!
activation function
!
BN
!
Drop out
!
. . .
3.
 CONV / FC
!
activation function
!
Drop out
!
BN
!
. . .
4.
 CONV / FC
!
Drop out
!
BN
!
activation function
!
. . .
The authors of [
IS15
] suggest to use Batch Normalization b efore the activation function
as in Items 1 and 4. Batch Normalization after the activation lead to b etter results in
https://git hub
:
com/ducha- aiki/caff enet- ben chmark/blob/master/ba tchnorm
:
md
Another normalization layer is Lo cal Resp onse Normalization as describ ed in [
KSH12
],
which includes
`
2
normalization as describ ed in [
WWQ13
]. Those two normalization layers,
however, are sup erseded by Batch Normalization.
3
also called
inference time
10
2.3. CNN Blo cks
2.3. CNN Blocks
This section describ es more complex building blo cks than simple layers. CNN blo cks act
similar to a layer, but they are themselves comp osed of layers.
2.3.1. Residual Blocks
Residual blo cks as intro duced in [
HZRS15a
] are a milestone in computer vision. They
enabled the computer vision community to go from ab out 16 layers as in VGG 16-D (see
App endix D.3) to several hundred layers. The key idea of deep residual networks (ResNets)
as intro duced in [
HZRS15a
] is to add an identity connection which skips two layers. This
identity connection adds the feature maps onto the other feature maps and thus requires
the output of the input layer of the residual blo ck to b e of the same dimension as last layer
of the residual blo ck.
Formally, it can b e describ ed as follows. If
x
i
are the feature maps after layer
i
and
x
0
is
the input image,
H
is a non-linear transformation of feature maps, then
y
=
H
(
x
)
describ es a traditional CNN. Note that this could b e multiple layers. A residual blo ck as
visualized in Figure 2.4 is describ ed by
y
=
H
(
x
) +
x
In [
HZRS15a
], they only used residual skip connections to skip two layers. Hence, if
conv
i
(
x
i
)
describ es the application of the convolutional layer
i
to the input
x
i
without the
nonlinearity, then such a residual blo ck is
x
i
+2
= conv
i
+1
(ReLU(conv
i
(
x
i
))) +
x
i
Figure 2.4.: ResNet mo dule
Image source: [HZRS15a]
[HM16] provides some insights why deep residual networks are successful.
11
2. Convolutional Neural Networks
2.3.2. Aggregation Blocks
Two common ways to add more parameters to neural networks are increasing their depth
by adding more layers or increasing their width by adding more neurons / ˝lters. Inception
blo cks [
AM15
] implicitly started a new idea which was explicitly describ ed in [
XGD
+
16
] as
ResNeXt blo ck: Increasing the cardinality
C
2
N

1
. By cardinality, the authors describ e
the concept of having
C
small convolutional networks with the same top ology but di˙erent
weights. This concept is visualized in Figure 2.5. Please note that Figure 2.5 do es not
combine aggregation blo cks with residual blo cks as the authors did.
256
-d in
concatenate
total
32
groups
. . .
128
-d out
4 @
1

1

256
4 @
3

3

4
4 @
1

1

256
4 @
3

3

4
4 @
1

1

256
4 @
3

3

4
Figure 2.5.:
Aggregation blo ck with a cardinality of
C
= 32
. Each of the 32 groups is a 2-layer
convolutional ne twork. The ˝rst layer receives 256 feature maps and applies four
1

1
˝lters to it. The second layer applies four
3

3
˝lters. Although every group has
the s ame top ology, the learned weights are di˙erent. The outputs of the groups are
concatenated.
The hyp erparameters of an aggregation blo ck are:
‹
The top ology of the group memb ers.
‹
The cardinality
C
2
N

1
. Note that a cardinality of
C
= 1
is equivalent in every
asp ect to using the group network without an aggregation blo ck.
12
2.3. CNN Blo cks
2.3.3. Dense Blocks
Dense blo cks are collections of convolutional layers which are intro duced in [
HLW16
]. The
idea is to connect each convolutional layer directly to subsequent convolutional layers.
Traditional CNNs with
L
layers and one input layer have
L
connections b etween layers,
but dense blo cks have
L
(
L
+1)
2
connections b etween layers. The input feature maps are
concatenated in depth. According to the authors, this prevents features from b eing re-
learned and allows much fewer ˝lters p er convolutional layer. Where AlexNet and VGG-16
have several hundred ˝lters p er convolutional layer (see Tables D.2 and D.3), the authors
used only on the order of 12 feature maps p er layer.
A dense blo ck is visualized in Figure 2.6.
256
-d in
k
@
3

3
concatenate
k
@
3

3
concatenate
256
-d
k
-d
(256 +
k
)
-d
k
-d
(256 +
L

k
)
-d out
Figure 2.6.: Dense blo ck with
L
= 2
layers and a growth factor of
k
.
Dense blo ck have ˝ve hyp erparameters:
‹
The activation function b eing used. The authors use ReLU.
‹
The size
k
w

k
h
of ˝lters. The authors use
k
w
=
k
h
= 3
.
‹
The numb er of layers
L
, where
L
= 2
is a simple convolutional layer.
‹
The numb er
k
of ˝lters added p er layer (called
growth rate
in the pap er)
It might b e necessary use
1

1
convolutions to reduce the numb er of
L

k
feature maps.
13
2. Convolutional Neural Networks
2.4. Transition Layers
Transition layers are used to overcome constraints imp osed by resource limitations or
architectural design choices. One constraint is the numb er of feature maps (see App endix C.3
for details). In order to reduce the numb er of feature maps while still keeping as much
relevant information as p ossible in the network, a convolutional layer
i
with
k
i
˝lters of
the shap e
1

1

k
i

1
is added. The numb er of ˝lters
k
i
directly controls the numb er of
generated feature maps.
In order to reduce the dimensionality (width and height) of the feature maps, one typically
applies p o oling.
Global p o oling is another typ e of transition layer. It applies p o oling over the complete
feature map size to shrink the input to a constant
1

1
feature map and hence allows one
network to have di˙erent input sizes.
14
2.5. Analysis Techniques
2.5. Analysis Techniques
CNNs have dozens of hyp erparameters and ways to tune them. Although there are
automatic metho ds like random search [
BB12
], grid search [
LBOM98
], gradient-based
hyp erparameter optimization [
MDA15
] and Hyp erband [
LJD
+
16
] some actions need a
manual investigation to improve the mo del's quality. For this reason, analysis techniques
which guide develop ers and researchers to the imp ortant hyp erparameters are necessary. In
the following, nine diagnostic techniques are explained.
A machine learning develop er has the following choices to improve the mo del's quality:
(I1)
 Change the problem de˝nition (e.g., the classes which are to b e distinguished)
(I2)
 Get more training data
(I3)
 Clean the training data
(I4)
 Change the prepro cessing (see App endix B.1)
(I5)
 Augment the training data set (see App endix B .2)
(I6)
 Change the training setup (see App endices B.3 to B.5)
(I7)
 Change the mo del (see App endices B.6 and B.7)
The prepro cessing is usually not changed in mo dern architectures. However, this still leaves
six very di˙erent ways to improve the classi˝er. Changing the training setup and the mo del
each have to o many p ossible choices to explore them completely. Thus, techniques are
necessary to guide the develop er to changes which are most promising to improve the mo del.
For all of the following metho ds, it is imp ortant to use only the training set and the
validation set.
2.5.1. Qualitative Analysis by Example
The most basic analysis technique which should always b e used is lo oking at examples
which the network correctly predicted with a high certainty and what the classi˝er got
wrong with a high certainty. Those examples can b e arranged by applying t-SNE [MH08].
One the one hand, this might reveal errors in the training data. Most of the time, training
data is manually lab eled by humans who make mistakes. If a mo del is ˝t to those errors,
its quality decreases.
On the other hand, this can show di˙erences in the distribution of validation data which
are not covered by the training set and thus indicate the need to collect more data.
15
2. Convolutional Neural Networks
2.5.2. Confusion Matrices
A
confusion matrix
is a matrix
(
c
)
ij
2
N
K

K

0
, where
K
2
N

2
is the numb er of classes,
which contains all correct and wrong classi˝cations. The item
c
ij
is the numb er of times
items of class
i
were classi˝ed as class
j
. This means the correct classi˝cation is on the
diagonal
c
ii
and all wrong classi˝cations are of the diagonal. The sum
P
K
i
=1
P
K
j
=1
c
ij
is the
total numb er of samples which were evaluated and
P
i
=1
c
ii
P
K
i
=1
P
K
j
=1
c
ij
is the accuracy.
The sums
r
(
i
) =
P
K
j
=1
c
ij
of each class
i
are worth b eing investigated as they show if the
classes are skewed. If the numb er of samples of one class dominates the data set, then the
classi˝er can get a high accuracy by simply always prediction the most common class. If
the accuracy of the classi˝er is close to the a priory probability of the most common class,
techniques to deal with skewed classes might help.
An automatic criterion to check for this problem is
accuracy

max(
f
r
(
i
)
j
i
= 1
; : : : ; k
g
)
P
k
i
=1
r
(
i
)
+
"
where
"
is a small value to comp ensate the fact that some examples might b e correct just
by chance.
Other values which should b e checked are the class-wise sensitivities:
s
(
k
) =
# correctly identi˝ed instances of class
k
# instances of class
k
=
c
k k
r
(
k
)
2
[0
;
1]
If
s
(
i
)
is much lower than
s
(
j
)
, it is an indicator that more or cleaner training data is
necessary for
s
(
i
)
.
The class-wise confusion
f
confusability
(
k
1
; k
2
) =
c
k
1
k
2
P
K
j
=1
c
k
1
j
indicates if class
k
1
gets often classi˝ed as class
k
2
. The highest values here can indicate
if two classes should b e merged or a sp ecialized mo del for separating those classes could
improve the overall system.
2.5.3. Validation Cur ves: Accuracy, loss and other metrics
Validation curves display a hyp erparameter (e.g., the training ep o ch) on the horizontal
axis and a quality metric on the vertical axis. Accuracy,
error
= (1

accuracy
)
or loss are
typical quality metrics. Other quality metrics can b e found in [OHIL16].
In case that the numb er of training ep o chs are used as the examined hyp erparameter,
validation curves give an indicator if training longer improves the mo del's p erformance. By
16
2.5. Analysis Techniques
plotting the error on the training set as well as the error on a validation set, one can also
estimate if over˝tting might b ecome a problem. See Figure 2.7 for an example.
10
20
30
40
50
60
70
80
90
100
0
:
2
0
:
4
0
:
6
0
:
8
o
v
e
r
f
i
t
t
i
n
g
Ep o chs
Error
Training set
Validation set
Figure 2.7.:
A typical validation curve: In this case, the hyp erparameter is the numb er of ep o chs
and the quality metric is the error
(1

accuracy
)
. The longer the ne twork is train ed,
the b etter it gets on the training set. At some p oint the network is ˝t to o well to the
training data and loses its capability to generalize. At this p oint the quality curve of
the training set and the validation set diverge. While the c las si˝er is s till imp roving on
the training set, it gets worse on the validation and the test set.
When the ep o ch-loss validation curve has plateaus as in Figure 2.8, this means the opti-
mization pro cess did not improve for several ep o chs. Three p ossible ways to reduce the
problem of plateaus are
 (i)
 to change weight initialization if the plateau was at the b eginning,
(ii)
 regularizing the mo del or
 (iii)
 changing the optimization algorithm.
Loss functions
The loss function (also called
error function
or
cost function
) is a function which assigns a
real value to a complex event like the predicted class of a feature vector. It is used to de˝ne
the
objective function
. For classi˝cation problems the loss function is typically cross-entropy
with
`
1
or
`
2
regularization, as it was describ ed in [NH92]:
E
C E
(
W
) =

X
x
2
X
K
X
k
=1
[
t
x
k
log (
o
x
k
) + (1

t
x
k
) log (1

o
x
k
)]
|
{z
}
cross-entropy data loss
+

1

`
1
z
}|
{
X
w
2
W
j
w
j
+

2

`
2
z
}|
{
X
w
2
W
w
2
|
{z
}
mo del complexity loss
where
W
are the weights,
X
is the training data set,
K
2
N

0
is the numb er of classes and
t
x
k
indicates if the training example
x
is of class
k
.
o
x
k
is the output of the classi˝cation
algorithm which dep ends on the weights.

1
; 
2
2
[0
;
1
)
weights the regularization and is
typically smaller than
0
:
1
.
17
2. Convolutional Neural Networks
Figure 2.8.:
Example for a validation cu rve (plotted loss function) with plateaus. The dark orange
curve is smo othed, but the non-s mo othed curve is also plotted in light orange.
The data loss is p ositive whenever the classi˝cation is not correct, whereas the mo del
complexity loss is higher for more complex mo dels. The mo del complexity loss exists due
to the intuition of
Occam's razor
: If two mo dels explain the same data with an accuracy of
100 %
, the simpler mo del is to b e preferred.
A reason to show the loss for the validation curve technique instead of other quality metrics
is that it contains more information ab out the quality of the mo del. A reason against the
loss is that it has no upp er b ound like the accuracy and can b e hard to interpret. The
loss only shows relative learning progress whereas the accuracy shows absolute progress to
human readers.
There are three observations in the loss validation curve which can help to improve the
network:
‹
If the loss do es not decrease for several ep o chs, the learning rate might b e to o low.
The optimization pro cess might also b e stuck in a lo cal minimum.
‹
Loss b eing NAN might b e due to to o high learning rates. Another reason is division
by zero or taking the logarithm of zero. In b oth cases, adding a small constant like
10

7
˝xes the problem.
‹
If the loss-ep o ch validation curve has a plateau at the b eginning, the weight initializa-
tion might b e bad.
18
2.5. Analysis Techniques
Quality criteria
There are several quality criteria for classi˝cation mo dels. Most quality criteria are based
the confusion matrix
c
which denotes at
c
ij
the numb er of times the real class was
i
and
j
was predicted. This means the diagonal contains the numb er of correct predictions. For
the following, let
t
i
=
P
k
j
=1
c
ij
b e the numb er of training samples for class
i
. The most
common quality criterion is accuracy:
accuracy
(
c
) =
P
k
i
=1
c
ii
P
k
i
=1
t
i
2
[0
;
1]
One problem of accuracy as a quality criterion are skewed classes. If one class is by far
more common than all other classes, then the simplest way to achieve a high score is to
always classify everything as the most common class.
In order to ˝x this problem, one can use the mean accuracy:
mean-accuracy
(
c
) =
1
k

k
X
i
=1
c
ii
t
i
2
[0
;
1]
For two-class problems there are many other metrics like precision, recall and
F

-score.
Quality criteria for semantic segmentation are explained in [Tho16].
Besides the quality of the classi˝cation result, several other quality criteria are imp ortant
in practice:
‹
Sp eed of evaluation for new images,
‹
latency,
‹
p ower consumption,
‹
robustness against (non)random p erturbations in the training data (see [
SZS
+
13
,
PMW
+
15]),
‹
robustness against (non)random p erturbations in the training lab els (see [
NDRT13
,
XXE12]),
‹
mo del size
As reducing the ˛oating p oint accuracy allows to pro cess more data on a given device [
Har15
],
analysis under this asp ect is also highly relevant in some scenarios.
However, the following fo cuses on the quality of the classi˝cation result.
19
2. Convolutional Neural Networks
2.5.4. Learning Cur ves
A learning curve is a plot where the horizontal axis displays the numb er of training samples
given to the network and the vertical axis displays the error. Two curves are plotted: The
error on the training set (of which the size is given by the horizontal axis) and the error on
the test set (which is of ˝xed size). See Figure 2.9 for an example. The learning curve for the
validation set is an indicator if more training data without any other changes will improve
the networks p erformance. Having the training set's learning curve, it is p ossible to estimate
if the capacity of the mo del to ˝t the data is high enough for the desired classi˝cation error.
The error on the validation set should never b e exp ected to b e signi˝cantly lower than the
error on the training set. If the error on the training set is to o high, then more data will
not
help. Instead, the mo del or the training algorithm need to b e adjusted.
If the training set's learning curve is signi˝cantly higher than the validation set's learning
curve, then removing features (e.g., by decreasing the images resolution), more training
samples or more regularization will help.
10
20
30
40
50
60
70
80
90
100
0
:
2
0
:
4
0
:
6
avoidable bias
variance
h
u
m
a
n
-
l
e
v
e
l
e
r
r
o
r
Training samples
Error
Validation set
Training set
Figure 2.9.:
A typical learning curve: The more data is used for training, the m ore errors a given
architecture will make to ˝t the given training data. At the same time, it is exp ected
that the training data gets more similar to the true distribution of the data which
should b e captured by the test data. At some p oint, the error on the training and
test set should b e ab out the same. The term avoidable bias was coined by Andrew
Ng [
Ng16
]. In some cases it is not p os sible to classify data correctly by the given
features. If humans can classify the data given the features correctly, however, then
the bias is avoidable by building a b etter classi˝er.
The ma jor drawback of this analysis technique is its computational intensity. In order to
get one p oint on the training curve and one p oint on the testing curve, a complete training
has to b e executed. On the full data set, this can b e several days on high-end computers.
20
2.5. Analysis Techniques
2.5.5. Input-feature based model explanations
Understanding which clues the mo del to ok to come to its prediction is crucial to check if
the mo del actually learns what the develop er thinks it learns. For example, a mo del which
has to distinguish sled dogs from Chihuahuas might simply lo ok at the background and
check if there is snow. Dep ending on the training and test data, this works exceptionally
well. However, it is not the desired solution.
For classi˝cation problems in computer vision, there are two typ es of visualizations which
help to diagnose such problems. Both color sup erpixels of the original image to convey
information how the mo del used those sup erpixels:
‹
Correct class heatmap
: The probability of the correct class is enco ded to give a
heat map which sup erpixels are imp ortant for the correct class. This can also b e done
by setting the opacity accordingly.
‹
Most-likely class image
: Each of the most likely classes for all sup erpixels is
represented by a color. The colored image thus gives clues why di˙erent predictions
were assigned a high probability.
Two metho ds to generate such images are explained in the following.
Occlusion Sensitivity Analysis
Occlusion sensitivity analysis is describ ed in [
ZF14
]. The idea is to o cclude a part of the
image by something. This could b e a gray square as in [
ZF14
] or a black sup erpixel as
in [
RSG16
]. Then the classi˝er is run on the image again. This is done for each region (e.g.,
sup erpixel or p osition of the square) and the regions are then colored to generate either a
correct class heatmap of the most-likely class image. It is imp ortant to note that the color
at region
r
i
denotes the result if
r
i
is o ccluded.
Both visualizations are shown in Figure 2.10. One can see that the network makes sensible
predictions for this image of the class Pomeranian. However, the image of the class Afghan
Hound gets confused with Ice lolly, which is a sign that this needs further investigation.
Gradient-based approaches
In [
SVZ13
], a gradient-based approach was used to generate
image-speci˝c class saliency
maps
. The authors describ e the problem as a ranking problem, where each pixel of the
image
I
0
is assigned a score
S
c
(
I
0
)
for a class
c
of interest. CNNs are non-linear functions,
but they can b e approximated by the ˝rst order Taylor expansion
S
c
(
I
)
ˇ
w
T
I
+
b
where
w
is the derivative of
S
c
at
I
0
.
21
2. Convolutional Neural Networks
2.5.6. Argmax Method
The
argmax method
has two variants:
‹
Fixed class argm ax
: Propagate all elements of a given class through the network
and analyze which neurons are activated most often / have the highest activation.
‹
Fixed neuron arg max
: Propagate the data through the network and ˝nd the
n
data elements which cause the highest activation for a given neuron.
Note that a neuron is a ˝lter in a CNN. The amount of activation of a ˝lter
F
by an
image
I
is calculated by applying
F
to
I
and calculating the element-wise sum of the result.
Fixed-neuron argmax was applied in [
ZF14
]. However, they did not stop with that. Besides
showing the 9 images which caused the highest activation, they also trained a deconvolutional
neural network to pro ject the activation of the ˝lter back into pixel space.
The ˝xed neuron argmax can b e used qualitatively to get an impression of the kind of
features which are learned. This is useful to diagnose problems, for example in [
AM15
] it is
describ ed that the network recognized the class dumbb ell only if a hand was present, to o.
Fixed neuron argmax can also b e used quantitatively to estimate the amount of parameters
b eing shared b etween classes or how many parameters are mainly assigned to which classes.
Going one step further from the ˝xed neuron argmax metho d is using an optimization
algorithm to change an initial image minimally in such a way that any desired class gets
predicted. This is called
caricaturization
in [MV16].
2.5.7. Feature Map Reconstructions
Feature map visualizations such as the ones made in [
ZF14
] (see Figure 2.11) give insights
into the learned features. This shows what the network emphasizes. However, it is not
necessarily the case that the feature maps allow direct and easy conclusions ab out the
learned features. This technique is called
inversion
in [MV16].
A key idea of feature map visualizations is to reconstruct a layers input, given its activation.
This makes it p ossible ˝nd which inputs would cause neurons to activate with extremely
high or low values.
More recent work like [
NYC16
] tries to make the reconstructions app earance lo ok more
natural.
22
2.5. Analysis Techniques
2.5.8. Filter comparison
One question which might lead to some insight is how robust the features are which
are learned. If the same network is trained with the same data, but di˙erent weight
initializations, the learned weights should still b e comparable.
If the set of learned ˝lters changes with initialization, this might b e an indicator for to o
little capacity of that layer. Hence adding more ˝lters to that layer could improve the
p erformance.
Filters can b e compared with the
k
-translation correlation as intro duced in [ZCZL16]:
ˆ
k
(
W
i
;
W
j
) = max
(
x;y
)

k ;:::;k
g
2
n
(0
;
0)
h
W
i
; T
(
W
j
; x; y
)
i
f
k
W
i
k
2
k
W
j
k
2
2
[

1
;
1]
;
where
T
(

; x; y
)
denotes the translation of the ˝rst op erand by
(
x; y
)
, with zero padding at
the b orders to keep the shap e.
h
;
i
f
denotes the ˛attened inner pro duct, where the two
op erands are ˛attened into column vectors b efore applying the standard inner pro duct. The
closer the absolute value of the
k
-translation correlation to one, the more similar two ˝lters
W
i
; W
j
are. According to [
ZCZL16
], standard CNNs like AlexNet (see App endix D.2) and
VGG-16 (see App endix D.3) have many ˝lters which are highly correlated. They found
this by comparing the
averaged maximum
k
-translational correlation
of the networks with
Gaussian-distributed initialized ˝lters. The averaged maximum
k
-translational correlation
is de˝ned as

ˆ
k
(
W
) =
1
N
N
X
i
=1
N
max
j
=1
;j
6
=
i
ˆ
k
(
W
i
;
W
j
)
where
N
is the numb er of ˝lters in the layer
W
and
W
i
denotes the
i
th ˝lter.
2.5.9. Weight update tracking
Andrej Karpathy prop osed in the 5th lecture of CS231n to track weight up dates to check if
the learning rate is well-chosen. He suggests that the weight up date should b e in the order
of
10

3
. If the weight up date is to o high, then the learning rate has to b e decreased. If the
weight up date is to o low, then the learning rate has to b e increased.
The order of the weight up dates as well as p ossible implications highly dep end on the mo del
and the training algorithm. See App endix B.5 for a short overview of training algorithms
for neural networks.
23
2. Convolutional Neural Networks
2.6. Accuracy boosting techniques
There are techniques which can almost always b e applied to improve accuracy of CNN
classi˝ers:
‹
Ensembles [CMS12]
‹
Training-time augmentation (see App endix B.2)
‹
Test-time transformations [DDF K16, How13, HZRS15b]
‹
Pre-training and ˝ne-tuning [ZDGD14, GDDM14]
One of the most simple ensemble techniques which was intro duced in [
CMS12
] is averaging
the prediction of
n
classi˝ers. This improves the accuracy even if the classi˝ers use exactly
the same training setup by reducing variance.
Data augmentation techniques give the optimizer the p ossibility to take invariances like
rotation into account by generating arti˝cial training samples from real training samples.
Data augmentation hence reduces bias and variance with no cost at inference time.
Data augmentation at inference time reduces the variance of the classi˝er. Similar to using
an ensemble, it increases the computational cost of inference.
Pretraining the classi˝er on another dataset to obtain start from a go o d p osition or ˝netuning
a mo del which was originally created for another task is also a common technique.
24
2.6. Accuracy b o osting techniques
Figure 2.10.:
Occlusion sensitivity analysis by [
ZF14
]: The left column shows three example images,
where a gray square o ccluded a part of the image. Th is gray squares center
(
x; y
)
was
moved over the complete image and the classi˝er was run on each of the o ccluded
images. The probability of the correct c lass, dep ending on th e gray squares p osition,
is showe d in the middle column. One can see that the predicted probability of the
correct class Pomeranian drops if th e face of the dog is o ccluded. The last image
gives the class with the highest predicted probability. In the case of the Pomeranian,
it always predicts the correct class if the head is visible. Howe ver, if the head of the
dog is o ccluded, it predicts other classes.
25
2. Convolutional Neural Networks
Figure 2.11.:
Filter visualization from [
ZF14
]: The ˝lters themselves as well as the input feature
maps which caused the highest activation are displayed.
26
3. Topology Learning
The top ology of a neural network is crucial for the numb er of parameters, the numb er
of ˛oating p oint op erations (FLOPs), the required memory, as well as the features b eing
learned. The choice of the top ology, however, is still mainly done by trial-and-error.
This chapter intro duces three general approaches to automatic top ology learning: Growing a
networks from a minimal network in Section 3.1, pruning in Section 3.2, genetic approaches
in Section 3.3 and reinforcement learning approaches in Section 3.4.
3.1. Growing approaches
Growing approaches for top ology learning start with a minimal network, which only has
the necessary numb er of input no des and the numb er of output no des which are determined
by the application and the features of the input. They then apply a criterion to insert new
layers / neurons into the network.
In the following, Cascade-Correlation, Meiosis Networks and Automatic Structure Opti-
mization are intro duced.
3.1.1. Cascade-Correlation
Cascade-Correlation was intro duced in [
FL89
]. It generates a cascading architecture which
is similar to dense blo ck describ ed in Section 2.3.3.
Cascade-Correlation works as follows:
1.
Initialization
: The numb er of input no des and the numb er of output no des are
de˝ned by the problem. Create a minimal, fully connected network for those.
2.
Training
: Train the network until the error no longer decreases.
3.
Candidate Generation
: Generate candidate no des. Each candidate no de is con-
nected to all inputs. They are not connected to other candidate no des and not
connected to the output no des.
27
3. Top ology Learning
4.
Correlation Maximization
: Train the weights of the candidates by maximizing
S
,
the correlation b etween candidates output value
V
with the networks residual error:
S
=
X
o
2
O






X
p
2
T

V
p


V

(
E
p;o


E
o
)






where
O
is the set of output no des,
T
is the training set,
V
p
is the candidate neurons
activation for a training pattern
p
.
E
p;o
is the residual output error at no de
o
for
pattern
p
.

V
and

E
o
are averaged values over all elements of
T
. This step is ˝nished
when the correlation no longer increases.
5.
Candidate sel ection
: Keep the candidate no de with the highest correlation, freeze
its incoming weights and add connections to the output no des.
6.
Continue
: If the error is higher than desired, continue with step 2.
One network with three hidden no des trained by Cascade-Correlation is shown in Figure 3.1.
1
Figure 3.1.:
A Cascade-Correlation network with three input no des (red) and one bias no de (gray)
to the left, three hidden no de s (green) in the middle and two output no des in the upp er
right corner. The black square s represent frozen weights which are found by correlation
maximization whereas the white squares are trainable weights.
3.1.2. Meiosis Networks
Meiosis Networks are intro duced in [
Han89
]. In contrast to most MLPs and CNNs, where
weights are deterministic and ˝xed at prediction time, each weight
w
ij
in Meiosis networks
follows a normal distribution:
w
ij
˘ N
(

ij
; ˙
2
ij
)
28
3.2. Pruning approaches
Hence every connection has two learned parameters:

ij
and
˙
2
ij
.
The key idea of Meiosis networks is to allow neurons to p erform Meiosis, which is cell
division. A no de
j
is splitted, when the random part dominates the value of the sampled
weights:
P
i
˙
ij
P
i

ij
>
1
and
P
k
˙
j k
P
k

j k
>
1
The mean of the new no des is sampled around the old mean, half the variance is assigned
to the new connections.
Hence Meiosis networks only change the numb er of neurons p er layer. They do not add
layers or add skip connections.
3.1.3. Automatic Structure Optimization
Automatic Structure Optimization (ASO) was intro duced in [
BM93
] for the task of on-
line handwriting recognition. It makes use of the confusion matrix
C
= (
c
ij
)
2
N
k

k

0
(see Section 2.5.2) to guide the top ology learning. They de˝ne a confusion-symmetry matrix
S
with
s
i
j
=
s
j
i
=
c
ij

c
j i
. The maximum of
S
de˝nes where the ASO algorithm adds
more parameters. The details how the resources are added are not transferable to CNNs.
3.2. Pruning approaches
Pruning approaches start with a network which is bigger than necessary and prune it. The
motivation to prune a network which has the desired accuracy is to save storage for easier
mo del sharing, memory for easier deployment and FLOPs to reduce inference time and
energy consumption. E sp ecially for emb edded systems, deployment is a challenge and low
energy consumption is imp ortant.
Pruning generally works as follows:
1.
 Train a given network until a reasonable solution is obtained,
2.
 prune weights according to a pruning criterion and
3.
 retrain the pruned network.
This pro cedure can b e rep eated.
One family of pruning criterions uses the
Hessian matrix
. For example, Optimal Brain
Damage (OBD) as intro duced in [
LDS
+
89
]. For every single parameter
k
, OBD calculates
the e˙ect on the ob jective function of deleting
k
. The authors call the e˙ect of the deletion
29
3. Top ology Learning
of parameter
k
the saliency
s
k
. The parameters with the lowest saliency are deleted, which
means they are set to 0 and are not up dated anymore.
A follow-up metho d called
Optimal Brain Surgeon
[
HSW93
] claims to cho ose the weights
in a much b etter way. This requires, however, to calculate the inverse Hessian matrix
H

1
2
R
n

n
where
n
2
N
is typically
n >
10
6
.
A much simpler and computationally cheap er pruning criterion is the
weight magnitude
.
[HPTD15] prunes all weights
w
which are b elow a threshold

:
w
 
8
<
:
w
if
w


0
otherwise
3.3. Genetic approaches
The general idea of genetic algorithms (GAs) is to enco de the solution space as genes, which
can recombine themselves via crossover and inversion. An intro duction to such algorithms
is given in [ES03].
Commonly used techniques to generate neural networks by GAs are NEAT [
SM02
] and its
successors Hyp erNEAT [SDG09] and ES-Hyp erNEAT [RL S10].
The results, however, are of unacceptable quality: On MNIST (see App endix E), where
random chance gives
10 %
accuracy, even simple top ologies trained with SGD achieve
ab out
92 %
accuracy [
TF-16a
] and state of the art is
99
:
79 %
[
WZZ
+
13
], the Hyp erNEAT
algorithm achieves only
23
:
9 %
accuracy [VH13].
Ko cmánek shows in [
Ko c15
] that Hyp erNE AT approaches can achieve
96
:
47 %
accuracy
on MNIST. Ko cmánek mentions that Hyp erNEAT b ecomes slower with each hidden layer
so that not more than three hidden layers could b e trained. At the same time, VGG-
19 [
SZ14
] already has 19 hidden layers and ResNets are successfully trained with 1202 layers
in [HZRS15a].
[
LX17
] shows that Genetic algorithms can achieve comp etitive results on MNIST and
SVHN, but the b est results on CIFAR-10 were
7
:
10 %
error whereas the state of the art is
at
3
:
74 %
[
HLW16
]. Similarly, the Genetic algorithm achieves
29
:
03 %
error on CIFAR-100,
but the state of the art is
17
:
18 %
[HLW16].
3.4. Reinforcement Learning
Reinforcement learning is a sub-˝eld of machine learning, which fo cuses on the question
how to cho ose actions that lead to high rewards.
30
3.5. Convolutional Neural Fabrics
One can think of the search for go o d neural network top ologies as a reinforcement learning
problem. The agent is a recurrent neural network which can generate bitstrings. Those
variable-length bitstrings enco de neural network top ologies.
In 2016, this approach was applied to construct neural networks for computer vision.
In [BGNR16], Q-learning with an
"
-greedy exploration was applied.
In [
ZL16
], the
REINFORCE
algorithm from [
Wil92
] was used to train state of the art mo dels
for CIFAR-10 and the Penn Treebank dataset. A drawback of this metho d is that enormous
amounts of computational resources were used to obtain those results.
3.5. Convolutional Neural Fabrics
Convolutional Neural Fabrics are intro duced in [
SV16
]. They side-step hard decisions
ab out top ologies by learning an ensemble of di˙erent CNN architectures. The idea is to
de˝ne a single architecture as a trellis through a 3D grid of no des. Each no de represents a
convolutional layer. One dimension is the index of the layer, the other two dimensions are
the amount of ˝lters and the feature size. Each no de is connected to nine other no des and
thus represents nine p ossible choices of convolutional layers:
‹
Resolution
:
 (i)
 convolution with
stride=1
or
 (ii)
 convolution with
stride=2
or
(iii)
 deconvolution (doubling the resolution)
‹
Channels
:
 (i)
 half the numb er of ˝lters than the layer b efore
 (ii)
 the same numb er
of ˝lters as the layer b efore
 (iii)
 double the numb er of ˝lters than the layer b efore
They always use ReLU as an activation function and they always use ˝lters of size
3

3
.
They don't use p o oling at all.
31
3. Top ology Learning
32
4. Hierarchical Classication
Designing a classi˝er for a new dataset is hard for two main reasons: Many design choices are
not clearly sup erior to others and evaluating one design choice takes much time. Esp ecially
CNNs are known to take several days [
KSH12
,
SLJ
+
15
] or even weeks [
SZ14
] to train.
Additionally, some metho ds for analyzing a dataset b ecome harder to use with more classes
and more training samples. Examples are t-SNE, the manual insp ection of errors and
confusion matrices, and the argmax metho d.
One idea to approach this problem is by building a hierarchy of classi˝ers. The ro ot
classi˝er distinguishes clusters of classes, whereas the leaf classi˝ers distinguish single
classes. Figure 4.1 gives an example for an hierarchy of classi˝ers.
Figure 4.1.:
Example for a hierarchy of classi˝ers. Each classi˝er is visualized by a rounded re ctangle.
The ro ot clas si˝er
C
0
has to di stinguish six coarse classes (p edestrian, four
+
-wheelers,
tra˚c signs, two-whee lers, street, other) or 17 ˝ne-grain ed classes. If
C
0
predicts a
pedestrian
, another classi˝er has to predict if it is an adult or a child. S imilar, if
C
0
predicts
traffic sign
, then another classi˝er has to predict if it is a sp eed limit, a
sign indicating danger or something else. If
C
0
, however, predicts
road
, then no oth er
classi˝er will b ecome active.
In this example , the problem has 17 classes. T he hierarchical approach intro duces
7 clusters of classes and thus uses 8 classi˝ers.
Such a hierarchy of classi˝ers needs clusters of classes.
33
4. Hierarchical Cl as si˝cation
4.1. Advantag es of classier hierarchies
Having a classi˝er hierarchy has ˝ve advantages:
‹
Division of lab or
: Di˙erent teams can work together. Instead of having a monolithic
task, the solutions can b e combined.
‹
Guarantees
: Changing a classi˝ er will only change the prediction of itself and its
children. Siblings are not a˙ected. In the example from Figure 4.1, the classi˝er
which distinguishes tra˚c signs can b e changed while the classi˝cation as
pedestrian
,
four
+
-wheelers
,
traffic sign
,
street
,
other
will not b e a˙ected. Also, the
classi˝cation b etween sp eed limits, danger signs and other signs will not change.
‹
Faster training
: Except for the ro ot classi˝er
C
0
, each other classi˝er will have
less than the total amount of training data. Dep ending on the combined classes, the
mo dels could also b e simpler. Hence the training time is reduced.
‹
Weighting of errors
: In practice, some errors are more severe than others. For
example, it could b e acceptable if the
two-wheelers
classi˝er has an error rate of
40 %
. But it is not acceptable if the
speed limit
classi˝er has such a high error rate.
‹
Post-ho c explanations
: The simpler a mo del is, the easier it is to explain why a
classi˝cation is made the way it is made.
4.2. Clustering classes
There are two ways to cluster classes: By similarity or by semantics. While semantic
clustering needs either additional information or manual work, the similarity can b e
automatically inferred from the data. As p ointed out in [
XZY
+
14
], semantically similar
classes are often also visually similar. For example, in the ImageNet dataset most dogs
are semantically and visually more similar to each other than to non-dogs. An example
where this is obviously not the case are symb ols: The summation symb ol
\sum
is identical
in app earance to the Greek letter
\Sigma
, but semantically much closer to the addition
op erator
+
.
One approach to cluster classes by similarity is to train a classi˝er and examine its
predictions. Each class is represented in the confusion matrix by one row. Those rows
can b e directly with standard clustering algorithms such as
k
-means, DBSCAN [
EKS
+
96
],
OPTICS [
ABKS99
], CLARANS [
NH02
], DIANA [
KR09
], AHC (see [
HPK11
]) or sp ectral
clustering as in [
XZY
+
14
]. Those clusterings, however, are hard to interpret and most of
them do not allow a human to improve the found clustering manually.
The confusion matrix
(
c
)
ij
2
N
k

k
states how often class
i
was present and class
j
was
34
4.2. Clustering classes
predicted. The more often this confusion happ ens, the more similar those two classes are to
the classi˝er. Based on the confusion matrix, the classes can b e clustered as explained in
the following.
[
HAE16
] indicates that more classes make it easier to generalize, but the accuracy gains
diminish after a critical p oint of classes is reached. Hence a binary tree might not b e a
go o d choice. As an alternative, an approach which allows building arbitrary many clusters,
is prop osed.
The prop osed algorithm has two main ideas:
‹
The order of columns and rows in the confusion matrix is arbitrary. This means one
can swap rows and columns. If row
i
and
j
are swapp ed, then the columns
i
and
j
have to b e swapp ed to in order to keep the same confusion matrix.
‹
If two classes are confused often, then they are similar to the classi˝er.
Hence the order of the classes is p ermutated in such a way that the highest errors are close
to the diagonal. One p ossible ob jective function to b e minimized is
f
(
C
) =
n
X
i
=1
n
X
j
=1
C
ij
 j
i

j
j
[4.1]
which punishes errors linearly with the distance to the diagonal. This metho d is called CMO
in the following.
As p ointed out by Tobias Ribizel (p ersonal communication), this optimization problem
is a weighted version of
Optimal Linear Arrangement problem
. That problem is NP-
complete [
GJ02
,
GJS76
]. Simulated Annealing as describ ed in Algorithm 1, however,
pro duces reasonable clusterings as well as visually app ealing confusion matrices. The
algorithm works as follows: First, decide with probability
0
:
5
if only two random rows are
swapp ed or a blo ck is swapp ed. If two rows are swapp ed, cho ose b oth of them randomly.
If a blo ck is swapp ed, then cho ose the start randomly and the end of the blo ck randomly
after the start. The insert p osition has to b e a valid p osition considering the blo ck length,
but b esides that it is also chosen uniformly random.
Simple row-swapping can exploit lo cal improvements. For example, in the context of
ImageNet, it can swap the dog-class
Silky Terrier
to the dog-class
Yorkshire terrier
and b oth dog classes
Dalmatian
and
Greyhound
next to each other. Both the two clusters
of dog breeds could b e separated by
car
and
bus
due to random chance. Moving any single
class increases the score, but moving either one of the dog breed clusters or the vehicle
cluster decreases the score. Hence it is b ene˝cial to implement blo ck moving.
One advantage of p ermutating the classes in order to minimize Equation (4.1) in comparison
to sp ectral clustering as used in [
XZY
+
14
] is that the adjusted confusion matrix can b e
35
4. Hierarchical Cl as si˝cation
split into many much smaller matrices along the diagonal. In the case of many classes (e.g.,
1000 classes of ImageNet or 369 classes of HASYv2) this p ermutation makes it p ossible to
visualize the typ es of errors made. If the errors are systematic due to visual similarity, many
confusions are not made and thus many elements of the confusion matrix are close to 0.
Those will b e moved to the corners of the confusion matrix by optimizing Equation (4.1).
Once a p ermutation of the classes is found which has a low score Equation (4.1), the clusters
can either b e made by hand by deciding why classes should not b e in one clusters. With
such a p ermutation, only
n

1
binary decisions have to b e made and hence only the list of
classes has to b e read. Alternatively, one can calculate the confusions
C
0
i;i
+1
+
C
0
i
+1
;i
for
each pair of classes which are neighb ors in the confusion matrix. The higher this value, the
more similar are the classes according to the classi˝er. Hence a threshold

can b e applied.

can either b e set automatically (e.g., such that
10 %
of all pairs are ab ove the threshold)
or semi-automatically by asking the user for information if two classes b elong to the same
cluster. Such an approach only needs
log
(
n
)
binary decisions from the user where
n
is the
numb er of classes.
Please note that CMO only works if the classi˝er is neither to o bad nor to o go o d. A classi˝er
which do es not solve the task at all might just give almost uniform predictions whereas the
confusion matrix of an extremely go o d classi˝er is almost diagonal and thus contains no
information ab out the similarity of classes. One p ossible solution to this problem is to take
the prediction of the class in contrast to using only the argmax in order to ˝nd a useful
p ermutation.
36
5. Experimental Evaluation
All exp eriments are implemented using Keras 2.0 [
Cho15
] with Tensor˛ow 1.0 [
AAB
+
16
]
and cuDNN 5.1 [
CWV
+
14
] as the backend. The exp eriments were run on di˙erent machines
with di˙erent Nvidia graphics pro cessing units (GPUs), including the Titan Black, GeForce
GTX 970 and GeForce 940MX.
The GTSRB [
SSSI12
], SVHN [
NWC
+
11b
], CIFAR-10 and CIFAR-100 [
Kri
], MNIST [
YL98
],
HASYv2 [
Tho17a
], STL-10 [
CLN10
] dataset are used for the evaluation. Those datasets are
used as their size is small enough to b e trained within a day. Other classi˝cation datasets
which were considered are listed in App endix E.
CIFAR-10
(Canadian Institute for Advanced Research 10) is a 10-class dataset of color
images of the size
32 px

32 px
. Its ten classes are airplane, automobile, bird, cat, deer,
dog, frog, horse, ship, truck. The state of the art achieves an accuracy of
96
:
54 %
[
HLW16
].
According to [Kar11], human accuracy is at ab out
94 %
.
CIFAR-100
is a 100-class dataset of color images of the size
32 px

32 px
. Its 100 classes
are group ed to 20 sup erclasses. It includes animals, p eople, plants, outdo or scenes, vehicles
and other items. CIFAR-100 is not a sup erset of CIFAR -10, as CIFAR-100 do es not contain
the class
airplane
. The state of the art achieves an accuracy of
82
:
82 %
[HLW16].
GTSRB
(German Tra˚c Sign Recognition Benchmark) is a 43-class dataset of tra˚c signs.
The
51 839
images are in color and of a minimum size of
25 px

25 px
up to
266 px

232 px
.
The state of the art achieves
99
:
46 %
accuracy with an ensemble of 25 CNNs [
SL11
].
According to [SSSI], human p erformance is at
98
:
84 %
.
HASYv2
(Handwritten Symb ols version 2) is a 369 class dataset of black-and-white images
of the size
32 px

32 px
. The 369 classes contain the Latin and Greek letters, arrows,
mathematical symb ols. The state of the art achieves an accuracy of
82
:
00 %
[Tho17a].
STL-10
(self-taught learning 10) is a 10-class dataset of color images of the size
96 px

96 px
.
Its ten classes are airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck. The state
of the art achieves an accuracy of
74
:
80 %
[
ZMGL15
]. It contains
100 000
unlab eled images
for unsup ervised training and
500
images p er class for sup ervised training.
SVHN
(Street View House Numb ers) exists in two formats. For the following exp eriments,
the cropp ed digit format was used. It contains the 10 digits cropp ed from photos of Go ogle
Street View. The images are in color and of size
32 px

32 px
. The state of the art
37
5. Exp erimental Evaluation
achieves an accuracy of
98
:
41 %
[
HLW16
]. According to [
NWC
+
11a
], human p erformance
is at
98
:
0 %
.
As a prepro cessing step, the pixel-features were divided by 255 to obtain values in
[0
;
1]
.
For GTSRB, the training and test data was scaled to
32 px

32 px
.
5.1. Baseline Model and Training setup
The baseline mo del is trained with Adam [
KB14
], an initial learning rate of
10

4
, a batch
size of 64 for at most 1000 ep o chs with data augmentation. The kind of data augmentation
dep ends on the dataset:
‹
CIFAR-10
,
CIFAR-100
and STL-10: Random width and height shift by at most

3
pixels in either direction; Random horizontal ˛ip.
‹
GTSRB
,
MNIST
: Random width and height shift by at most

5
pixels in either
direction; random rotation by at most

15
degrees; random channel shift; random
zo om in
[0
:
5
;
1
:
5]
; random shear by at most 6 degrees.
‹
HASYv2
: Random width and height shift by at most

5
pixels in either direction;
random rotation by at most

5
degree.
‹
SVHN
: No data augmentation.
If the dataset do es not de˝ne a training/test set, a strati˝ed
67 %
/
33 %
split is applied. If
the dataset do es not de˝ne a validation set, the training set is split in a strati˝ed manner
into
90 %
training set /
10 %
test set.
Early stopping [
Pre98
] with the validation accuracy as a stopping criterion and a patience of
10 ep o chs is applied. After this, the mo del is trained without data augmentation for at most
1000 ep o chs with early stopping and the validation accuracy as a stopping criterion and a
patience of 10 ep o chs. Kernel weights are initialized according to the uniform initialization
scheme of He [HZRS15b] (see App endix B.3).
The architecture of the baseline mo del uses a pattern of
Conv-Blo ck
(
n
) = (
Convolution

Batch Normalization

Activation
)
n

Po oling
The activation function is the Exp onential Linear Unit (E LU) (see Table B.3), except for
the last layer where softmax is used. Before the last two convolutional layer, a drop out
layer with drop out probability
0
:
5
is applied. The architecture is given in detail in Table 5.1.
Please note that the numb er of input- and output channels of the network dep ends on
the dataset. If the input image is larger than
32 px

32 px
, for each p ower of two a
Conv-Blo ck
(2)
is added at the input. For MNIST, the images are bilinearly upsampled to
32 px

32 px
.
38
5.1. Baseline Mo del and Training setup
# Typ e Filters @
Patch size / stride
Parameters FLOPs Output siz e
Input 0 0 3 @ 32

32
1 Convolution 32 @
3

3

3
/ 1 896 1 736 704
32
@
32

32
2 BN + ELU 64 163 904
32
@
32

32
3 Convolution 32 @
3

3

32
/ 1 9 248 18 841 600
32
@
32

32
4 BN + ELU 64 163 904
32
@
32

32
Max p o oling
2

2
/ 2 0 40 960 32 @ 16

16
5 Convolution 64 @
3

3

32
/ 1 18 496 9 420 800 64 @ 16

16
6 BN + ELU 128 82 048 64 @ 16

16
7 Convolution 64 @
3

3

64
/ 1 36 928
18 857 984
64 @ 16

16
8 BN + ELU 128 82 048 64 @ 16

16
Max p o oling
2

2
/ 2 20 480 64 @ 8

8
9 Convolution 64 @
3

3

64
/ 1 36 928 4 714 496 64 @ 8

8
10 BN + ELU 128 20 608 64 @ 8

8
Max p o oling
2

2
/ 2 5 120 64 @ 4

4
11 Convolution (v) 512 @
4

4

64
/ 1
524 800
1 048 064 512 @ 1

1
12 BN + ELU 1 024 3 584 512 @ 1

1
Drop out 0.5 0 0 512 @ 1

1
13 Convolution 512 @
1

1

512
/ 1 262 656 523 776 512 @ 1

1
14 BN + ELU 1 024 3 584 512 @ 1

1
Drop out 0.5 0 0 512 @ 1

1
15 Convolution k @
1

1

512
/ 1
k

(512 + 1) 1024

k
k @ 1

1
Global avg Po oling
1

1
0
k
k @ 1

1
16 BN + Softmax
2
k
7
k
k @ 1

1
P
515
k
+892 512
1032
k
+55 729 664
103 424+
2
k
Table 5.1.:
Baseline architecture with 3 input channels of size
32

32
. All convolutional layers
use
SAME
padding, except for layer 11 which use d
VALID
padding in order to decrease
the feature map size to
1

1
. If th e input feature map is bigger than
32

32
, for
each p ower of two there are two
Convolution + BN + ELU
blo cks and one
Max pooling
blo ck added. This is the framed part in the table.
32

32
Input
C
32@3

3
=
1
BN + ELU
C
32@3

3
=
1
BN + ELU
16

16
max p o oling
2

2
=
2
C
64@3

3
=
1
BN + ELU
C
64@3

3
=
1
BN + ELU
8

8
max p o oling
2

2
=
2
C
64@3

3
=
1
BN + ELU
4

4
max p o oling
2

2
=
2
C
512@4

4
=
1
(V)
BN + ELU
Drop out,
p
= 0
:
5
1

1
C
512@1

1
=
1
BN + ELU
Drop out,
p
= 0
:
5
C
k
@1

1
=
1
Global AVG p o oling
BN + Softmax
Figure 5.1.:
Architecture of the baseline mo del.
C
32@3

3
=
1
is a convolutional layer with 32 ˝lters
of kernel size
3

3
with stride 1.
39
5. Exp erimental Evaluation
5.1.1. Baseline Evaluation
The results for the baseline mo del evaluated on eight datasets are given in Table 5.2. The
sp eed for inference for di˙erent GPUs is given in Table 5.3.
Dataset
Single Mo del Accuracy Ensemble of 10
Training Set Test Set Training Set Test Set
Asirra
94
:
22 %
˙
= 3
:
49 94
:
37 %
˙
= 3
:
47 97
:
07 % 97
:
37 %
CIFAR-10
91
:
23 %
˙
= 1
:
10 85
:
84 %
˙
= 0
:
87 92
:
36 % 86
:
75 %
CIFAR-100
76
:
64 %
˙
= 1
:
48 63
:
38 %
˙
= 0
:
55 78
:
30 % 64
:
70 %
GTSRB
100
:
00 %
˙
= 0
:
00 99
:
18 %
˙
= 0
:
11 100
:
00 % 99
:
46 %
HASYv2
89
:
49 %
˙
= 0
:
42 85
:
35 %
˙
= 0
:
10 89
:
94 % 86
:
03 %
MNIST
99
:
93 %
˙
= 0
:
07 99
:
53 %
˙
= 0
:
06 99
:
99 % 99
:
58 %
STL-10
94
:
12 %
˙
= 0
:
87 75
:
67 %
˙
= 0
:
34 96
:
35 % 77
:
62 %
SVHN
99
:
02 %
˙
= 0
:
07 96
:
28 %
˙
= 0
:
10 99
:
42 % 97
:
20 %
Table 5.2.:
Baseline mo del acc uracy on eight datasets. The single mo del actuary is the 10 mo dels
used in the ensemble. The e mpirical standard deviation
˙
of the accuracy is also given.
CIFAR-10, CIFAR-100 and STL-10 mo dels use test-time transformations. None of the
mo dels uses unlab eled data or data from other datasets. For HASYv2 no test time
transformations are used.
Network GPU Te nsor˛ow
Inference p er Training
1 Image 128 images time / ep o ch
Baseline Default Intel i7-4930K
3 ms 244 ms 231
:
0 s
Baseline Optimized Intel i7-4930K
2 ms 143 ms 149
:
0 s
Baseline Default G eForce 940M X
4 ms 120 ms 145
:
6 s
Baseline Default G TX 970
6 ms 32 ms 25
:
0 s
-
26
:
3 s
Baseline Default G TX 980
3 ms 24 ms 20
:
5 s
-
21
:
1 s
Baseline Default G TX 980 Ti
5 ms 27 ms 22
:
0 s
-
22
:
1 s
Baseline Default G TX 1070
2 ms 15 ms 14
:
4 s
-
14
:
5 s
Baseline Default T itan Black
4 ms 2 5 ms 28
:
1 s
-
28
:
1 s
Baseline Optimized T itan Black
3 ms 22 ms 24
:
4 s
-
24
:
4 s
DenseNet-40-12 Default GeForce 940MX
27 ms 2403 ms

Table 5.3.:
Sp eed comparison of the baseline mo del on CIFAR-10. The base line mo del is evaluated on
six Nvidia GPUs and one CPU. The weights for DenseNet-40-12 are taken from [
Ma j17
].
Weights the baseline mo del can b e found at [
Tho17b
]. The optimized Tensor˛ow build
makes use of SSE4.X, AVX, AVX2 and FMA in structions.
40
5.1. Baseline Mo del and Training setup
5.1.2. Weight distribution
The distribution of ˝lter weights by layer is visualized in Figure 5.2 and the distribution
of bias weights by layer is shown in Figure 5.3. Although b oth ˝gures only show the
distribution for one sp eci˝c mo del trained on CIFAR-100, the following observed patterns
are consistent for 70 mo dels (7 datasets and 10 mo dels p er dataset):
‹
The empiric
[0
:
5

p ercentile
;
99
:
5

p ercentile
]
interval which contains
99 %
of the
˝lter weights is almost symmetric around zero. The same is true for the bias weights.
‹
The farther a layer is from the input away, the smaller the 99-p ercentile interval is,
except for the last layer (see Table A.1).
‹
The 99-p ercentile interval of the ˝rst layers ˝lter weights is ab out
[

0
:
5
;
+0
:
5]
, except
for MNIST and HASYv2 where it is in
[

0
:
8
;
0
:
8]
.
‹
The 99-p ercentile interval of the ˝rst layers bias weights is always in
[

0
:
2
;
0
:
2]
.
‹
The distribution of ˝lter weights of the last convolutional layer is not symmetric. In
some cases the distribution is also not unimo dal.
‹
The bias weights of the last three layers are very close to zero. The absolute value of
most of them is smaller than
10

2
.
Similarly, Figure 5.4 and Figure 5.5 show the distribution of the

and the

parameter of
Batch Normalization. It is exp ected that

is close to 1 and

is close to 0. In those cases,
the Batch Normalization layer equals the identity and thus is only relevant for the training.
While

and

do not show as clear patterns as the ˝lter and bias weights of convolutional
layers, some observations are also consistent through all mo dels even for di˙erent datasets:
‹

of the last layer (layer 16) is bigger than 1.3.
‹
The 99-p ercentile interval for

of the last layer is longer than the other 99-p ercentile
intervals.
‹
The 99-p ercentile interval for

of the fourth-last (layer 14 for STL-10, layer 10 for
all other mo dels) is more negative then all other layers.
Finally, the distribution of ˝lter weight ranges is plotted in Figure 5.6 for each convolutional
layer. The ranges are calculated for each channel and ˝lter separately. The smaller the
values are, the less information is lost if the ˝lters are replaced by smaller ˝lters.
41
5. Exp erimental Evaluation
Figure 5.2.:
Violin plots of the distribution of ˝lter weights of a baseline mo d el trained on CIFAR-
100. The weights of the ˝rst layer are relatively evenly spread in the interval
[

0
:
4
;
+0
:
4]
.
With every layer the interval w hich contains
95 %
of the weights and is centered around
the mean b ecomes smaller, esp ecially with layer 11 where the feature maps are of
size
1

1
. In contrast to the other layers, the last convolutional layer has a bimo dal
distribution.
This plot indicates that the network might b ene˝t from bigger ˝lters in the ˝rst layer,
whereas the ˝lters in layers 7  11 could p otentially b e smalle r.
Figure 5.3.:
Violin plots of the distribution of bias weights of a baseline mo del trained on CIFA R-100.
While the ˝rst laye rs biases are in
[

0
:
1
;
+0
:
1]
, after each max-p o oling layer the interval
which contains
95 %
of the weights and i s centered around the mean b ecomes smaller.
In the last three convolutional layer, most bias weights are in
[

0
:
005
;
+0
:
005]
.
42
5.1. Baseline Mo del and Training setup
Figure 5.4.:
Violin plots of the distribution of the

parameter of Batch Normalization layers of a
baseline mo del train ed on CIFAR-100.
Figure 5.5.:
The distribution of the

parameter of Batch Normalization layers of a baseline mo del
trained on CIFAR-100.
43
5. Exp erimental Evaluation
Figure 5.6.:
The distribution of the range of values (max - min) of ˝l te rs by channel and layer. For
each ˝lter, the range of values is recorded by channe l. The smaller this range is, the
less information is lost if a
n

n
˝lter is replaced by a
(
n

1)

(
n

1)
˝lter.
44
5.1. Baseline Mo del and Training setup
5.1.3. Training behavior
Due to early stopping, the numb er of ep o chs which a mo del was trained di˙er. The numb er
of ep o chs trained with augmentation ranged from 133 ep o chs to 182 ep o chs with a standard
deviation of 17.3 ep o chs for CIFAR-100.
Figure 5.7 shows the worst and the b est validation accuracy during the training with
augmented data. D i˙ erent initializations lead to very similar validation accuracies during
training. The image might lead to the wrong conclusion that mo dels which are b etter at
the start are also b etter at the end. In order to check this hyp othesis, the relative order of
validation accuracies for the 10 CIFAR-100 mo dels was examined. If the relative ordering
stays approximately the same, then it can b e considered to run the ˝rst few ep o chs many
times and only train the b est mo dels to the end. For 10 mo dels, there can b e
10
2

10
2
= 45
pair-wise changes in the ordering at maximum if the relative order of validation accuracies
is reversed. For the baseline mo del, 21.8 changes in the relative order of accuracies o ccurred
in average for each pair of ep o chs
(
i; i
+ 1)
. This means if one knows only the relative order
of the validation accuracy of two mo dels
m
and
m
0
in ep o ch
i
, it is doubtful if one can
make any statement ab out the ordering of
m
and
m
0
in ep o ch
i
+ 1
.
0
10
20
30
40
50
60
70
80
90
100
110
120
130
140
0
:
2
0
:
3
0
:
4
0
:
5
0
:
6
0
:
7
ep o ch
validation accuracy
maximum validation accuracy
minimum validation accuracy
1
:
5
2
2
:
5
3
3
:
5
4
4
:
5
loss
maximum validation accuracy
minimum validation accuracy
mean loss
Figure 5.7.:
Minimum and maximum validation accuracy of the 10 trained mo dels by ep o ch. The
di˙erences do not exceed
1 %
and do es not increase by training ep o ch. Four mo del s
stopp ed the ˝rst training stage at ep o ch 133 wh ich causes the shift in the loss and the
maximum validation accuracy.
Figures 5.8 to 5.10 show how the weights changed while training on CIFAR-100. It was
exp ected that the absolute value of weight up dates during ep o chs (sum, max, and mean)
decrease in later training stages. The intuition was that weights need to b e adjusted in a
coarse way ˝rst. After that, the intuition was that only slight mo di˝cations are applied by
45
5. Exp erimental Evaluation
the SGD based training algorithm (ADAM). The mean, max and sum of weight up dates as
displayed in Figures 5.8 to 5.10, however, do not show such a clear pattern. The biggest
change happ ens as exp ected in the ˝rst ep o ch after the weights are initialized. The change
from augmented training to non-augmented training was at ep o ch 156 to ep o ch 157
It can b e observed, that layers which receive more input feature maps get larger weight
up dates in mean. As layers which are closer to the output take more input feature maps,
their weight up dates are larger. This pattern do es not o ccur when SGD is used as the
optimizer.
Figure 5.8.: Mean weight up dates of the baseline mo del b etween ep o chs by layer.
46
5.1. Baseline Mo del and Training setup
Figure 5.9.: Maximum weight up dates of the baseline mo del b etween ep o chs by layer.
Figure 5.10.: Sum of weight up dates of the baseline mo del b etween ep o chs by layer.
47
5. Exp erimental Evaluation
5.2. Confusion Matrix Ordering
The visualization of the confusion matrix can give valuable information ab out which part
of the task is hard. For more than ab out 10 classes, however, it b ecomes hard to visualize
and read.
For CIFAR-10, the prop osed metho d groups the four ob ject classes and the six animal
classes together (see Figure 5.11a).
(a)
CIFAR-10 Test set
(b)
Random
Figure 5.11.:
Figure 5.11a shows an ordered confusion matrix of the CI FAR-10 dataset. T he diagonal
elements are set to 0 in order to make other elements easier to s ee.
Figure 5.11b s hows a confusion matrix with random m istakes.
The ˝rst image of Figure 5.12 shows one example of a classi˝er with only
97
:
13 %
test
accuracy where a go o d p ermutation was found. Please note that this is not the b est classi˝er.
The confusion matrix which resulted from a baseline classi˝er with
99
:
32 %
test accuracy is
displayed in as the second image.
Those results suggest that the ordering of classes is a valuable to ol to make patterns easier
to see. Humans, however, are go o d at ˝nding patterns even if they come from random noise.
Hence, for comparison, a confusion matrix of a classi˝er with 30 classes,
60 %
accuracy
and
40 %
uniformly random errors of a balanced dataset is created, optimized according to
Equation (4.1) and shown in Figure 5.11b. It clearly lo oks di˙erent than Figure 5.11a.
On the HASYv2 dataset the class-ordering is necessary to see anything as most p ossible
confusions do not happ en. See Figure 5.13 for comparison of the ˝rst 50 classes of the
unsorted confusion matrix and the sorted confusion matrix. If confusion matrices of a
maximum size of
50

50
are displayed, the ordered metho d can show only 8 matrices
b ecause the o˙-diagonal matrices are almost 0. Without sorting, 64 matrices have to b e
displayed.
48
5.2. Confusion Matrix Ordering
Figure 5.12.:
The ˝rst image shows the confusion matrix for the test of GTSRB set after optimization
to Equation (4.1). The diagonal elements are set to 0 in order to make other elements
easier to see. The symb ols next to the lab el on the vertical axis indicate the shap e
and the color of the signs.
The second image shows the same, but with baseline mo del.
Best viewed in electronic form.
49
Figure 5.13.:
The ˝rst 50 entries of the confusion matrix of the HASYv2 dataset. The diagonal
elements are set to 0 in order to make other ele ments easier to s ee. The top image
shows arbitrary class ordering, the b ottom image shows the optimized ordering.
5.3. Sp ectral Clustering vs CMO
5.3. Spectral Clustering vs CMO
This section evaluates the clustering quality of CMO in comparison to the clustering quality
of sp ectral clustering.
The evaluated mo del achieves
70
:
50 %
training accuracy and
53
:
16 %
test accuracy on
CIFAR-100. Figure 5.14 shows the sorted confusion matrix.
Figure 5.14.:
The ˝rst 50 e ntries of the ordered confusion matrix of the CIFAR-100 dataset. The
diagonal elements are s et to 0 in order to make othe r elements easier to see. Best
viewed in electronic form.
CIFAR-100 has pre-de˝ned coarse classes. Those are used as a ground truth for the clusters
which are to b e found. The numb er of errors is determined by
 (i)
 Join all
n
clusters which
contain the classes of the coarse class
C
to a set
M
. The error is
n
.
 (ii)
 Within
M
, ˝nd the
set of classes
M

which do not b elong to
C
.
 (iii)
 The ˝nal error is
n
+
j
M

j
. As can b e
seen in Table 5.4, b oth clustering metho ds ˝nd reasonable clusters. CMO, however, has
only half the error of sp ectral clustering.
The results for the HASYv2 dataset are qualitatively similar (see Table 5.5). It should b e
noted that the numb er of clusters was determined by using the semi-automatic metho d
based on CMO as describ ed in Section 4.2.
51
5. Exp erimental Evaluation
Cluster Sp ectral clustering Errors C MO Errors
˝sh aquarium ˝sh, orchid + ˛at˝sh
+ ray, shark + trout, lion
5 aquarium ˝sh, orchid + ˛at˝s h
+ ray + shark, trout
4
˛owers
orchid, aquarium ˝sh + sun-
˛ower + p oppy, tulip + rose ,
train
5
orchid, aquarium ˝sh + sun-
˛ower, p oppy, tulip, rose
2
p eople
baby, b oy, man + girl + woman
2 baby, b oy, girl, woman, man 0
reptiles
cro co d ile, plain, road, table,
wardrob e + dinosaur + lizard
+ snake , worm + turtle
9
cro co dile, lizard, lobster, cater-
pillar + dinosaur + snake + tur-
tle, crab
6
trees
maple, oak, pine + willow, forest
+ palm
3 palm, willow, pine, maple, oak 0
Total 24 12
Table 5.4.:
Di˙erences in sp ec tral cl ustering and CMO. Classes in a cluster are separated by
,
whereas cluste rs are separated by
+
.
Cluster Sp ectral clustering Errors CMO Errors
A
A
,
A
,
A
0
A
,
A
,
A
, Å 1
B
B
,
B
0
B
,
B
0
C
C
,
c
,
ˆ
and
C
,
˘
,
E
and
C
4
C
,
c
,
ˆ
,
C
and
C
1
D
D
,
D
,
D
,
.
1
D
,
D
,
D
0
E
E
and
E
,
"
2
E
and
E
,
"
,

,
2
4
F
F
and
F
,
F
1
F
and
F
,
F
1
H
H
and
H
,
{
and
H
3
H
and
H
,
H
1
K
K
,

0
K
,

0
L
L
,
b
and
L
,
L
1
L
,
b
and
L
,
L
1
M
M
and
M
and
M
2
M
and

,
M
and
M
3
N
N
and
N
,
N
and
N
2
N
and
N
,
N
and
N
,
@
3
O
O
,
O
,
0
,

,
°
,
#
and
o
1
O
,
O
,
0
,

,
°
and
#
and
o
2
P
P
,
P
and
p
,
ˆ
and
P
and
}
3
P
and
P
,
P
,
}
and
p
,
ˆ
2
Q
Q
,
Q
,
Q
,

,
t
,
&
,
`
,
=
, Æ,
1
7
Q
and
Q
,
Q
1
R
R
,
R
and
R
,
R
,
k
and
<
3
R
and
<
,
R
,
R
,
R
1
S
S
,
s
,
S
0
S
,
s
,
S
0
T
T
,
>
and
T
,
˝
1
T
,
>
and
T
,
˝
1
U
U
,
[
and
u
,
U
,
A
1
U
,
u
,
U
,
A
and
[
2
V
V
,
v
,
_
0
V
,
v
,
_
0
W
W
,
w
,
!
0
W
,
w
and
!
1
X
X
,
x
,
X
,
˜
,

0
X
,
x
,
X
,
˜
,

0
Y
Y
and
y
1
Y
,
y
0
Z
Z
,
z
,
Z
and
Z
,
Z
1
Z
,
z
,
Z
,
Z
,
Z
0
Total 34 25
Table 5.5.: Di˙erences in sp ectral clustering and CMO.
52
5.4. Hierarchy of Classi˝ers
5.4. Hierarchy of Classiers
In a ˝rst step, a classi˝er is trained on the 100 classes of CIFAR-100. The ˝ne-grained ro ot
classi˝er achieves an accuracy of
65
:
29 %
with test-time transformations. The accuracy on
the found sub-classes are listed in Table 5.6. The fact that the ro ot classi˝er achieves b etter
results within a cluster than the sp ecialized leaf classi˝ers in 13 of 14 cases could either
b e due to limited training data, over˝tting or the small size of
32 px

32 px
of the data.
The exp eriment also shows that most of the errors are due to not identifying the correct
cluster. Hence, in this case, more work in improving the ro ot classi˝er is necessary rather
than improving the discrimination of classes within a cluster.
Although the classes within a cluster capture most of the classi˝cations, many misclassi˝ca-
tions happ en outside of the clusters. For example, in cluster 3, a p erfect leaf classi˝er would
push the accuracy in the
ful l
column only to
63
:
50 %
due to errors of the ro ot classi˝er
where the ro ot classi˝er do es not predict the correct cluster.
The leaf classi˝ers use the same top ology as the ro ot classi˝er. By initializing them with
the ro ot classi˝ers weights their p erformance can b e pushed at ab out the
inner
accuracy.
They are, however, only useful if their accuracy is well ab ove the
inner
accuracy of the ro ot
classi˝er. Hence, for CIFAR-100, building hierarchies of classi˝ers is not useful.
Cluster Classes
accuracy
ro ot classi˝er leaf classi˝er
cluster identi˝ed class identi˝ed | cluster class identi˝ed | cluster
1 3
69
:
67 % 84
:
27 % 72
:
98 %
2 5
46
:
60 % 58
:
54 % 43
:
47 %
3 2
58
:
50 % 92
:
13 % 83
:
46 %
4 2
50
:
50 % 87
:
83 % 81
:
74 %
5 3
44
:
67 % 79
:
29 % 71
:
01 %
6 2
29
:
50 % 78
:
67 % 72
:
00 %
7 2
52
:
50 % 92
:
11 % 87
:
72 %
8 2
59
:
50 % 86
:
23 % 81
:
88 %
9 2
59
:
00 % 90
:
08 % 87
:
79 %
10 2
62
:
00 % 85
:
52 % 73
:
10 %
11 2
67
:
00 % 87
:
01 % 75
:
32 %
12 2
72
:
50 % 94
:
77 % 76
:
77 %
13 2
64
:
00 % 82
:
58 % 86
:
27 %
14 2
79
:
67 % 89
:
85 % 89
:
10 %
Table 5.6.:
Accuracies of the ro ot classi˝er trained on the ful l set of 100 classes evaluated on
14 clusters of classes. Each class has 100 elements to test. The colum n
cluster identi˝ed
gives the p ercentage that the ro ot classi˝ers argmax prediction is within the correct
cluster, but not necessarily the correct class. The columns
class identi˝ed | cluster
only
consider data p oints where the ro ot classi˝er correctly identi˝ed the cluster.
53
5. Exp erimental Evaluation
5.5. Increased width for faster learning
More ˝lters in one layer could simplify the optimization problem as each ˝lter needs smaller
up dates. Hence a CNN
N
with
n
i
˝lters in layer
i
is exp ected to take more ep o chs than a
CNN
N
0
with
2

n
i
˝lters in layer
i
to achieve the same validation accuracy.
This hyp othesis can b e falsi˝ed by training a CNN
N
and a CNN
N
0
and comparing the
trained numb er of ep o chs. As more ˝lters can lead to di˙erent results dep ending on the
layer where they are added, ˝ve mo dels are trained. The details ab out those mo dels are
given in Table 5.7
Name Layer
Filter count Total
Baseline New parameters
m
9
9 64 638
5 978 566
m
0
9
9 64 974
8 925 622
m
11
11 512 3786
5 982 698
m
0
11
11 512 1024
1 731 980
m
13
13 512 8704
5 982 092
Table 5.7.:
Mo dels which are identical to the baseline, except that the numb er of ˝lters of on e layer
was increased.
The detailed results are given in Table 5.8. As exp ected, the numb er of training ep o chs of
the mo dels with increased numb ers of parameters is lower. The wall-clo ck time, however, is
higher due to the increase in computation p er forward- and backward-pass.
For
m
9
,
m
11
and
m
13
, the ˝lter weight range of the layer with increased capacity decreases
compared to Figure 5.6, the ˝lter weights of the layer with increased capacity are more
concentrated around zero compared to F igure 5.2. For mo del
m
13
, the distribution of
weight of the output layer changed to a more b ell-shap ed distribution. Except for this, the
distribution of ˝lter weights in other layers did not change for all three mo dels compared to
the baseline.
Mo del Parameters
Accuracy Training
Single Mo del Ensemble Mean Ep o chs Mean Time
Mean std
baseline
944 012 63
:
38 %
0.55
64
:
70 %
154.7
3856 s
m
9
5 978 566 65
:
53 %
0.37
66
:
72 %
105.7
4472 s
m
0
9
8 925 622 65
:
10 %
1.09
66
:
54 %
95.6
5261 s
m
11
5 982 698
65
:
73 %
0.77
67
:
38 %
149.2
5450 s
m
0
11
1 731 980 62
:
12 %
0.48
62
:
89 %
143.6
3665 s
m
13
5 982 092 62
:
39 %
0.66
63
:
77 %
147.8
4485 s
Table 5.8.:
Training time in ep o chs and wall-clo ck tim e for the baseline and mo dels
m
9
,
m
11
,
m
13
as well as their acc urac ies.
54
5.6. Weight up dates
5.6. Weight updates
Section 5.5 shows that wider networks learn faster. One hyp othesis why this happ ens is
that every single weight up dates can b e smaller to learn the same function. Thus the loss
function is smo other and thus gradient descent based optimization algorithms lead to more
consistent weight up dates.
Consequently, it is exp ected that layers with fewer ˝lters have more erratic up dates. If
there are many ˝lters, the weights of a ˝lter which do es not contribute much to the end
results or is even harmful ˝lter can gradually b e set to zero, essentially removing one path
in the network.
In order to test the hyp othesis, the baseline mo del was adjusted. The numb er of ˝lters in
layer 5 was reduced from 64 ˝lters to 3 ˝lters. As one can see in Figure 5.15, the mean
weight up date of the layers 1, 3, 5, 7 and 9 have a far bigger range than the layers 11, 13 and
15 after ep o ch 50. Compared to the baseline mo dels mean up dates (Figure 5.8, Page 46),
the mean weight up dates of layers 1 and 3 are higher, the range of the mean weight up date
from ep o ch 50 is higher for layer 5 and the range of mean up dates of layer 7 is higher.
For the maximum and the sum, no similar pattern could b e observed (see Figures A.3
and A.4).
Figure 5.15.:
Mean weight up dates b etween ep o chs by layer. The mo del is the baseline mo del, but
with layer 5 reduced to 3 ˝lters.
55
5. Exp erimental Evaluation
5.7. Multiple narrow layers vs One wide layer
On a given feature map size one can have an arbitrary numb er of convolutional layers with
SAME
padding and each layer can have an arbitrary numb er of ˝lters. A convolutional layer
with more ˝lters is called
wider
[
ZK16
], a convolutional layer with fewer ˝lters is thus called
narrower and the numb er of ˝lters in a convolutional layer is the layers
width
.
If the numb er of parameters which may b e used for the feature map scale is ˝xed and high
enough, there are still many combinations. If
n
i
with
i
= 0
; : : : ; k
is the numb er of output
feature maps of layer
i
where
i
= 0
is the input layer and all ˝lters are
3

3
˝lters without
a bias, then the numb er of parameters is
Parameters
=
k
X
i
=1

(
n
i

1

3
2
+ 1)

n
i

Hence the width of one layer do es not only in˛uence the parameters in this layer, but also
in the next layer.
The numb er of p ossible subsequent layers of one feature map size is enormous, even if
constraints are placed on the numb er of parameters. For example, the ˝rst convolutional
layer of the baseline mo del has 896 parameters. If one assumes that less than 3 ˝lters p er
layer are not desirable, one keeps all layers having a bias and all layers only use
3

3
˝lters,
then the maximum depth is 10. If one furthermore assumes that at least 800 parameters
should b e used, there are still 120 p ossible layer combinations. As exp erimentally evaluating
one layer combination takes ab out 10 hours on a GTX 970 for CIFAR-100 it is not p ossible
to evaluate all layer combinations. In the following, a couple of changes to the network
width / depth will b e evaluated.
Each layer expands the p erceptive ˝eld. Hence deep er layer can use more of the input for
every single output value. But deep er networks need more time for inference as the output
of layer
i
has to b e computed b efore the output of
i
+ 1
can b e computed. Hence there is
less p otential to parallelize computations. Each ˝lter can b e seen as a concept which can
b e learned. The deep er the ˝lter is in the network, the higher is the abstraction level of the
concept. In most cases, b oth is necessary: Many di˙erent concepts (width) and high-level
concepts (depth).
Reducing the two ˝rst convolutional layers of the baseline mo del (see Page 39) to one
convolutional layer of 48 ˝lters (
944 396
parameters in total, whereas the baseline mo del
has
944 012
parameters) resulted in a mean accuracy of
61
:
64 %
(-
1
:
74 %
) and a standard
deviation of
˙
= 1
:
12
(+0.57). The ensemble achieved
63
:
18 %
(-
1
:
52 %
). As exp ected,
the training time p er ep o ch was reduced. For the GTX 980, it was reduced from
22
:
0 s
of
the baseline mo del to
15 s
of the mo del with one less convolutional layer, one less Batch
Normalization and one less activation layer. The inference time was also reduced from
6 ms
56
5.8. Batch Normalization
to
4 ms
for 1 image and from
32 ms
to
23 ms
for 128 images. Due to the loss in accuracy of
more then one p ercentage p oint of the mean mo del and the increased standard deviation of
the mo dels p erformance, at least two convolutional layers are on the
32 px

32 px
feature
map scale are recommendable for CIFAR-100.
Changing the baseline to have less ˝lters but more layers is another option. This was tried
for the ˝rst blo ck at the
32 px

32 px
feature map scale. The two convolutional layers
(layers 1  4 in Page 39) were replaced by two convolutional layers with 27 ˝lters and one
convolutional layer with 26 ˝lters in the
convolution - BN - ELU
pattern. The mo del
has
944 132
parameters. Compared to the baseline mo del, the time for inference was the
same. This is unexp ected, b ecause the inference time changed when a layer was removed at
this scale. The mean test accuracy was
63
:
66 %
(+0.28) and the standard deviation was
˙
= 1
:
03
(+0.48). The ensemble achieved
64
:
91 %
test accuracy (+0.21).
Having two nonlinearities at each feature map scale could b e imp ortant to learn nonlinear
transformations at that scale. As the baseline mo del do es only have one nonlinearity at the
8

8
feature maps scale, another convolutional layer with 64 ˝lters, Batch Normalization
and ELU was added. To keep the numb er of parameters constant, layer 11 of the baseline
mo del was reduced from 512 ˝lters to 488 ˝ lters. The new mo del achieves a mean accuracy
of
63
:
09 %
(-0.29) with a standard deviation of
˙
= 0
:
70
(+0.15). The ensemble achieves
an accuracy of
64
:
39 %
(+0.31). This could indicate that having two convolutional layers
is more imp ortant for layers close to the input than intermediate layer. Alternatively, the
parameters could b e more imp ortant in layer 11 than having a new convolutional layer after
layer 9.
In order to control the hyp othesis that having two convolutional layers are less imp ortant in
the middle of a network, the second convolutional layer at the
16

16
feature map scale is
removed. The ˝rst convolutional layer was increased from 32 ˝lters to 59 ˝lters, the second
convolutional layer was increased from 32 ˝lter s to 58 ˝lters in order to keep the amount of
parameters of the mo del constant. The adjusted mo del achieved
62
:
72 %
(-0.66) mean test
accuracy with a standard deviation of
˙
= 0
:
84
(+0.29). The ensemble achieved
63
:
88 %
test accuracy (-0.66).
Even more extreme, if b oth convolutional layers are removed from the
16

16
feature map
scale, the mean test accuracy drops to
61
:
21 %
(-2.17) with a standard deviation of
˙
= 0
:
51
(-0.04). The ensemble achieves a test accuracy of
63
:
07 %
(-1.63). Thus it is very imp ortant
to have at least one convolutional layer at this feature map scale.
5.8. Batch Normalization
In [
CUH15
], the authors write that Batch Normalization do es not improve ELU networks.
Hence the e˙ect of removing Batch Normalization from the baseline is investigated in this
57
5. Exp erimental Evaluation
exp eriment.
As b efore, 10 mo dels are trained on CIFAR-100. The training setup and the mo del
m
no-bn
are identical to the baseline mo del
m
, except that in
m
no-bn
the Batch Normalization layers
are removed.
One notable di˙erence is the training time: While
m
needs
21 ms
p er ep o ch in average on
a GTX 980,
m
no-bn
only needs
21 ms
p er ep o ch. The numb er of ep o chs used for training,
however, also increased noticeably from 149 ep o chs to 178 ep o chs in average. The standard
deviation of trained ep o chs is 17.3 ep o chs for the baseline mo del and 23.4 ep o chs for
m
no-bn
.
The mean accuracy of
m
no-bn
is
62
:
86 %
and hence 0.52 p ercentage p oints worse. The
standard deviation b etween mo dels increased from 0.55 to 0.61. This is likely a result of the
early stopping p olicy and the di˙erences in training ep o chs. This can p otentially b e ˝xed
by retraining the mo dels which stopp ed earlier than the mo del which was trained for the
biggest amount of ep o chs. The ensemble test accuracy is
63
:
88 %
and hence 0.82 p ercentage
p oints worse than the baseline.
The ˝lter weight range and distribution is approximately the same as Figure 5.6 and
Figure 5.2, but the distribution of bias weights changed noticeably: While the bias weights of
the baseline are spread out in the ˝rst layer and much more concentrated in subsequent layers
(see Figure 5.3), the mo del without Batch Normalization has rather concentrated weights
in the ˝rst layers and only the bias weights of the last layer is spread out (see Figure A.2).
Another mo del
m
0
no-bn
which has one more ˝lter in the convolutional layer 1, 3, 5, and 7 to
comp ensate for the loss of parameters in Batch Normalization. The mean test accuracy of
10 such mo dels is
62
:
87 %
which is 0.51 p ercentage p oints worse than the baseline. The
ensemble of
m
0
no-bn
achieves
64
:
33 %
which is 0.37 p ercentage p oints worse than the baseline.
The mean training time was
14 s
p er ep o ch and 157.4 ep o chs with a standard deviation of
20.7 ep o chs.
Hence it is not advisable to remove Batch Normalization for the ˝nal mo del. It could,
however, b e p ossible to remove Batch Normalization for the exp eriments to iterate quicker
through di˙erent ideas if the relative p erformance changes b ehave the same with or without
Batch Normalization.
58
5.9. Batch size
5.9. Batch siz e
The mini-batch size
m
2
N

1
in˛uences
‹
Ep o chs until convergence
: The smaller
m
, the more often the mo del is up dated
in one ep o ch. Those up dates, however, are based on fewer samples of the dataset.
Hence the gradients of di˙erent mini-batches can noticeably di˙er. In the literature,
this is referred to as gradient noise [KMN
+
16].
‹
Training t ime p er ep o ch
: The smaller the batch size, the higher the training time
p er ep o ch as the hardware is not optimally utilized.
‹
Resulting mo del quality
: The choice of the hyp erparameter
m
in˛uences the
accuracy of the classi˝er when training is ˝nished. [
KMN
+
16
] supp orts the view that
smaller
m
result in less sharp minima. Hence smaller
m
lead to b etter generalization.
Empiric evaluation results can b e found in Table 5.9. Those results con˝rm the claim
of [KMN
+
16] that lower batch sizes generalize b etter.
m
Training
Ep o chs
Mean total Single mo del Ensemble
time training time Accuracy std Accuracy
8
118
s
ep o ch
81

153
14 131 s 61
:
93 %
˙
= 1
:
03 65
:
68 %
16
62
s
ep o ch
103  173
8349 s
64
:
16 %
˙
= 0
:
81
66
:
98 %
32
35
s
ep o ch
119  179
5171 s 64
:
11 %
˙
= 0
:
75 65
:
89 %
64
25
s
ep o ch
133  195
2892 s
63
:
38 %
˙
= 0
:
55
64
:
70 %
128
18
s
ep o ch
145  239
3126 s 62
:
23 %
˙
= 0
:
73 63
:
55 %
Table 5.9.:
Training time p er ep o ch and single mo del test set accuracy (mean and standard deviation)
of baseline mo dels trained with di˙erent mini-batch sizes
m
on GTX 970 GPUs on
CIFAR-100.
5.10. Bias
Figure 5.3 suggests that the bias is not imp ortant for the layers 11, 13 and 15. Hence a
mo del
m
no-bias
is created which is identical to the baseline mo del
m
, except that the bias of
layers 11, 13 and 15 is removed.
The mean test accuracy of 10 trained
m
no-bias
is
63
:
74 %
which is an improvement of
0.36 p ercentage p oints over the baseline. The ensemble achieves a test accuracy of
65
:
13 %
which is 0.43 p ercentage p oints b etter than the baseline. Hence the bias can safely b e
removed.
Removing the biases did not have a noticeable e˙ect on the ˝lter weight range, the ˝lter
weight distribution or the distribution of the remaining biases. Also, the

and

parameters
of the Batch Normalization layers did not noticeably change.
59
5. Exp erimental Evaluation
5.11. Learned Color Space Transformation
In [
MSM16
] it is describ ed that placing one convolutional layer with 10 ˝lters of size
1

1
directly after the input and then another convolutional layer with 3 ˝lters of size
1

1
acts
as a learned transformation in another color space and b o osts the accuracy.
This approach was evaluated on CIFAR-100 by adding a convolutional layer with ELU ac-
tivation and 10 ˝lters followed by another convolutional layer with ELU activation and
3 ˝lters. The mean accuracy of 10 mo dels was
63
:
31 %
with a standard deviation of 1.37.
The standard deviation is noticeable higher than the standard deviation of the baseline
mo del (0.55) and the accuracy also decreased by 0.07 p ercentage p oints. The accuracy of
the ensemble is at
64
:
77 %
and hence 0.07 p ercentage p oints higher than the accuracy of
the baseline mo dels.
The inference time for 1 image and for 128 images did not change compared to the baseline.
The training time p er ep o ch increased from
26 s
to
30 s
on the GTX 970.
Hence it is not advisable to use the learned color space transformation.
5.12. Pooling
An alternative to max p o oling with stride 2 with a
2

2
kernel is using a
3

3
kernel with
stride 2.
This approach was evaluated on CIFAR-100 by replacing all max p o oling layers with the
3

3
kernel max p o oling (and
SAME
padding). The mean accuracy of 10 mo dels was
63
:
32 %
(

0
:
06
) and the standard deviation was 0.57 (
+0
:
02
). The ensemble achieved
65
:
15 %
test
accuracy (
+0
:
45
).
The training time p er ep o ch decreased from
20
:
5 s
-
21
:
1 s
to
18
:
6 s
(mean of 10 training runs)
on the Nvidia GTX 970. The time for inference increased from
25 ms
to
26 ms
for a batch
of 128 images.
5.13. Activation Functions
Nonlinear, di˙erentiable activation functions are imp ortant for neural networks to allow them
to learn nonlinear decision b oundaries. One of the simplest and most widely used activation
functions for CNNs is ReLU [
KSH12
], but others such as ELU [
CUH15
], parametrized
recti˝ed linear unit (PReLU) [
HZRS15b
], softplus [
ZYL
+
15
] and softsign [
BDLB09
] have
b een prop osed. The baseline uses EL U.
60
5.13. Activati on Fun ctions
Activation functions di˙er in the range of values and the derivative. The de˝nitions and
other comparisons of eleven activation functions are given in Table B.3.
Theoretical explanations why one activation function is preferable to another in some
scenarios are the following:
‹
Vanishing Gradient
: Activation functions like tanh and the logistic function sat-
urate outside of the interval
[

5
;
5]
. This means weight up dates are very small for
preceding neurons, which is esp ecially a problem for very deep or recurrent networks as
describ ed in [
BSF94
]. Even if the neurons learn eventually, learning is slower [
KSH12
].
‹
Dying ReLU
: The dying ReLU problem is similar to the vanishing gradient problem.
The gradient of the ReLU function is 0 for all non-p ositive values. This means if all
elements of the training set lead to a negative input for one neuron at any p oint in the
training pro cess, this neuron do es not get any up date and hence do es not participate
in the training pro cess. This problem is addressed in [MHN13].
‹
Mean unit activatio n
: Some publications like [
CUH15
,
IS15
] claim that mean
unit activations close to 0 are desirable. They claim that this sp eeds up learning
by reducing the bias shift e˙ect. The sp eedup of learning is supp orted by many
exp eriments. Hence the p ossibility of negative activations is desirable.
Those considerations are listed in Table 5.10 for 11 activation functions. Besides the
theoretical prop erties, empiric results are provided in Tables 5.11 and 5.12. The baseline
network was adjusted so that every activation function except the one of the output layer
was replaced by one of the 11 activation functions.
As exp ected, PReLU and ELU p erformed b est. Unexp ected was that the logistic function,
tanh and softplus p erformed worse than the identity and it is unclear why the pure-softmax
network p erformed so much b etter than the logistic function. One hyp othesis why the
logistic function p erforms so bad is that it cannot pro duce negative outputs. Hence the
logistic

function was develop ed:
logistic

(
x
) =
1
1 +
e

x

0
:
5
The logistic

function has the same derivative as the logistic function and hence still su˙ers
from the vanishing gradient problem. The network with the logistic

function achieves an
accuracy which is
11
:
30 %
b etter than the network with the logistic function, but is still
5
:
54 %
worse than the ELU.
Similarly, ReLU was adjusted to have a negative output:
ReLU

(
x
) = max(

1
; x
) =
ReLU
(
x
+ 1)

1
The results of ReLU

are much worse on the training set, but p erform similar on the test
61
5. Exp erimental Evaluation
set. The result indicates that the p ossibility of hard zero and thus a sparse representation
is either not imp ortant or similar imp ortant as the p ossibility to pro duce negative outputs.
This contradicts [GBB11, SMGS14].
A key di˙erence b etween the logistic

function and ELU is that ELU do es neither su˙ers
from the vanishing gradient problem nor is its range of values b ound. For this reason, the
S2ReLU activation function, de˝ned as
S2ReLU
(
x
) =
R eLU
(
x
2
+ 1)

R eLU
(

x
2
+ 1) =
8
>
>
>
<
>
>
>
:

x
2
+ 1
if
x
 
2
x
if

2

x

2
x
2
+ 1
if
x >

2
This function is similar to SReLUs as intro duced in [
JXF
+
16
]. The di˙erence is that S2ReLU
do es not intro duce learnable parameters. The S2ReLU was designed to b e symmetric, b e
the identity close to zero and have a smaller absolute value than the identity farther away.
It is easy to compute and easy to implement.
Those results not only the absolute values, but also the relative comparison might
dep end on the network architecture, the training algorithm, the initialization and the
dataset. Results for MNIST can b e found in Table 5.13 and for HASYv2 in Table A.2. For
b oth datasets, the logistic function has a much shorter training time and a noticeably lower
test accuracy.
Function Vanishing Gradient Negative Activation p ossible Bound activation
Identity
No
Yes
No
Logistic
Yes
No
Yes
Logistic

Yes
Yes
Yes
Softmax
Yes
Yes
Yes
tanh
Yes
Yes
Yes
Softsign
Yes
Yes
Yes
ReLU
Yes
1
No
Half-sided
Softplus
No
No
Half-sided
S2ReLU
No
Yes
No
LReLU/PReLU
No
Yes
No
ELU
No
Yes
No
Table 5.10.: Prop erties of activation functions.
1
The dying ReLU problem is similar to the vanishing gradient problem.
62
5.13. Activati on Fun ctions
Function
Single mo del Ensemble of 10
Training set Test set Training set Test set
Identity
66
:
25 %
˙
= 0
:
77
56
:
74 %
˙
= 0
:
51 68
:
77 % 58
:
78 %
Logistic
51
:
87 %
˙
= 3
:
64 46
:
54 %
˙
= 3
:
22 61
:
19 % 54
:
58 %
Logistic

66
:
49 %
˙
= 1
:
99 57
:
84 %
˙
= 1
:
15 69
:
04 % 60
:
10 %
Softmax
75
:
22 %
˙
= 2
:
41 59
:
49 %
˙
= 1
:
25 78
:
87 % 63
:
06 %
Tanh
67
:
27 %
˙
= 2
:
38 55
:
70 %
˙
= 1
:
44 70
:
21 % 58
:
10 %
Softsign
66
:
43 %
˙
= 1
:
74 55
:
75 %
˙
= 0
:
93 69
:
78 % 58
:
40 %
ReLU
78
:
62 %
˙
= 2
:
15 62
:
18 %
˙
= 0
:
99 81
:
81 % 64
:
57 %
ReLU

76
:
01 %
˙
= 2
:
31 62
:
87 %
˙
= 1
:
08 78
:
18 % 64
:
81 %
Softplus
66
:
75 %
˙
= 2
:
45 56
:
68 %
˙
= 1
:
32 71
:
27 % 60
:
26 %
S2ReLU
63
:
32 %
˙
= 1
:
69 56
:
99 %
˙
= 1
:
14 65
:
80 % 59
:
20 %
LReLU
74
:
92 %
˙
= 2
:
49 61
:
86 %
˙
= 1
:
23 77
:
67 % 64
:
01 %
PReLU
80
:
01 %
˙
= 2
:
03 62
:
16 %
˙
= 0
:
73
83
:
50 % 64
:
79 %
ELU
76
:
64 %
˙
= 1
:
48
63
:
38 %
˙
= 0
:
55 78
:
30 % 64
:
70 %
Table 5.11.:
Training and test accuracy of adjusted baseline mo dels trained with di˙erent activation
functions on CIFAR-100. For LReLU,

= 0
:
3
was chosen.
Function
Inference p er Training
Ep o chs
Mean total
1 Image 128 time training time
Identity
8 ms 42 ms 31
s
ep o ch
108 
148
3629 s
Logistic
6 ms
31 ms
24
s
ep o ch
101
 167
2234 s
Logistic

6 ms
31 ms 22
s
ep o ch
133  255
3421 s
Softmax
7 ms 37 ms 33
s
ep o ch
127  248
5250 s
Tanh
6 ms
31 ms
23
s
ep o ch
125  211
3141 s
Softsign
6 ms
31 ms
23
s
ep o ch
122  205
3505 s
ReLU
6 ms
31 ms
23
s
ep o ch
118  192
3449 s
Softplus
6 ms
31 ms
24
s
ep o ch
101
 165
2718 s
S2ReLU
5 ms
32 ms 26
s
ep o ch
108  209
3231 s
LReLU
7 ms 34 ms 25
s
ep o ch
109  198
3388 s
PReLU
7 ms 34 ms 28
s
ep o ch
131  215
3970 s
ELU
6 ms
31 ms
23
s
ep o ch
146  232
3692 s
Table 5.12.:
Training time and inference time of adjusted baseline mo dels trained with di˙erent
activation functions on GTX 970 GPUs on CIFAR -100. It was exp ected that the
identity is the fastest function. This result is like ly an implementation sp eci˝c problem
of Keras 2.0.4 or Tensor˛ow 1.1.0.
63
5. Exp erimental Evaluation
Function
Single mo del Ensemble Ep o chs
Accuracy std Accuracy Range Mean
Identity
99
:
45 %
˙
= 0
:
09 99
:
63 %
55  77 62.2
Logistic
97
:
27 %
˙
= 2
:
10 99
:
48 %
37
 76
54.5
Softmax
99
:
60 %
˙
= 0
:
03
99
:
63 %
44  73 55.6
Tanh
99
:
40 %
˙
= 0
:
09 99
:
57 %
56  80 67.6
Softsign
99
:
40 %
˙
= 0
:
08 99
:
57 %
72  101 84.0
ReLU
99
:
62 %
˙
= 0
:
04
99
:
73 %
51  94 71.7
Softplus
99
:
52 %
˙
= 0
:
05 99
:
62 %
62 
70
68.9
PReLU
99
:
57 %
˙
= 0
:
07
99
:
73 %
44  89 71.2
ELU
99
:
53 %
˙
= 0
:
06 99
:
58 %
45  111 72.5
Table 5.13.:
Test accuracy of adjusted bas eline mo dels trained with di˙erent activation functions
on MNIST.
5.14. Label smoothing
Ensembles consisting of
n
mo dels trained by the same pro cedure on the same data but
initialized with di˙erent weights and trained with a di˙erent order of the training data
p erform consistently b etter than single mo dels. One drawback of ensembles in applications
such as self-driving cars is that they increase the computation by a factor of
n
. One idea
why they improve the test accuracy is by reducing the variance.
The idea of lab el smo othing is to use the ensemble prediction of the training data as lab els
for another classi˝er. For every element
x
of the training set, the one-hot enco ded target
t
(
x
)
is smo othed by the ensemble prediction
y
E
(
x
)
t
0
(
x
) =


t
(
x
) + (1


)
y
E
(
x
)
where

2
[0
;
1]
is the smo othing factor.
There are three reasons why lab el smo othing could b e b ene˝cial:
‹
Training sp eed
: The ensemble prediction contains more information ab out the
image than binary class decisions. Classi˝ers in computer vision predict how similar
the input lo oks to other input of the classes they are trained on. By smo othing the
lab els, the information that one image could also b elong to another class is passed to
the optimizer. In early stages of the optimization this could lead to a lower loss on
the non-smo othed validation set.
‹
Higher accuracy
: Using smo othed lab els for the optimization could lead to a higher
accuracy of the base-classi˝er due to a smo othed error surface. It might b e less likely
64
5.14. Lab el smo othing
that the classi˝er gets into bad lo cal minima.
‹
Lab el noise
: Dep ending on the way how the lab els are obtained, it might not always
b e clear which lab el is the correct one. Also, lab eling errors can b e present in training
datasets. Those errors severely harm the training. By smo othing the lab els errors
could b e relaxed.
10 mo dels
m
smo oth
are trained with the

= 0
:
5
smo othed lab els from the prediction
of an ensemble of 10 baseline mo dels. The mean accuracy of the mo dels trained on the
smo othed training set lab els was
63
:
61 %
(+
0
:
23 %
) and the standard deviation was
˙
= 0
:
72
(+
0
:
17 %
). The ensemble of 10
m
smo oth
mo dels achieved
64
:
79 %
accuracy (
+
0
:
09 %
). Hence
the e˙ect of this kind of lab el smo othing on the ˝nal accuracy is questionable.
The training sp eed didn't noticeably change either: The numb er of trained ep o chs ranged
from 144 to 205, the mean numb er of ep o chs was 177. The baseline training ranged from
146 to 232 ep o chs with a mean of 174 ep o chs. After 10, 30 and 80 ep o chs b oth training
metho ds accuracy di˙ered by less than one p ercentage p oint. Hence it is unlikely that lab el
smo othing has a p ositive e˙ect on the training sp eed.
Hinton et al. called this metho d
distil lation
in [
HVD15
]. Hinton et al. used smo oth and
hard lab els for training, this work only used smo othed lab els.
65
5. Exp erimental Evaluation
5.15. Optimiz ed Classier
In comparison to the baseline classi˝er, the following changes are applied to the optimized
classi˝er:
‹
Remove the bias for the last layers
: For all layers which output a
1

1
feature
map, the bias is removed
‹
Increase the max p o oling kernel to
3

3
‹
More ˝lters in the ˝rst layers
The detailed architecture is given in Table 5.14 and visualized in Figure 5.16. The evaluation
is given in Table 5.15 and the timing comparison is given in Table 5.16.
# Typ e Filters @
Patch size / stride
Parameters FLOPs Output size
Input 0 0 3 @ 32

32
1 Convolution 69 @
3

3

3
/ 1 1 932 3 744 768
69
@
32

32
2 BN + ELU 138 353 418
69
@
32

32
3 Convolution 69 @
3

3

32
/ 1 42 918 37 684 096
69
@
32

32
4 BN + ELU 138 353 418
69
@
32

32
Max p o oling
2

2
/ 2 0 40 960 32 @ 16

16
5 Convolution 64 @
3

3

32
/ 1 39 808 20 332 544 64 @ 16

16
6 BN + ELU 128 82 048 64 @ 16

16
7 Convolution 64 @
3

3

64
/ 1 36 928
18 857 984
64 @ 16

16
8 BN + ELU 128 82 048 64 @ 16

16
Max p o oling
2

2
/ 2 20 480 64 @ 8

8
9 Convolution 64 @
3

3

64
/ 1 36 928 4 714 496 64 @ 8

8
10 BN + ELU 128 20 608 64 @ 8

8
Max p o oling
2

2
/ 2 5 120 64 @ 4

4
11 Convolution (v) 512 @
4

4

64
/ 1
524 288
1 048 064 512 @ 1

1
12 BN + ELU 1 024 3 584 512 @ 1

1
Drop out 0.5 0 0 512 @ 1

1
13 Convolution 512 @
1

1

512
/ 1 262 144 523 776 512 @ 1

1
14 BN + ELU 1 024 3 584 512 @ 1

1
Drop out 0.5 0 0 512 @ 1

1
15 Convolution k @
1

1

512
/ 1
512

k
512

k
k @ 1

1
Global avg Po oling
1

1
0
k
k @ 1

1
16 BN + Softmax
2
k
7
k
k @ 1

1
P
514
k
+947 654
520
k
+87 870 996
179 200+
2
k
Table 5.14.:
Optimized architecture with 3 input channels of size
32

32
. All convolutional layers
use
SAME
padding, except for layer 11 which used
VALID
padding in order to decrease
the feature map s ize to
1

1
. If the input fe atu re map is bigger than
32

32
, for each
p ower of two there are two
Convolution + BN + ELU
blo cks and one
Max pooling
blo ck added. This is the framed part in th e table.
66
5.15. Optimized Classi˝er
32

32
Input
C
69@3

3
=
1
BN + ELU
C
69@3

3
=
1
BN + ELU
16

16
max p o oling
3

3
=
2
C
64@3

3
=
1
BN + ELU
C
64@3

3
=
1
BN + ELU
8

8
max p o oling
3

3
=
2
C
64@3

3
=
1
BN + ELU
4

4
max p o oling
3

3
=
2
C*
512@4

4
=
1
(V)
BN + ELU
Drop out,
p
= 0
:
5
1

1
C*
512@1

1
=
1
BN + ELU
Drop out,
p
= 0
:
5
C*
k
@1

1
=
1
Global AVG p o oling
BN + Softmax
Figure 5.16.:
Architecture of the optim ized mo del.
C
32@3

3
=
1
is a convolutional layer with
32 ˝lters of kernel size
3

3
with stride 1. The * indicates that no bias is used.
Dataset
Single Mo del Accuracy Ensemble of 10
Training Set Test Set Training Set Test Set
Asirra
95
:
83 %
˙
= 4
:
70 90
:
75 %
˙
= 4
:
73 98
:
78 % 93
:
09 %
CIFAR-10
94
:
58 %
˙
= 0
:
70 87
:
92 %
˙
= 0
:
46 96
:
47 % 89
:
86 %
CIFAR-100
77
:
96 %
˙
= 2
:
18 64
:
42 %
˙
= 0
:
73 81
:
44 % 67
:
03 %
GTSRB
100
:
00 %
˙
= 0
:
00 99
:
28 %
˙
= 0
:
10 100
:
00 % 99
:
51 %
HASYv2
88
:
79 %
˙
= 0
:
45 85
:
36 %
˙
= 0
:
15 89
:
36 % 85
:
92 %
MNIST
99
:
88 %
˙
= 0
:
10 99
:
48 %
˙
= 0
:
13 99
:
99 % 99
:
67 %
STL-10
95
:
43 %
˙
= 3
:
57 75
:
09 %
˙
= 2
:
39 98
:
54 % 78
:
66 %
SVHN
99
:
08 %
˙
= 0
:
07 96
:
37 %
˙
= 0
:
12 99
:
50 % 97
:
47 %
Table 5.15.:
Optimized mo del accuracy on eight datasets. The single mo del actuary is the 10 mo de ls
used in the ense mble. The empirical standard d eviati on
˙
of the accuracy is also given.
CIFAR-10, CIFAR-100 and STL-10 mo dels use test-time transformations. None of the
mo dels uses unlab eled data or data from other datasets. For MNIST, GTSRB, SVHN
and HASY, no test time trans formations are used.
Network GPU Tensor˛ow
Inference p er Training
1 Image 128 im age s time / ep o ch
Optimized Default Intel i7-4930K
5 ms 432 ms 386 s
Optimized Optimized Intel i7-4930K
4 ms 307 ms 315 s
Optimized Default GeForce 940MX
4 ms 205 ms 192 s
Optimized Default GTX 970
6 ms 41 ms 35 s
Optimized Default GTX 980
3 ms 35 ms 27 s
Optimized Default GTX 980 Ti
6 ms 36 ms 26 s
Optimized Default GTX 1070
2 ms 24 ms 21 s
Optimized Default Titan Black
4 ms 46 ms 43 s
Table 5.16.:
Sp eed comparison of the optimized mo del on CI FAR-10. The baseline mo del is
evaluated on six Nvidia GPUs and one CPU. The weights for DenseNet-40-12 are taken
from [
Ma j17
]. Weights the baseline mo del can b e found at [
Tho17b
]. The optimized
Tensor˛ow build makes use of SSE4.X, AVX, AVX2 and FMA instructions.
67
5. Exp erimental Evaluation
5.16. Early Stopping vs More Data
A separate validation set is necessary for two reasons:
 (1)
 Early stopping and
 (2)
 preventing
over˝tting due to many exp eriments. To prevent over˝tting, a di˙erent dataset can b e used.
For example, all decisions ab out hyp erparameters in this thesis are based on CIFAR-100,
but the network is ˝nally trained and evaluated with the same hyp erparameters on all
datasets.
2
The validation set can hence b e removed if early stopping is removed. Instead,
the validation data is used in a ˝rst run to determine the numb er of ep o chs necessary for
training. In a second training run the validation data is added to the training set. The
numb er of used ep o chs for the second run is given in Table 5.17.
Dataset Mean ep o chs Train data classes average data / class
Asirra 60
15 075
2
7538
MNIST 41
54 000
10
5400
SVHN 45
543 949
10
54 395
CIFAR-10 84
45 000
10
4500
HASYv2 92
136 116
369
369
GTSRB 97
35 288
43
821
STL-10 116
4500
10
450
CIFAR-100 155
45 000
100
450
Table 5.17.:
Mean numb er of training ep o chs for the optim ized mo del. For comparison, the total
amount of used training data, the numb er of c lasses of the dataset and the average
amount of data p er class is given.
Alternatively, the mo del can b e trained with early stopping (ES) purely on the training
loss. All three metho ds  early stopping on the validation set accuracy, early stopping on
the training loss and training a ˝xed numb er of ep o chs are evaluated. While having more
data help ed with Asirra and CIFAR-100, the results as shown in Table 5.18 on the other
datasets are only marginally di˙erent. For CIFAR-10, training with more data did not
improve the results when the numb er of ep o chs is ˝xed, but notably improved the results
when the training loss was used as the early stopping criterion.
5.17. Regularization
Stronger regularization might even improve the results when using the training loss as an
early stopping criterion.
`
2
regularization with a weighting factor of

= 0
:
0001
is used in
all other exp eriments. While the accuracy as shown in Table 5.19 do es not show a clear
pattern, the numb er of ep o chs increases with lower mo del regularization (see Table 5.20).
2
Except d ata augmentation and test time transformations.
3
Only 1 mo del is trained due to the long training time of 581 ep o chs and 12 hours for this mo del.
4
Only 3 mo dels are i n this ensemble due to the long training time of more th an 8 hours p er mo del.
68
5.17. Regularization
Dataset
Early Stopping Fixed ep o chs
val. acc train loss
Asirra
93
:
09 % 96
:
01 %
3
96
:
01 %
CIFAR-10
89
:
86 % 91
:
75 % 88
:
88 %
CIFAR-100
67
:
03 % 71
:
01 % 69
:
08 %
HASYv2
85
:
92 % 82
:
89 %
4
85
:
05 %
MNIST
99
:
67 % 99
:
64 % 99
:
57 %
STL-10
78
:
66 % 83
:
25 % 78
:
64 %
Table 5.18.:
Comparisons of trained optim ized mo dels with e arly stopping on the validation accuracy
compared training setups without a validation set and thus more training data. The
second column uses the training loss as a stopping criterion, the third column uses a
˝xed numb er of ep o chs which is equal to the me an numb er of training ep o chs of the
mo dels with early stopping on the validation set accuracy.

Single Mo del Accuracy Ensemble of 10
Training Set Test Set Training Set Test Set

= 0
:
01 73
:
83 %
˙
= 1
:
78 58
:
94 %
˙
= 1
:
33 87
:
78 % 69
:
98 %

= 0
:
001 82
:
86 %
˙
= 0
:
89 63
:
03 %
˙
= 0
:
67 91
:
86 % 71
:
02 %

= 0
:
0001 77
:
96 %
˙
= 2
:
18 64
:
42 %
˙
= 0
:
73 81
:
44 % 67
:
03 %
Table 5.19.: Di˙erent choices of
`
2
mo del regularization applied to the optimized mo de l.

min max mean std

= 0
:
01
457 503 404.6 37.2

= 0
:
001
516 649 588.4 41.6

= 0
:
0001
579 833 696.1 79.1
Table 5.20.:
Training time in ep o chs of mo dels with early stopping on training loss by di˙erent
choices of
`
2
mo del regularization applied to the optimized mo del.
69
5. Exp erimental Evaluation
70
6. Conclusion and Outlook
This master thesis gave an extensive overview over the design patterns of CNNs in Chapter 2,
the metho ds how CNNs can b e analyzed and the principle directions of top ology learning
algorithms in Chapter 3.
Confusion Matrix Ordering (CMO), originally develop ed as a metho d to make visualizations
of confusion matrices easier to read (see Figure 5.13), was intro duced as a class clustering
algorithm in Chapter 4 and evaluated in Sections 4.2 and 5.4. The imp ortant insights are:
‹
Ordering the classes in the confusion matrix allows to display the relevant parts even
for several hundred classes.
‹
A hierarchy of classi˝ers based on the classes do es not improve the results on CIFAR-
100. There are three p ossible reasons for this:

32 px

32 px
is to o low dimensional

100 classes are not enough for this approach

More classes are always easier to distinguish if each new class comes with more
data. One reason why this might b e the case is that distinguishing the ob ject
from background has similar prop erties even for di˙erent classes.
‹
Lab el smo othing had only a minor e˙ect on the accuracy and no e˙ect on the training
time when a single base classi˝er was used to train with the smo othed lab els by an
ensemble of base classi˝ers.
A baseline mo del was de˝ned and evaluated on eight publicly available datasets. The
baselines top ology and training setup are describ ed in detail as well as its b ehavior during
training and prop erties of the weights of the trained mo del.
The in˛uence of various hyp erparameters is examined in Sections 5.5 to 5.12 for CIFAR-100.
The insights of those exp eriments are:
‹
Averaging ensembles of 10 base classi˝ers of the same architecture and trained with the
same setup consistently improve the accuracy. The amount of improvement dep ends
on the base classi˝ers, but the ensemble tends to improve the test accuracy by ab out
one p ercentage p oint.
‹
Wider networks learn in fewer ep o chs. This, however, do es not mean that the
71
6. Conclusion and Outlo ok
wall-clo ck time is lower due to increased computation in forward- and backward
passes.
‹
Batch Normalization increases the training time noticeably. For the describ ed ELU
baseline mo del it also increases accuracy, which contradicts [CUH15].
‹
The lower the batch size, the longer the time for each ep o ch of training and the less
ep o chs need to b e trained. Higher accuracy by lower batch sizes was empirically
con˝rmed. The batch size, however, can also b e to o low.
‹
An analysis of the weights of the baseline indicated that the bias of layers close to
the output layer can b e removed. This was exp erimentally con˝rmed.
‹
It could not b e con˝rmed that learned color space transformation, as describ ed
in [
MSM16
], improves the network. Neither with ELU nor with leaky recti˝ed linear
unit (LReLU) and

= 0
:
3
.
‹
It could b e con˝rmed that E LU networks gives b etter results than any other activation
function on CIFAR-100. For the character datasets MNIST and HASYv2, however,
ReLU, LReLU, PReLU, Softplus and ELU all p erformed similar.
‹
Changing the activation functions to the identity had very little impact on the HASYv2
and MNIST classi˝ers. Note that those networks are still able to learn nonlinear
decision b oundaries due to max-p o oling and
SAME
padding. For CIFAR-100, however,
the accuracy drops by
6
:
64 %
when EL U is replaced by the identity.
Based on the results of those exp eriments, an optimized classi˝er was develop ed and
evaluated on all eight datasets.
The state of the art of STL-10 was improved from
74
:
80 %
[
ZMGL15
] to
78
:
66 %
without
using the unlab eled part of the dataset. The state of the art of HASYv2 was improved
from
81
:
00 %
[
Tho17a
] to
85
:
92 %
, for GTSRB the state of the art was improved from
99
:
46 %
[
SL11
] to
99
:
51 %
, for Asirra it was improved from
82
:
7 %
[
Gol08
] to
93
:
09 %
.
1
This was mainly achieved by the combination of ELU, Drop out, ensembles, training data
augmentation and test-time transformations. The removal of the bias of layers close to the
output and re-usage of those parameters in layers close to the input as well as using
3

3
p o oling instead of
2

2
p o oling improved the baseline.
While writing this masters thesis, several related questions could not b e answered:
‹
Deep er CNNs have generally higher accuracy, if trained long enough and if over˝tting
is not a problem. But at which subsampling-level do es having more layers have the
biggest e˙ect? Can this question b e answered b efore a deep er network is trained?
‹
Is lab el smo othing helpful for noisy lab els?
1
The basel ine is b etter than the optimized mo del on Asirra and on HASYv2.
72
‹
How do es the choice of activation functions in˛uence residual architectures? Could the
results b e the same for di˙erent activation functions in architectures with hundreds
of layers?
‹
The results for the p o oling kernel were inconclusive. Larger p o oling kernels might b e
advantageous as well as fractional max p o oling [Gra15].
‹
Why is the mean weight up date (see Figure 5.8) not decreasing? Is this an e˙ect that
can and should b e ˝xed?
‹
Why is softmax so much b etter than the logistic function? Can the reason b e used to
further improve ELU?
Besides those questions, the in˛uence of optimizers on time p er ep o ch, ep o chs until
convergence, total training time, memory consumption, accuracy of the mo dels and standard
deviation of the mo dels was not evaluated. This, and the stopping criterion for training
might b e crucial for the mo dels quality.
73
74
A. Figures, Tables and Algorithms
(a)
Original image
(b)
Smo oth ing ˝lter
(c)
Laplace edge detection ˝lter
(d)
Sob el edge detection ˝lter
(e)
Prewitt edge detection ˝lter
(f )
Canny ˝lter
Figure A.1.: Examp les of image ˝lters. Best viewed in electronic form.
Layer
99-p ercentile interval
˝lter bias
1 [-0.50, 0.48] [-0.06, 0.07]
3 [-0.21, 0.19] [-0.07, 0.07]
5 [-0.20, 0.17] [-0.07, 0.05]
7 [-0.15, 0.14] [-0.05, 0.06]
9 [-0.14, 0.15] [-0.04, 0.03]
11 [-0.08, 0.08] [-0.00, 0.00]
13 [-0.08, 0.08] [-0.00, 0.00]
15 [-0.10, 0.11] [-0.01, 0.01]
Table A.1.:
99-p ercentile intervals for ˝ lter weights and bias weights by layer of a baseline mo del
trained on CIFAR-100.
75
Figure A.2.:
The distribution of bias wei ghts of a mo del without batch normalization trained on
CIFAR-100.
Algorithm 1
Simulated Annealing for minimizing Equation (4.1).
Require:
C
2
N
n

n
, steps
2
N
,
T
2
R
+
,
c
2
(0
;
1)
pro cedure
SimulatedAnnealing
(
C
, steps,
T
,
c
)
b estScore
 
accuracy
(
C
)
b estC
 
C
for
i
= 0
;
i <
steps;
i
 
i
+ 1
do
p
 
randomFloat
(0
;
1)
if
p <
0
:
5
then
.
Swap rows
i
 
randomInteger
(1
; : : : ; n
)
j
 
randomInteger
(1
; : : : ; n
)
n f
i
g
p
 
randomUniform
(0
;
1)
C
0
 
swap
(
C ; i; j
)
s
 
accuracy
(
C
0
)
if
p <
exp(
s

b estScore
T
)
then
C
 
C
0
if
s >
b estScore
then
b estScore
 
s
b estC
 
C
T
 
T

c
else
.
Move Blo ck
s
 
randomInteger
(1
; : : : ; n
)
.
Blo ck start
e
 
randomInteger
(
s; : : : ; n
)
.
Blo ck end
i
 
randomInteger
(1
; : : : ; n

(
e

s
))
.
Blo ck insert p osition
Move Blo ck (s, . . . , e) to p osition
i
return
b estM
76
Figure A.3.:
Maximum weight u p dates b etween ep o chs by layer. The mo del is the baseline mo de l,
but with layer 5 re duced to 3 ˝lters.
Function
Single mo del Ensemble of 10 Ep o chs
Training set Test set Train Test Range Mean
Identity
87
:
92 %
˙
= 0
:
40 84
:
69 %
˙
= 0
:
08 88
:
59 % 85
:
43 %
92  140 114.5
Logistic
81
:
46 %
˙
= 5
:
08 79
:
67 %
˙
= 4
:
85 86
:
38 % 84
:
60 %
58

91 77.3
Softmax
88
:
19 %
˙
= 0
:
31 84
:
70 %
˙
= 0
:
15 88
:
69 % 85
:
43 %
124  171 145.8
Tanh
88
:
41 %
˙
= 0
:
36 84
:
46 %
˙
= 0
:
27 89
:
24 % 85
:
45 %
89  123 108.7
Softsign
88
:
00 %
˙
= 0
:
47 84
:
46 %
˙
= 0
:
23 88
:
77 % 85
:
33 %
77  119 104.1
ReLU
88
:
93 %
˙
= 0
:
46
85
:
35 %
˙
= 0
:
21 89
:
35 % 85
:
95 %
96  132 102.8
Softplus
88
:
42 %
˙
= 0
:
29
85
:
16 %
˙
= 0
:
15 88
:
90 % 85
:
73 %
108  143 121.0
LReLU
88
:
61 %
˙
= 0
:
41 85
:
21 %
˙
= 0
:
05
89
:
07 % 85
:
83 %
87  117 104.5
PReLU
89
:
62 %
˙
= 0
:
41
85
:
35 %
˙
= 0
:
17
90
:
10 %
86
:
01 %
85  111 100.5
ELU
89
:
49 %
˙
= 0
:
42
85
:
35 %
˙
= 0
:
10 89
:
94 %
86
:
03 %
73  113 92.4
Table A.2.:
Test accuracy of adjusted baseline mo dels trained with di˙erent activation functions on
HASYv2. For LReLU,

= 0
:
3
was chosen.
77
Figure A.4.:
Sum of weight up dates b etwee n ep o chs by layer. The m o del is the baseline mo del, but
with layer 5 reduced to 3 ˝lters.
Function
Single mo del Ensemble of 10 Ep o chs
Training set Test set Train Test Range Mean
Identity
87
:
49 %
˙
= 2
:
50 69
:
86 %
˙
= 1
:
41 89
:
78 % 71
:
90 %
51  65 53.4
Logistic
45
:
32 %
˙
= 14
:
88 40
:
85 %
˙
= 12
:
56 51
:
06 % 45
:
49 %
38  93 74.6
Softmax
87
:
90 %
˙
= 3
:
58 67
:
91 %
˙
= 2
:
32 91
:
51 % 70
:
96 %
108  150 127.5
Tanh
85
:
38 %
˙
= 4
:
04 67
:
65 %
˙
= 2
:
01 90
:
47 % 71
:
29 %
48  92 65.2
Softsign
88
:
57 %
˙
= 4
:
00 69
:
32 %
˙
= 1
:
68 93
:
04 % 72
:
40 %
55  117 83.2
ReLU
94
:
35 %
˙
= 3
:
38 71
:
01 %
˙
= 1
:
63 98
:
20 % 74
:
85 %
52  98 75.5
Softplus
83
:
03 %
˙
= 2
:
07 68
:
28 %
˙
= 1
:
74 93
:
04 % 75
:
99 %
56  89 68.9
LReLU
93
:
83 %
˙
= 3
:
89 74
:
66 %
˙
= 2
:
11 97
:
56 % 78
:
08 %
52  120 80.1
PReLU
95
:
53 %
˙
= 1
:
92 71
:
69 %
˙
= 1
:
37 98
:
17 % 74
:
69 %
59  101 78.8
ELU
95
:
42 %
˙
= 3
:
57 75
:
09 %
˙
= 2
:
39 98
:
54 % 78
:
66 %
66  72 67.2
Table A.3.:
Test accuracy of adjusted baseline mo dels trained with di˙erent activation functions on
STL-10. For LReLU,

= 0
:
3
was chosen.
78
B. Hyperparameters
Hyp erparameters are parameters of mo dels which are not optimized automatically (e.g., by
gradient descent), but by metho ds like random search [
BB12
], grid search [
LBOM98
] or
manual search.
B.1. Preprocessing
Prepro cessing used to b e of ma jor imp ortance in machine learning. However, with the
availability of data sets with hundreds of examples p er class and the p ossibility of CNNs to
learn features themselves, most mo dels to day rely on raw pixel values. The only common
prepro cessing is size normalization. In order to get a ˝xed input-size for a CNN, the
following pro cedure can b e used:
‹
Take one or multiple crops of the image which have the desired asp ect ratio.
‹
Scale the crop(s) to the desired size.
‹
In training, all crops can b e used indep endently. In testing, all crops can b e passed
through the network and the output probability distributions can get fusioned, for
example by averaging.
Other prepro cessing metho ds are:
‹
Color space transformations (RGB, HSV, etc.)
‹
Mean subtraction
‹
Standardization of pixel-values to
[0
;
1]
by dividing through
255
(used by [HLW16])
‹
Dimensionality reduction

Principal comp onent analysis (PCA): An unsup ervised linear transformation
which can b e learned in the ˝rst hidden layer. It is hence doubtful if PCA
improves the network.

Linear discriminant analysis (LDA)
‹
Zero Comp onents Analysis (ZCA) whitening (used by [KH09])
79
B.2. Data augmentation
Data augmentation techniques aim at making arti˝cially more data from real data items by
applying invariances. For computer vision, they include:
Name Augmentation Factor Used by
Horizontal ˛ip
2
[KSH12, WYS
+
15]
Vertical ˛ ip
2
[DWD15]
1
Rotation
˘
40
(

= 20
) [DSRB 14]
Scaling
˘
14
(

2
[0
:
7
;
1
:
4]
) [DSRB14]
Crops
32
2
= 1024
[KSH12, WYS
+
15]
Shearing [Gra15]
GANs [B CW
+
17]
Brightness
˘
20
(

2
[0
:
5
;
1
:
5]
) [How13]
Hue
51
(

= 0
:
1
) [MRM15, DSRB14]
Saturation
˘
20
(

= 0
:
5
) [DSRB14]
Contrast
˘
20
(

2
[0
:
5
;
1
:
5]
) [How13]
Channel shift [KSH12]
Table B.1.:
Overview of data augmentation techniques. The augmentation factor is calculated for
typical situations. For example, the augmentation fac tor for random crops i s calculated
for
256 px

256 px
images which are cropp ed to
224 px

224 px
.
Taking several scales if the original is of higher resolution than desired is another technique.
Combinations of the techniques ab ove can also b e applied. Please note that the order of
op erations do es matter in many cases and hence the order is another augmentation factor.
Less common, but also reasonable are:
‹
Adding noise
‹
Elastic deformations
‹
Color casting (used by [WYS
+
15])
‹
Vignetting (used by [WYS
+
15])
‹
Lens distortion (used by [WYS
+
15])
1
Vertical ˛ipping combined with
180

rotation is equivalent to horizontal ˛ippin g
80
B.3. Initialization
Weight initializations are usually chosen to b e small and centered around zero. One way to
characterize many initialization schemes is by
w
˘

 U
[

1
;
1] +

 N
(0
;
1) +

with
 ;  ; 

0
Table B.2 shows six commonly used weight initialization schemes. Several schemes use the
same idea, that unit-variance is desired for each layer as the training converges faster [
IS15
].
Name
  
Reference
Constant

= 0

= 0


0
used by [ZF14]
Xavier/Glorot uniform

=
q
6
n
in
+
n
out

= 0

= 0
[GB10]
Xavier/Glorot normal

= 0

=

2
(
n
in
+
n
out
)

2

= 0
[GB10]
He

= 0

=
2
n
in

= 0
[HZRS15b]
Orthogonal 

= 0
[SMG13]
LSUV 

= 0
[MM15]
Table B.2.: Weight initialization schemes of the form
w
˘

 U
[

1
;
1] +

 N
(0
;
1) +

.
n
in
; n
out
are the numb er of units in the previous layer and the next layer. Typically,
biases are initializ ed with constant 0 and weights by one of the other schemes to prevent
unit-coadaptation. However, drop out makes it p ossible to u se constant initialization for
all parameters.
LSUV and Orthogonal initialization cannot b e d escrib ed with this simple pattern.
B.4. Objective function
For classi˝cation tasks, the cross-entropy
E
C E
(
W
) =

X
x
2
X
K
X
k
=1
[
t
x
k
log (
o
x
k
) + (1

t
x
k
) log (1

o
x
k
)]
is by far the most commonly used ob jective function (e.g., used by [
ZF14
]). In this equation,
X
is the set of training examples,
K
is the numb er of classes,
t
x
k
2 f
0
;
1
g
indicates if the
training example
x
is of class
k
,
o
x
k
is the output of the classi˝er for the training example
x
and class
k
.
However, regularization terms weighted with a constant

2
(0
;
+
1
)
are sometimes added:
‹
LASSO:
`
1
(e.g., used in [HPTD15])
‹
Weight decay:
`
2
(e.g.,

= 0
:
0005
as in [MSM16])
‹
Orthogonality regularization (
j
(
W
T

W

I
)
j
, see [VTKP17])
81
B.5. Optimization Techniques
Most relevant optimization techniques for CNNs are based on SGD, which up dates the
weights according to the rule
w
j i
 
w
j i
+ 
w
j i
with

w
j i
=


@ E
x
@ w
j i
where

2
(0
;
1)
, typically
0
:
01
(e.g., [MSM16]), is called the
learning rate
.
A slight variation of SGD is mini-batch gradient descent with the mini-batch
B
(typically
mini-batch sizes are
j
B
j 2 f
32
;
64
;
128
;
256
;
512
g
, e.g. [
ZF14
]). Larger mini-batch sizes
lead to sharp minima and thus p o or generalization [
KMN
+
16
]. Smaller mini-batch sizes
lead to longer training times due to computational overhead and to more training steps due
to gradient noise.
w
j i
 
w
j i
+ 
w
j i
with

w
j i
=


@ E
B
@ w
j i
Nine variations which adjust the learning rate during training are:
‹
Momentum:
w
(
t
+1)
j i
 
w
(
t
)
j i
+ 
w
(
t
+1)
j i
with

w
(
t
+1)
j i
=


@ E
B
@ w
j i
+


w
(
t
)
j i
with

2
[0
;
1]
, typically
0
:
9
(e.g., [ZF14, MSM16])
‹
Adagrad [DHS11]
‹
RProp and the mini-batch version RMSProp [TH12]
‹
Adadelta [Zei12]
‹
Power Scheduling [
Xu11
]:

(
t
) =

(0)(1 +
a

t
)

c
, where
t
2
N
0
is the training step,
a; c
are constants.
‹
Performance Scheduling [
SHY
+
13
]: Measure the error on the cross validation set and
decrease the learning rate when the algorithms improvement is b elow a threshold.
‹
Exp onential Decay Learning Rate [
SHY
+
13
]:

(
t
) =

(0)

10

t
k
where
t
2
N
0
is the
training step,

(0)
is the initial learning rate,
k
2
N

1
is the numb er of training steps
until the learning rate is decreased by
1
10
th.
‹
Newb ob Scheduling [
new00
]: Start with Performance Scheduling, then use E xp onential
Decay Scheduling.
‹
Adam and AdaMax [KB14]
82
‹
Nadam [Doz15]
Some of those are explained in [Rud16].
Other ˝rst-order gradient optimization metho ds are:
‹
Quickprop [Fah88]
‹
Nesterov Accellerated Momentum (NAG) [Nes83]
‹
Conjugate Gradient metho d [
Cha92
]: Combines a line search for the step size with
the gradients direction.
Higher-order gradient metho ds like Newtons metho d or quasi-Newton metho ds like BFGS
and L-BFGS need the inverse of the Hessian matrix which is intractable for to day's CNNs.
However, there are alternatives which do not use gradient information:
‹
Genetic algorithms such as NeuroEvolution of Augmenting Top ologies (NEAT) [
SM02
]
‹
Simulated Annealing [vLA87]
‹
Twiddle: A lo cal hill-climbing algorithm explained by Sebastian Thrun and describ ed
on [Tho14b]
There are also approaches which learn the optimization algorithm [ADG
+
16, LM16].
83
B.6. Network Design
CNNs have the following hyp erparameters:
‹
Depth
: The numb er of layers
‹
Width
: The numb er of ˝lters p er layer
‹
Layer and blo ck connectivity graph
‹
Layer and blo ck hyp erparameters
:

Activation Functions as shown in Table B.3

For more, see Sections 2.2 and 2.3.
Name Function
'
(
x
)
Range of Values
'
0
(
x
)
Used by
Sign function
y
8
<
:
+1
if
x

0

1
if
x <
0
f 
1
;
1
g
0
[KS02]
Heaviside
step function
y
8
<
:
+1
if
x >
0
0
if
x <
0
f
0
;
1
g
0
[MP43]
Logistic function
1
1+
e

x
[0
;
1]
e
x
(
e
x
+1)
2
[DJ99]
Tanh
e
x

e

x
e
x
+
e

x
= tanh(
x
) [

1
;
1] sech
2
(
x
)
[LBBH98, Tho14a]
ReLU
y
max(0
; x
) [0
;
+
1
)
8
<
:
1
if
x >
0
0
if
x <
0
[KSH12]
LReLU
y
2
(PReLU)
'
(
x
) = max(
 x; x
) (

;
+
1
)
8
<
:
1
if
x >
0

if
x <
0
[MHN13, HZRS15b]
Softplus
log (
e
x
+ 1) (0
;
+
1
)
e
x
e
x
+1
[DBB
+
01, GBB11]
ELU
8
<
:
x
if
x >
0

(
e
x

1)
if
x

0
(

;
+
1
)
8
<
:
1
if
x >
0
 e
x
otherwise
[CUH15]
Softmax
z
o
(
x
)
j
=
e
x
j
P
K
k
=1
e
x
k
[0
;
1]
K
o
(
x
)
j

P
K
k
=1
e
x
k

e
x
j
P
K
k
=1
e
x
k
[KSH12, Tho14a]
Maxout
z
o
(
x
) = max
x
2
x
x
(

;
+
1
)
8
<
:
1
if
x
i
= max
x
0
otherwise
[GWFM
+
13]
Table B.3.:
Ove rvie w of activation functions. Func tion s marked with
y
are not di˙erentiable at 0
and functions marked w ith
z
op erate on all elements of a layer simultaneously. The
hyp erparameters

2
(0
;
1)
of Leaky ReL U and ELU are typically

= 0
:
01
. Other
activation function like randomized leaky ReLU s exist [
XWCL15
], but are far less
commonly used.
Some fu nctions are s mo othed versions of others, like the logis tic function for the
Heaviside step function, tanh for the sign function, softplus for ReLU.
Softmax is the standard activation function for the last layer of a classi˝cation network
as it pro duces a probability distribu tion. See Figure B.1 for a plot of some of them.
2

is a hyp erparameter in leaky ReLU, but a learnable parameter in th e parametric ReLU function.
84

2
:
0

1
:
5

1
:
0

0
:
5
0
:
5
1
:
0
1
:
5
2
:
0

1
:
0

0
:
5
0
:
5
1
:
0
1
:
5
2
:
0
x
y
'
1
(
x
) =
1
1+
e

x
'
2
(
x
) = tanh(
x
)
'
3
(
x
) = max(0
; x
)
'
4
(
x
) = log (
e
x
+ 1)
'
5
(
x
) = max(
x; e
x

1)
Figure B.1.:
Activation functions plotted in
[

2
;
+2]
.
tanh
and ELU are able to pro duce n egative
numb ers. The im age of ELU , ReLU and Softplus is not b ound on the p ositive side,
whereas
tanh
and the logistic function are al ways b elow 1.
B.7. Regularization
Regularization techniques aim to make the ˝tted function smo other and reduce over˝tting.
Regularization techniques are:
‹
`
1
,
`
2
, and Orthogonality regularization: See App endix B.4
‹
Max-norm regularization (e.g. used ins [SHK
+
14])
‹
Drop out (intro duced in [
SHK
+
14
]), DropConnect (see [
WZZ
+
13
]), Sto chastic Depth
(see [HSL
+
16])
‹
Feature scale clipping (see [ZF14])
‹
Data augmentation (according to [ZBH
+
16])
‹
Global average p o oling (according to [ZKL
+
15])
‹
Dense-Sparse-Dense training (see [HPN
+
16])
‹
Soft targets (see [HVD15])
85
86
C. Calculating Network Characteristics
C.1. Parameter Numbers
‹
A fully connected layer with
n
no des,
k
inputs has
n

(
k
+ 1)
parameters. The
+1
is
due to the bias.
‹
A convolutional layer
i
with
k
i
˝lters of size
n

m
b eing applied to
k
i

1
feature maps
has
k
i

k
i

1
(
n

m
+ 1)
parameters. The
+1
is due to the bias.
‹
A fully connected layer with
n
no des after
k
feature maps of size
m
1

m
2
has
n

(
k

m
1

m
2
+ 1)
parameters.
‹
A dense blo ck with a depth of
L
, a growth rate of
n
and
3

3
˝lters has
L
+
n

3
2
+
3
2

n
2
P
L
i
=0
(
L

i
) =
L
+ 9
n
+ 9
n
2
L
2

L
2
parameters.
According to [
HPTD15
], AlexNet has 60 million parameters which is roughly the numb er
calculated in Table D.2.
C.2. FLOPs
The FLOPs of a layer dep end on the implementation, the compiler and the hardware. Hence
the following numb er are only giving rough estimates.
In the following,
n
'
denotes the numb er of FLOPs to compute the non-linearity
'
. For
simplicity,
n
'
= 5
was chosen.
‹
A fully connected layer with
n
no des and
k
inputs has to calculate
'
(
W

x
+
b
)
with
W
2
R
n

k
,
x
2
R
k

1
,
b
2
R
n

1
. It hence needs ab out
n

(
k
+ (
k

1) + 1) = 2
nk
additions / multiplications b efore the non-linearity
'
is calculated. The total numb er
of FLOPs is
2

n

k
+
n

n
'
.
‹
In the following, biases are ignored. A convolutional layer with
k
i
˝lters of size
n

m
b eing applied to
k
i

1
˝lter maps of size
w

h
results in
k
i
˝lter maps of size
w

h
if
padding is applied. For each element of each ˝lter map,
n

m

k
i

1
multiplications and
(
n

m

k
i

1

1)
additions have to b e made. This results in
(2
nmk
i

1

1)

(
k
i

w

h
)
op erations. The total numb er of FLOPs is
(2

n

m

k
i

1

1)

(
k
i

w

h
) +
k
i

w

h

n
'
.
This is, of course, a naive way of calculating a convolution. There are other ways of
calculating convolutions [LG16].
87
‹
A fully connected layer with
n
no des after
k
feature maps of size
w

h
needs
2
n
(
k

w

h
)
FLOPs. The total numb er of FLOPs is
2
n

(
k

w

h
) +
n

n
'
.
‹
As Drop out is only calculated during training, the numb er of FLOPs was set to 0.
‹
The numb er of FLOPs for max p o oling is dominated by the numb er of p ositions to
which the p o oling kernel is applied. For a feature map of size
w

h
a max p o oling
˝lter with stride
s
gets applied
w

h
s
2
. The numb er of F LOPs p er application dep ends
on the kernel size. A
2

2
kernel is assumed to need 5 FLOPs.
‹
The numb er of FLOPs for Batch Normalization is the same as the numb er of its
parameters.
Here are some references which give information for the FLOPs:
‹
AlexNet

1.5B in total [HPTD15].

725M in total [KPY
+
15].

3300M in total in Table D.2
‹
VGG-16:

15484M in total [HPTD15].

31000M in total in Table D.3.
‹
Go ogleNet: 1566M in total [HP TD 15].
One can see that the numb ers are by a factor of 2 up to a factor of 4 di˙erent for the same
network.
C.3. Memor y Footprint
The memory fo otprint of CNNs determines when networks can b e used at all and if they
can b e trained e˚ciently. In order to b e able to train CNNs e˚ciently, one weight up date
step has to ˝t in the memory of the GPU. This includes the following:
‹
Activations
: All activations of one mini-batch in order to calculate the gradients
in the backward pass. This is the numb er of ˛oats in the feature maps of all weight
layers combined.
‹
Weights
‹
Optimization algorithm
: The optimization algorithm intro duces some overhead.
For example, Adam stores two parameters p er weights.
At inference time, every two consecutive layers have to ˝t into memory. When the forward
pass of layer A to layer B is calculated, the memory can b e freed if no skip connections are
used.
88
D. Common Architectures
In the following, some of the most imp ortant CNN architectures are explained. Understand-
ing the development of these architectures helps understanding critical insights the machine
learning community got in the past years for convolutional networks for image recognition.
It starts with LeNet-5 from 1998, continues with AlexNet from 2012, VGG-16 D from
2014, the Inception mo dules v1 to v3 as well as ResNets in 2015. The recently develop ed
Inception-v4 is also covered.
The summation row gives the sum of all ˛oats for the output size column. This allows
conclusions ab out the maximum mini-batch size which can b e in memory for training.
89
D.1. LeNet-5
One of the ˝rst CNNs used was LeNet-5 [
LBBH98
]. LeNet-5 uses two times the common
pattern of a single convolutional layer with
tanh
as a non-linear activation function followed
by a p o oling layer and three fully connected layers. One fully connected layer is used to
get the right output dimension, another one is necessary to allow the network to learn a
non-linear combination of the features of the feature maps.
Its exact architecture is shown in Figure D.1 and describ ed in Table D.1. It reaches a test
error rate of
0
:
8 %
on MNIST.
Figure D.1.: Architecture of LeNe t-5 as shown in [LBBH98].
# Typ e Fi lters @
Patch size / stride
Parameters FLOPs Output size
Input 0 0 1 @ 32

32
1 Convolution 6 @
5

5

1
/ 1 156 307 800
6
@
28

28
2 Scaled average p o oling
2

2
/ 2 2 336 6 @ 14

14
3 Convolution 16 @
5

5

6
/ 1 2 416
942 400
16 @ 10

10
4 Scaled average p o oling
2

2
/ 2 2 1 600 16 @ 5

5
5 Fully Connected 120 neurons
48 120
240 000 120
6 Fully Connected 84 neurons 10 164 20 580 84
7 Fully Connected (output) 10 neurons 850 1 730 10
P
61 710 15 144 446 9118
Table D.1.:
LeNet-5 architecture: After layers 1, 3, 5 and 6 the
tanh
activation function is applied.
After layer 7, the softmax function is applied. One can see that convolutional layer
need much fewer parameters, but an order of magnitude more FLOPs p er parameter
than fully conn ected layers.
90
D.2. AlexNet
The ˝rst CNN which achieved ma jor improvements on the ImageNet dataset was AlexNet [
KSH12
].
Its architecture is shown in Figure D.2 and describ ed in Table D.2. It has ab out
60

10
6
param-
eters. A trained AlexNet can b e downloaded at www.cs.toronto.edu/guerzhoy/tf_alexnet.
Note that the uncompressed size is at least
60 965 224 oats

32
bit
oat
ˇ
244 MB
.
Figure D.2.:
Architecture of AlexNet as s hown in [
KSH12
]: C onvolutional Layers are followe d
by p o oling layers multiple times. At the end, a fully connected network is applied.
Conceptually, it is identical to the architecture of LeNet-5 (see Figure D.1).
# Typ e Filters @
Patch size / stride
Parameters FLOPs Output size
Input 3 @
224

224
1 Convolution 96 @
11

11

3
/ 4
34 944 211 M
96 @
55

55
LCN
12 M
96 @
55

55
2 Max p o oling
3

3
/ 2
0 301 k
96 @
27

27
3 Convolution 256 @
5

5

48
/ 1
307 456
448 M
256 @
13

13
LCN
3 M
256 @
13

13
4 Max p o oling
3

3
/ 2
0 50 k
256 @
13

13
5 Convolution 384 @
3

3

256
/ 1
885 120 299 M
384 @
13

13
7 Convolution 384 @
3

3

192
/ 1
663 936 224 M
384 @
13

13
9 Convolution 256 @
3

3

192
/ 1
442 624 150 M
256 @
13

13
10 Max p o oling
3

3
/ 2
0 50 k
256 @
6

6
11 FC 4096 neurons
37 752 832
75 M
4096
12 FC 4096 neurons
16 781 312 34 M
4096
13 FC 1000 neurons
4 097 000 8 M
1000
P
60 965 224 3300 M 1 122 568
Table D.2.:
AlexNet architecture: One sp ecial case of Ale xNe t is grouping of convolutions due to
computational restrictions at the time of its developme nt. This also reduces the numb er
of parameters and allows parallel computation on separate GPUs. However, to make
the architecture easier to compare, this grouping was ignored for the parameter count.
The FLOPs are take n from [
HPTD15
] and combined w ith rough estimates for Lo cal
Contrast Norm alization and max p o oling.
The calculated nu mb er of parameters was checked against the downloaded version. It
also has
60 965 224
parameters.
91
D.3. VGG-16 D
Another widespread architecture is the VGG-16 (D) [
SZ14
]. VGG comes from the
V
isual
G
eometry
G
roup in Oxford which develop ed this architecture. It has
16
layers which can
learn parameters. A ma jor di˙erence compared to AlexNet is that VGG-16 uses only
3

3
˝lters and is much deep er. A visualization of the architecture is shown in Figure D.3 and a
detailed textual description is given in Table D.3.
A trained VGG-16 D for Tensor˛ow can b e downloaded at
https://gi thub
:
com/machris aa/
tensorflow- vgg
. Note that the uncompressed size is at least
138 357 544 oats

32
bit
oat
ˇ
520 MB
. The downloaded Numpy binary ˝le
npz
needs
553 MB
without compression and
514 MB
with compression.
224

224
Input
C
64@3

3
=
1
C
64@3

3
=
1
112

112
max p o oling
2

2
=
1
C
128@3

3
=
1
C
128@3

3
=
1
56

56
max p o oling
2

2
=
1
C
256@3

3
=
1
C
256@3

3
=
1
C
256@3

3
=
1
28

28
max p o oling
2

2
=
1
C
512@3

3
=
1
C
512@3

3
=
1
C
512@3

3
=
1
14

14
max p o oling
2

2
=
1
C
512@3

3
=
1
C
512@3

3
=
1
C
512@3

3
=
1
7

7
max p o oling
2

2
=
1
Fully Connected 4096
Drop out,
p
= 0
:
5
Fully Connected 4096
Drop out,
p
= 0
:
5
Fully Connected 1000
Figure D.3.:
Architecture of VGG-16 D.
C
512@3

3
=
1
is a convolutional layer with 512 ˝lters of
kernel s ize
3

3
with stride 1. All convolutional layers use
SAME
padding.
92
# Typ e Filters @
Patch size / stride
Parameters FLOPs Output size
Input 3 @
224

224
1 Convolution 64 @
3

3

3
/ 1
1 792 186 M
64 @
224

224
2 Convolution 64 @
3

3

64
/ 1
36 928
3712 M
64 @
224

224
Max p o oling
2

2
/ 2
0 2 M
64 @
112

112
3 Convolution 128 @
3

3

64
/ 1
73 856 1856 M
128 @
112

112
4 Convolution 128 @
3

3

128
/ 1
147 584 3705 M
128 @
112

112
Max p o oling
2

2
/ 2
0 1 M
128 @
56

56
5 Convolution 256 @
3

3

128
/ 1
295 168 1853 M
256 @
56

56
6 Convolution 256 @
3

3

256
/ 1
590 080 3703 M
256 @
56

56
7 Convolution 256 @
3

3

256
/ 1
590 080 3703 M
256 @
56

56
Max p o oling
2

2
/ 2
0
<
1 M
256 @
28

28
8 Convolution 512 @
3

3

256
/ 1
1 180 160 1851 M
512 @
28

28
9 Convolution 512 @
3

3

512
/ 1
2 359 808 3701 M
512 @
28

28
10 Convolution 512 @
3

3

512
/ 1
2 359 808 3701 M
512 @
28

28
Max p o oling
2

2
/ 2
0
<
1 M
512 @
14

14
11 Convolution 512 @
3

3

512
/ 1
2 359 808 925 M
512 @
14

14
12 Convolution 512 @
3

3

512
/ 1
2 359 808 925 M
512 @
14

14
13 Convolution 512 @
3

3

512
/ 1
2 359 808 925 M
512 @
14

14
Max p o oling
2

2
/ 2
0
<
1 M
512 @
7

7
14 FC 4096 neurons
102 764 544
206 M
4096
Drop out
0
0 4096
15 FC 4096 neurons
16 781 312 34 M
4096
Drop out
0
0 4096
16 FC 1000 neurons
4 097 000 8 M
1000
P
138 357 544 31 000 M 15 245 800
Table D.3.:
VGG-16 D architecture: The authors chose to give only layers a numb er which have
learnable parameters. All convolutions are zero padded to prevent size changes and
use ReLU activation fu nctions. The channels mean is su btrac ted from each pixel as
a prepro cessing s tep (

103
:
939
;

116
:
779
;

123
:
68
). As Drop out is only c alculated
during training time, the numb e r of FLOPs is 0. The drop out probability is
0
:
5
.
The calculated nu mb er of parameters was checked against the downloaded version. It
also has
138 357 544
parameters.
93
D.4. GoogleNet, Inception v2 and v3
The large numb er of parameters and op erations is a problem when such mo dels should get
applied in practice to thousands of images. In order to reduce the computational cost while
maintaining the classi˝cation quality, Go ogleNet [
SLJ
+
15
] and the Inception mo dule were
develop ed. The Inception mo dule essentially only computes
1

1
˝lters,
3

3
˝lters and
5

5
˝lters in parallel, but applied b ottleneck
1

1
˝lters b efore to reduce the numb er of
parameters. It is shown in Figure D.4.
Figure D.4.: Inception mo dule
Image source: [SLJ
+
15]
Compared to Go ogleNet, Inception v2 [
SVI
+
15
] removed the
5

5
˝lters and replaced
them by two successive layers of
3

3
˝lters. A visualization of an Inception v2 mo dule
is given in Figure D.5. Additionally, Inception v2 applies successive asymmetric ˝lters to
approximate symmetric ˝lters with fewer parameters. The authors call this approach
˝lter
factorization
.
Inception v3 intro duced Batch Normalization to the network [SVI
+
15].
Figure D.5.: Inception v2 mo dule
Image source: [SVI
+
15]
94
D.5. Inception-v4
Inception-v4 as describ ed in [
SIV16
] consists of four main building blo cks: The stem,
Inception A, Inception B and Inception C. To quote the authors: Inception-v4 is a deep er,
wider and more uniform simpli˝ed architecture than Inception-v3. The stem, Reduction A
and Reduction B use max-p o oling, whereas Inception A, Inception B and Inception C use
average p o oling. The stem, mo dule B and mo dule C use separable convolutions.
#

Typ e Parameters Output size
Input 3 @
299

299
1 Stem
605 728
384 @
35

35
2
4

Inception A
317 632
384 @
35

35
3 Reduction A
2 306 112
1024 @
17

17
4
7

Inception B
2 936 256
1024 @
17

17
5 Reduction B
2 747 392
1536 @
8

8
6
3

Inception C
4 553 088
1536 @
8

8
Global Average Po oling 0 1536 @
1

1
Drop out (p=0.8) 0 1536 @
1

1
7 Softmax
1 537 000
1000
P
42 679 816
Table D.4.: Inception-v4 network.
95
96
E. Datasets
Well-known b enchmark datasets for classi˝cation problems in computer vision are listed
in Table E.1. The b est results known to me are given in Table E.2. However, every semantic
segmentation dataset (e.g., PASCAL VOC) can also b e used to b enchmark image classi˝ers
using Algorithm 2.
Database
Image Resolution
(width

height)
Numb er
of
Images
Numb er
of
Classes
Channels Data source
MNIST
28 px

28 px 70 000
10 1 [YL98, LBBH98]
HASYv2
32 px

32 px 168 233
369 1 [Tho17a]
SVHN
32 px

32 px 630 420
10 3
[
NWC
+
11b
],
[
NWC
+
11a
]
CIFAR-10
32 px

32 px 60 000
10 3 [Kri, KH09]
CIFAR-100
32 px

32 px 60 000
100 3 [Kri, KH09]
STL-10
96 px

96 px 13 000
10 3 [CLN11, CLN10]
Caltech-101
(80 px

3481 px)

(92 px

3999 px)
9144
102 3 [FFP03, FFFP06]
Caltech-256
(75 px

7913 px)

(75 px

7913 px)
30 607
257 3 [Gri06, GG07]
ILSVRC 2012
1
(8 px

9331 px)

(10 px

6530 px)
1
:
2

10
6
1000
3 [Ima12, RDS
+
14]
Places365
2
(290
px

3158
px
)

(225
px

2630
px
)
1
:
8

10
6
365 3 [Zho16, ZKL
+
16]
GTSRB
(25 px

266 px)

(25 px

232 px)
51 839
43 3 [SSSI, SSSI12]
Asirra
3
(4 px

500 px)

(4 px

500 px)
25 000
2 3 [Asi17, EDHS07]
Graz-02
480 px

640 px
and
640 px

480 px
1096
3 3 [Mar08, MS07]
Table E.1.:
An overview over publicly available image databases for classi˝cation. The numb er
of images row gives the su m of the training and the test images. Some datasets, like
SVHN, have additional unlab eled data which is not given in this table.
1
ImageNet Large Scale Visual Recognition Comp etition
2
The dimensions are only calculated for the valid ati on set.
3
Asirra is a CAPTCHA created by Microsoft and was used in the Cats vs Dogs comp etition on Kaggle
97
Dataset Mo del typ e / name Result Score
Achieved /
Claimed by
MNIST 
0
:
21 %
error [WZZ
+
13]
HASYv2 TF-CNN
81
:
00 %
accuracy [Tho17a]
SVHN DenseNet (
k
= 24
)
1
:
59 %
error [HLW16]
CIFAR-10 DenseNet-BC (
k
= 40
)
3
:
46 %
error [HLW16]
CIFAR-100 WRN-28-10
16
:
21 %
error [LH16]
STL-10 SWWAE-4layer
74
:
80 %
accuracy [ZMGL15]
Caltech-101 SPP-net (pretrained)
93
:
42 %

0
:
5 %
accuracy [HZRS14]
Caltech-256 ZF-Net (pretrained)
74
:
2 %

0
:
3 %
accuracy [ZF14]
ImageNet 2012 ResNet ensemble
3
:
57 %
Top-5 error [HZRS15a]
GTSRB MCDNN
99
:
46 %
accuracy [SL11]
Asirra SVM
82
:
7 %
accuracy [Gol08]
Graz-02 Optimal NBNN
78
:
98 %
accuracy [BMDP10]
Table E.2.: An overview over state of the art results achieved in com puter vision datasets.
Algorithm 2
Create a classi˝cation dataset from a semantic segmentation dataset
Require:
Semantic segmentation dataset (
D
S
)
pro cedure
CreateDataset
(Annotated dataset
D
S
)
D
C
 
List
w
 
desired image width
h
 
desired image height
for
Image and asso ciated lab el
(
x; y
)
in
D
S
do
i
 
randint
(0
; L:
width

w
)
j
 
randint
(0
; L:
height

h
)
c
L
 
crop
(
y ;
(
i; j
)
;
(
i
+
w ; j
+
h
))
if
at least 50% of
s
are of one class
then
c
I
 
crop
(
x;
(
i; j
)
;
(
i
+
w ; j
+
h
))
D :
append
((
c
I
; c
L
))
return
(
D
C
)
98
F. List of Tables
2.1 Po oling typ es . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.1 Baseline architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
5.2 Baseline mo del evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
5.3 Baseline mo del sp eed comparison . . . . . . . . . . . . . . . . . . . . . . . . 40
5.4 Clustering errors for sp ectral clustering and CMO on CIFAR-100 . . . . . . 52
5.5 Di˙erences in sp ectral clustering and CMO. . . . . . . . . . . . . . . . . . . 52
5.6 Accuracies for hierarchy of classi˝ers on CIFAR-100 . . . . . . . . . . . . . . 53
5.7 Parameters of mo dels with increased capacity . . . . . . . . . . . . . . . . . 54
5.8 Training time for mo dels with increased capacity . . . . . . . . . . . . . . . 54
5.9 Baseline mo del training time . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.10 Activation function prop erties . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.11 Activation function evaluation results on CIFAR-100 . . . . . . . . . . . . . 63
5.12 Activation function timing results on CIFAR-100 . . . . . . . . . . . . . . . 63
5.13 Activation function evaluation results on MNIST . . . . . . . . . . . . . . . 64
5.14 Optimized architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
5.15 Optimized mo del evaluation results . . . . . . . . . . . . . . . . . . . . . . . 67
5.16 Optimized mo del sp eed comparison . . . . . . . . . . . . . . . . . . . . . . . 67
5.17 Optimized mo del mean training ep o chs . . . . . . . . . . . . . . . . . . . . . 68
5.18 Optimized mo del trained with early stopping vs training with more data . . 69
5.19 Mo del regularization with early stopping on training loss . . . . . . . . . . . 69
5.20 Mo del regularization with early stopping on training loss - Training time . . 69
A.1 99-p ercentile intervals for ˝lter weights on CIFAR-100 . . . . . . . . . . . . 75
A.2 Activation function evaluation results on HASYv2 . . . . . . . . . . . . . . . 77
A.3 Activation function evaluation results on STL-10 . . . . . . . . . . . . . . . 78
B.1 Data augmentation techniques . . . . . . . . . . . . . . . . . . . . . . . . . . 80
B.2 Weight initialization schemes . . . . . . . . . . . . . . . . . . . . . . . . . . 81
B.3 Activation functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
D.1 LeNet-5 architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
D.2 AlexNet architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
D.3 VGG-16 D architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
D.4 Inception-v4 network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
99
E.1 Image Benchmark datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
E.2 State of the Art results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
100
G. List of Figures
2.1 Application of a single image ˝lter (Convolution) . . . . . . . . . . . . . . . 3
2.2 Application of a convolutional layer . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Max p o oling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.4 ResNet mo dule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.5 Aggregation blo ck . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.6 Dense blo ck . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.7 Validation curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.8 Validation curve with plateaus . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.9 Learning curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.10 Occlusion analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.11 Filter visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.1 Cascade-correlation network . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.1 Class Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.1 Baseline architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
5.2 Baseline mo del ˝lter weight distribution . . . . . . . . . . . . . . . . . . . . 42
5.3 Baseline mo del bias weight distribution . . . . . . . . . . . . . . . . . . . . . 42
5.4 Baseline mo del

distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 43
5.5 Baseline mo del

distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 43
5.6 Baseline mo del ˝lter weight range distribution . . . . . . . . . . . . . . . . . 44
5.7 Baseline mo del CIFAR-100 validation accuracy . . . . . . . . . . . . . . . . 45
5.8 Baseline Weight up dates (mean) . . . . . . . . . . . . . . . . . . . . . . . . 46
5.9 Baseline Weight up dates (maximum) . . . . . . . . . . . . . . . . . . . . . . 47
5.10 Baseline Weight up dates (sum) . . . . . . . . . . . . . . . . . . . . . . . . . 47
5.11 Confusion matrices for CIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . 48
5.12 Confusion matrices for GTSRB . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.13 Confusion matrices for HASYv2 . . . . . . . . . . . . . . . . . . . . . . . . . 50
5.14 Confusion matrix of CIFAR-100 . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.15 Mean weight up dates of mo del with b ottleneck . . . . . . . . . . . . . . . . 55
5.16 Optimized architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
A.1 Image F ilters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
A.2 Bias weight distribution without BN . . . . . . . . . . . . . . . . . . . . . . 76
101
A.3 Maximum weight up dates of baseline with b ottleneck . . . . . . . . . . . . . 77
A.4 Sum of weight up dates of baseline with b ottleneck . . . . . . . . . . . . . . 78
B.1 Activation functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
D.1 LeNet-5 architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
D.2 AlexNet architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
D.3 VGG-16 D architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
D.4 Inception mo dule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
D.5 Inception v2 mo dule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
102
H. Bibliography
[AAB
+
16]
M. Abadi, A. Agarwal
et al.
, Tensor˛ow: Large-scale machine learning on
heterogeneous distributed systems,
arXiv preprint arXiv:1603.04467
, Mar.
2016. [Online]. Available: https://arxiv
:
org/abs/1603
:
04467
[ABKS99]
M. Ankerst, M. M. Breunig
et al.
,  OPTICS: Ordering p oints to identify the
clustering structure, in
ACM Sigmod record
, vol. 28, no. 2. ACM, 1999, pp.
4960.
[ADG
+
16]
M. Andrychowicz, M. Denil
et al.
, Learning to learn by gradient descent by
gradient descent, in
Advances in Neural Information Processing Systems 29
(NIPS)
, D. D. Lee, M. Sugiyama
et al.
, Eds. Curran Asso ciates, Inc., Mar.
2016, pp. 39813989. [Online]. Available: http://pap ers
:
nips
:
cc/pap er/6461-
learning- to- learn- by- gradient- descent- by- gradient- descent
:
p df
[AM15]
M. T. Alexander Mordvintsev, Christopher Olah, Inceptionism:
Going deep er into neural networks, Jun. 2015. [Online]. Avail-
able: https://research
:
go ogleblog
:
com/2015/06/inceptionism- going- deep er-
into- neural
:
html
[Asi17]
Kaggle cats and dogs dataset, Oct. 2017. [Online]. Available: https:
//www
:
microsoft
:
com/en- us/download/details
:
aspx?id=54765
[BB12]
J. Bergstra and Y. Bengio, Random search for hyp er-parameter optimization,
Journal of Machine Learning Research
, vol. 13, no. Feb, pp. 281305,
Feb. 2012. [Online]. Available: http://jmlr
:
csail
:
mit
:
edu/pap ers/volume13/
b ergstra12a/b ergstra12a
:
p df
[BCW
+
17]
J. Bao, D. Chen
et al.
,  CVAE -GAN: Fine-grained image generation through
asymmetric training,
arXiv preprint arXiv:1703.10155
, Mar. 2017. [Online].
Available: https://arxiv
:
org/abs/1703
:
10155
[BDLB09]
J. Bergstra, G. Desjardins
et al.
, Quadratic p olynomials learn b etter im-
age features, Département d'Informatique et de Recherche Op érationnelle,
Université de Montréal, Tech. Rep. 1337, 2009.
[BGNR16]
B. Baker, O. Gupta
et al.
, Designing neural network architectures using
reinforcement learning,
arXiv preprint arXiv:1611.02167
, Nov. 2016. [Online].
Available: https://arxiv
:
org/abs/1611
:
02167
103
[BM93]
U. Bo denhausen and S. Manke,
Automatical ly Structured Neural
Networks For Handwritten Character And Word Recognition
. London:
Springer London, Sep. 1993, pp. 956961. [Online]. Available: http:
//dx
:
doi
:
org/10
:
1007/978- 1- 4471- 2063- 6_283
[BMDP10]
R. Behmo, P. Marcomb es
et al.
, Towards optimal naive Bayes nearest
neighb or, in
European Conference on Computer Vision (ECCV)
. Springer,
2010, pp. 171184.
[BPL10]
Y.-L. Boureau, J. Ponce, and Y. LeCun, A theoretical analysis of
feature p o oling in visual recognition, in
International Conference on
Machine Learning (ICML)
, no. 27, 2010, pp. 111118. [Online]. Available:
http://yann
:
lecun
:
com/exdb/publis/p df/b oureau- icml- 10
:
p df
[BSF94]
Y. Bengio, P. Simard, and P. Frasconi, Learning long-term dep endencies
with gradient descent is di˚cult,
IEEE transactions on neural networks
,
vol. 5, no. 2, pp. 157166, 1994.
[Cha92]
C. Charalamb ous, Conjugate gradient algorithm for e˚cient training
of arti˝cial neural networks,
IEEE Proceedings G-Circuits, Devices
and Systems
, vol. 139, no. 3, pp. 301310, 1992. [Online]. Available:
http://ieeexplore
:
ieee
:
org/do cument/143326/
[Cho15]
 F. Chollet, Keras, https://github
:
com/fchollet/keras, 2015.
[CLN10]
A. Coates, H. Lee, and A. Y. Ng, An analysis of single-layer networks
in unsup ervised feature learning,
Ann Arbor
, vol. 1001, no. 48109,
p. 2, 2010. [Online]. Available: http://cs
:
stanford
:
edu/~acoates/pap ers/
coatesleeng _aistats_2011
:
p df
[CLN11]
A. Coates, H. Lee, and A. Y. Ng,  STL-10 dataset, 2011. [Online]. Available:
http://cs
:
stanford
:
edu/~acoates/stl10
[CMS12]
D. Ciregan, U. Meier, and J. Schmidhub er, Multi-column deep neural
networks for image classi˝cation, in
Conference on Computer Vision and
Pattern Recognition (CVPR)
. IEEE, Feb. 2012, pp. 36423649. [Online].
Available: https://arxiv
:
org/abs/1202
:
2745v1
[CUH15]
D.-A. Clevert, T. Unterthiner, and S. Ho chreiter, Fast and accurate
deep network learning by exp onential linear units (ELUs),
arXiv
preprint arXiv:1511.07289
, Nov. 2015. [Online]. Available: https:
//arxiv
:
org/abs/1511
:
07289
[CWV
+
14]
S. Chetlur, C. Wo olley
et al.
,  cuDNN: E˚cient primitives for deep
learning,
arXiv preprint arXiv:1410.0759
, Oct. 2014. [Online]. Available:
https://arxiv
:
org/abs/1410
:
0759
104
[DBB
+
01]
C. Dugas, Y. Bengio
et al.
, Incorp orating second-order functional
knowledge for b etter option pricing, in
Advances in Neural Infor-
mation Processing Systems 13 (NIPS)
, T. K. Leen, T. G. Dietterich,
and V. Tresp, Eds. MIT Press, 2001, pp. 472478. [Online].
Available: http://pap ers
:
nips
:
cc/pap er/1920- incorp orating- second- order-
functional- knowledge- for- b etter- option- pricing
:
p df
[DDFK16]
S. Dieleman, J. De Fauw, and K. Kavukcuoglu, Exploiting cyclic symmetry
in convolutional neural networks,
arXiv preprint arXiv:1602.02660
, Feb.
2016. [Online]. Available: https://arxiv
:
org/abs/1602
:
02660
[DHS11]
J. Duchi, E. Hazan, and Y. Singer, Adaptive subgradient metho ds for
online learning and sto chastic optimization,
Journal of Machine Learning
Research
, vol. 12, no. Jul, pp. 21212159, 2011. [Online]. Available:
http://www
:
jmlr
:
org/pap ers/volume12/duchi11a/duchi11a
:
p df
[DHS16]
J. Dai, K. He, and J. Sun, Instance-aware semantic segmentation via
multi-task network cascades, in
Conference on Computer Vision and Pattern
Recognition (CVPR)
. IEEE, 2016, pp. 31503158. [Online]. Available:
https://arxiv
:
org/abs/1512
:
04412
[DJ99]
W. Duch and N. Jankowski, Survey of neural transfer functions,
Neural
Computing Surveys
, vol. 2, no. 1, pp. 163212, 1999. [Online]. Available:
ftp://ftp
:
icsi
:
b erkeley
:
edu/pub/ai/jagota/vol2_6
:
p df
[Doz15]
T. Dozat, Incorp orating Nesterov momentum into Adam, Stanford
University, Tech. Rep., 2015. [Online]. Available: http://cs229
:
stanford
:
edu/
pro j2015/054_rep ort
:
p df
[DSRB14]
A. Dosovitskiy, J. T. Springenb erg
et al.
, Discriminative unsup ervised
feature learning with convolutional neural networks, in
Advances in Neural
Information Processing Systems 27 (NIPS)
, Z. Ghahramani, M. Welling
et al.
, Eds. Curran Asso ciates, Inc., 2014, pp. 766774. [Online].
Available: http://pap ers
:
nips
:
cc/pap er/5548- discriminative- unsup ervised-
feature- learning- with- convolutional- neural- networks
:
p df
[DWD15]
S. Dieleman, K. W. Willett, and J. Dambre, Rotation-invariant convolutional
neural networks for galaxy morphology prediction,
Monthly notices of the
royal astronomical society
, vol. 450, no. 2, pp. 14411459, 2015.
[EDHS07]
J. Elson, J. J . Douceur
et al.
, Asirra: A CAPTCHA that
exploits interest-aligned manual image categorization, in
ACM Con-
ference on Computer and Communications Security (CCS)
, no. 14.
Asso ciation for Computing Machinery, Inc., Oct. 2007. [Online].
105
Available: https://www
:
microsoft
:
com/en- us/research/publication/asirra- a-
captcha- that- exploits- interest- aligned- manual- image- categorization/
[EKS
+
96]
M. Ester, H.-P. Kriegel
et al.
, A density-based algorithm for discovering
clusters in large spatial databases with noise. in
Kdd
, vol. 96, no. 34, 1996,
pp. 226231.
[ES03]
A. E. E ib en and J. E. Smith,
Introdu ction to evolutionary computing
.
Springer, 2003, vol. 53. [Online]. Available: https://dx
:
doi
:
org/10
:
1007/978- 3-
662- 44874- 8
[Fah88]
S. E. Fahlman, An empirical study of learning sp eed in back-propagation
networks, 1988. [Online]. Available: http://rep ository
:
cmu
:
edu/cgi/
viewcontent
:
cgi?article=2799&context=compsci
[FFFP06]
L. Fei-Fei, R. Fergus, and P. Perona, One-shot learning of ob ject
categories,
IEEE transactions on pattern analysis and machine intel ligence
,
vol. 28, no. 4, pp. 594611, Apr. 2006. [Online]. Available: http:
//vision
:
stanford
:
edu/do cuments/Fei- FeiFergusPerona2006
:
p df
[FFP03]
R. F. Fei-Fei and P. Perona, Caltech 101, 2003. [Online]. Available: http:
//www
:
vision
:
caltech
:
edu/Image_Datasets/Caltech101/Caltech101
:
html
[FGMR10]
P. F. Felzenszwalb, R. B. Girshick
et al.
, Ob ject detection with discrimina-
tively trained part-based mo dels,
IEEE transactions on pattern analysis and
machine intel ligence
, vol. 32, no. 9, pp. 16271645, 2010.
[FL89]
S. E. Fahlman and C. Lebiere, The cascade-correlation learning architecture,
1989. [Online]. Available: http://rep ository
:
cmu
:
edu/compsci/1938/
[GB10]
X. Glorot and Y. Bengio, Understanding the di˚culty of training deep
feedforward neural networks. in
Aistats
, vol. 9, 2010, pp. 249256. [Online].
Available: http://jmlr
:
org/pro ceedings/pap ers/v9/glorot10a/glorot10a
:
p df
[GBB11]
X. Glorot, A. Bordes, and Y. Bengio, Deep sparse recti˝er neural
networks. in
Aistats
, vol. 15, no. 106, 2011, p. 275. [Online]. Available:
http://www
:
jmlr
:
org/pro ceedings/pap ers/v15/glorot11a/glorot11a
:
p df
[GDDM14]
R. Girshick, J. Donahue
et al.
, Rich feature hierarchies for accurate ob ject
detection and semantic segmentation, in
Conference on Computer Vision
and Pattern Recognition (CVPR)
. IEEE, 2014, pp. 580587. [Online].
Available: https://arxiv
:
org/abs/1311
:
2524
[GG07]
P. P. Greg Gri˚n, Alex Holub, Caltech-256 ob ject category dataset, Apr.
2007. [Online]. Available: http://authors
:
library
:
caltech
:
edu/7694/
106
[GG16]
Y. Gal and Z. Ghahramani, Bayesian convolutional neural networks with
Bernoulli approximate variational inference,
arXiv preprint arXiv:1506.02158
,
Jan. 2016. [Online]. Available: https://arxiv
:
org/abs/1506
:
02158v6
[GJ02]
M. R. Garey and D . S. Johnson,
Computers and intractability
. wh freeman
New York, 2002, vol. 29.
[GJS76]
M. R. Garey, D. S. Johnson, and L. Sto ckmeyer, Some simpli˝ed NP-complete
graph problems,
Theoretical computer science
, vol. 1, no. 3, pp. 237267,
1976.
[Gol08]
P. Golle, Machine learning attacks against the Asirra CAPTCHA, in
ACM
conference on Computer and communications security (CCS)
, no. 15. ACM,
2008, pp. 535542.
[Gra15]
B. Graham, Fractional max-p o oling,
arXiv preprint arXiv:1412.6071
, May
2015. [Online]. Available: https://arxiv
:
org/abs/1412
:
6071
[Gri06]
A. P. Gri˚n, G. Holub, Caltech 256, 2006. [Online]. Available:
http://www
:
vision
:
caltech
:
edu/Image_Datasets/Caltech256/
[GWFM
+
13]
I. J. Go o dfellow, D. Warde-Farley
et al.
, Maxout networks.
ICML
,
vol. 28, no. 3, pp. 13191327, 2013. [Online]. Available: http:
//www
:
jmlr
:
org/pro ceedings/pap ers/v28/go o dfellow13
:
p df
[HAE16]
M. Huh, P. Agrawal, and A. A. Efros, What makes ImageNet go o d for
transfer learning?
arXiv preprint arXiv:1608.08614
, Aug. 2016. [Online].
Available: https://arxiv
:
org/abs/1608
:
08614
[Han89]
S. J . Hanson, Meiosis networks. in
NIPS
, 1989, pp. 533541. [Online].
Available: http://pap ers
:
nips
:
cc/pap er/227- meiosis- networks
:
p df
[Har15]
M. Harris, New features in CUDA 7.5, Jul. 2015. [Online]. Available:
https://devblogs
:
nvidia
:
com/parallelforall/new- features- cuda- 7- 5/
[HLW16]
G. Huang, Z. Liu, and K. Q. Weinb erger, Densely connected convolutional
networks,
arXiv preprint arXiv:1608.06993
, Aug. 2016. [Online]. Available:
https://arxiv
:
org/abs/1608
:
06993v1
[HM16]
M. Hardt and T. Ma, Identity matters in deep learning,
arXiv
preprint arX iv:1611.04231
, Nov. 2016. [Online]. Available: https:
//arxiv
:
org/abs/1611
:
04231
[How13]
A. G. Howard, Some improvements on deep convolutional neural network
based image classi˝cation,
arXiv preprint arXiv:1312.5402
, Dec. 2013.
[Online]. Available: https://arxiv
:
org/abs/1312
:
5402
107
[HPK11]
J. Han, J. Pei, and M. Kamb er,
Data mining: concepts and techniqu es
.
Elsevier, 2011.
[HPN
+
16]
S. Han, J. Po ol
et al.
,  DSD: Regularizing deep neural networks with
dense-sparse-dense training ˛ow,
arXiv preprint arXiv:1607.04381
, Jul. 2016.
[Online]. Available: https://arxiv
:
org/abs/1607
:
04381
[HPTD15]
S. Han, J. Po ol
et al.
, L earning b oth weights and connections for e˚cient
neural network, in
Advances in Neural Information Processing Systems 28
(NIPS)
, C. Cortes, N. D. Lawrence
et al.
, Eds. Curran Asso ciates, Inc., J un.
2015, pp. 11351143. [Online]. Available: http://pap ers
:
nips
:
cc/pap er/5784-
learning- b oth- weights- and- connections- for- e˚cient- neural- network
:
p df
[HSK
+
12]
G. E. Hinton, N. Srivastava
et al.
, Improving neural networks by preventing
co-adaptation of feature detectors,
arXiv preprint arXiv:1207.0580
, Jul.
2012. [Online]. Available: https://arxiv
:
org/abs/1207
:
0580
[HSL
+
16]
G. Huang, Y. Sun
et al.
, Deep networks with sto chastic depth,
arXiv preprint arXiv:1603.09382
, Mar. 2016. [Online]. Available: https:
//arxiv
:
org/abs/1603
:
09382
[HSW93]
B. Hassibi, D. G. Stork, and G. J. Wol˙, Optimal brain surgeon
and general network pruning, in
International Conference on Neural
Networks
. IEEE, 1993, pp. 293299. [Online]. Available: http:
//ee
:
caltech
:
edu/Babak/pubs/conferences/00298572
:
p df
[HVD15]
G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in a neural
network,
arXiv preprint arXiv:1503.02531
, Mar. 2015. [Online]. Available:
https://arxiv
:
org/abs/1503
:
02531
[HZRS14]
K. He, X. Zhang
et al.
, Spatial pyramid p o oling in deep convolutional
networks for visual recognition, in
European Conference on Computer
Vision ( ECCV)
. Springer, 2014, pp. 346361. [Online]. Available:
https://arxiv
:
org/abs/1406
:
4729
[HZRS15a]
K. He, X. Zhang
et al.
, D eep residual learning for image recognition,
arXiv preprint arXiv:1512.03385
, Dec. 2015. [Online]. Available: https:
//arxiv
:
org/abs/1512
:
03385v1
[HZRS15b]
K. He, X. Zhang
et al.
, Delving deep into recti˝ers: Surpassing human-level
p erformance on imagenet classi˝cation, in
International Conference on
Computer V ision (ICCV)
, Feb. 2015, pp. 10261034. [Online]. Available:
https://arxiv
:
org/abs/1502
:
01852
[Ima12]
Imagenet large scale visual recognition challenge 2012 (ILSVRC2012),
108
2012. [Online]. Available: http://www
:
image- net
:
org/challenges/LSVRC/
2012/nonpub- downloads
[IS15]
S. Io˙e and C. Szegedy, Batch normalization: Accelerating deep network
training by reducing internal covariate shift,
arXiv preprint arXiv:1502.03167
,
Feb. 2015. [Online]. Available: https://arxiv
:
org/abs/1502
:
03167
[JXF
+
16]
X. Jin, C. Xu
et al.
, Deep learning with s-shap ed recti˝ed linear activation
units, in
Thirtieth AAAI Conference on Arti˝cial Intel ligence
, Dec. 2016.
[Online]. Available: https://arxiv
:
org/abs/1512
:
07030
[Kar11]
A. Karpathy, Lessons learned from manually classifying CIFAR-10, Apr.
2011. [Online]. Available: http://karpathy
:
github
:
io/2011/04/27/manually-
classifying- cifar10/
[KB14]
D. Kingma and J. Ba, Adam: A metho d for sto chastic optimization,
arXiv preprint arXiv:1412.6980
, Dec. 2014. [Online]. Available: https:
//arxiv
:
org/abs/1412
:
6980
[KH09]
A. Krizhevsky and G. Hinton, Learning multiple layers of features from tiny
images, Apr. 2009. [Online]. Available: https://www
:
cs
:
toronto
:
edu/~kriz/
learning- features- 2009- TR
:
p df
[KMN
+
16]
N. S. Keskar, D. Mudigere
et al.
, On large-batch training for deep learning:
Generalization gap and sharp minima,
arXiv preprint arXiv:1609.04836
,
Sep. 2016. [Online]. Available: https://arxiv
:
org/abs/1609
:
04836
[Ko c15]
T. Ko cmánek,  Hyp erNEAT and novelty search for image recognition, Ph.D.
dissertation, Master's thesis, Czech Technical University in Prague, 2015.
[Online]. Available: http://ko cmi
:
tk/photos/DiplomaThesis
:
p df
[KPY
+
15]
Y.-D. Kim, E. Park
et al.
, Compression of deep convolutional neural networks
for fast and low p ower mobile applications,
arXiv preprint arXiv:1511.06530
,
Nov. 2015. [Online]. Available: https://arxiv
:
org/abs/1511
:
06530
[KR09]
L. Kaufman and P. J. Rousseeuw,
Finding groups in data: an introduction to
cluster analysis
. John Wiley & Sons, 2009, vol. 344.
[Kri]
A. Krizhevsky, The CIFAR-10 dataset. [Online]. Available: https:
//www
:
cs
:
toronto
:
edu/~kriz/cifar
:
html
[KS02]
V. Kurkova and M. Sanguineti, Comparison of worst case errors in linear
and neural network approximation,
IEEE Transactions on Information
Theory
, vol. 48, no. 1, pp. 264275, Jan. 2002. [Online]. Available:
http://ieeexplore
:
ieee
:
org/abstract/do cument/971754/
109
[KSH12]
A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classi˝cation
with deep convolutional neural networks, in
Advances in Neural
Information Processing Sys tems 25 (NIPS)
, F. Pereira, C. J. C. Burges
et al.
, Eds. Curran Asso ciates, Inc., 2012, pp. 10971105. [Online].
Available: http://pap ers
:
nips
:
cc/pap er/4824- imagenet- classi˝ cation- with-
deep- convolutional- neural- networks
:
p df
[KSlB
+
10]
K. Kavukcuoglu, P. Sermanet
et al.
, Learning convolutional feature
hierarchies for visual recognition, in
Advances in Neural Information
Processing Systems 23 (N IPS)
, J. D. La˙erty, C. K. I. Williams
et al.
, Eds. Curran Asso ciates, Inc., 2010, pp. 10901098. [Online].
Available: http://pap ers
:
nips
:
cc/pap er/4133- learning- convolutional- feature-
hierarchies- for- visual- recognition
:
p df
[LAE
+
16]
W. Liu, D . Anguelov
et al.
,  SSD: Single shot multib ox detector, in
European Conference on Computer Vision (ECCV)
. Springer, 2016, pp.
2137. [Online]. Available: https://arxiv
:
org/abs/1512
:
02325
[Las17]
Noise layers, Jan. 2017. [Online]. Available: http://lasagne
:
readthedo cs
:
io/
en/latest/mo dules/layers/noise
:
html#lasagne
:
layers
:
Drop outLayer
[LBBH98]
Y. LeCun, L. Bottou
et al.
, Gradient-based learning applied to do cument
recognition,
Proceedings of the IEEE
, vol. 86, no. 11, pp. 22782324, Nov.
1998. [Online]. Available: http://yann
:
lecun
:
com/exdb/publis/p df/lecun-
01a
:
p df
[LBH15]
Y. LeCun, Y. Bengio, and G. Hinton, Deep learning,
Nature
,
vol. 521, no. 7553, pp. 436444, May 2015. [Online]. Available:
http://www
:
nature
:
com/nature/journal/v521/n7553/abs/nature14539
:
html
[LBOM98]
Y. A. L eCun, L. Bottou
et al.
,
E˚cient BackProp
, ser. Lecture Notes in
Computer Science. Berlin, Heidelb erg: Springer Berlin Heidelb erg, 1998, vol.
1524, pp. 950. [Online]. Available: http://dx
:
doi
:
org/10
:
1007/3- 540- 49430- 8
[LDS
+
89]
Y. LeCun, J. S. Denker
et al.
, Optimal brain damage. in
NIPs
, vol. 2, 1989,
pp. 598605. [Online]. Available: http://yann
:
lecun
:
com/exdb/publis/p df/
lecun- 90b
:
p df
[Le13]
Q. V. Le, Building high-level features using large scale unsup ervised
learning, in
International conference on acoustics, speech and signal
processing
. IEE E, 2013, pp. 85958598. [Online]. Available: http:
//ieeexplore
:
ieee
:
org/stamp/stamp
:
jsp?arnumb er=6639343
[LG16]
A. Lavin and S. Gray, Fast algorithms for convolutional neural networks, in
110
Conference on Computer Vision and Pattern Recognition (CVPR)
. IEEE, Sep.
2016, pp. 40134021. [Online]. Available: https://arxiv
:
org/abs/1509
:
09308
[LGT16]
C.-Y. Lee, P. W. Gallagher, and Z. Tu, Generalizing p o oling functions in
convolutional neural networks: Mixed, gated, and tree, in
International
Conference on Arti˝cial Intel ligence and Statistics
, 2016. [Online]. Available:
https://arxiv
:
org/abs/1509
:
08985v2
[LH16]
I. Loshchilov and F. Hutter,  SGDR: sto chastic gradient descent
with warm restarts,
Learning
, Aug. 2016. [Online]. Available: https:
//arxiv
:
org/abs/1608
:
03983
[LJD
+
16]
L. Li, K. Jamieson
et al.
, Hyp erband: A novel bandit-based approach to
hyp erparameter optimization,
arXiv preprint arXiv:1603.06560
, Mar. 2016.
[Online]. Available: https://arxiv
:
org/abs/1603
:
06560
[LM16]
K. Li and J. Malik, Learning to optimize,
arXiv preprint arXiv:1606.01885
,
Jun. 2016. [Online]. Available: https://arxiv
:
org/abs/1606
:
01885
[LSD15]
J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for
semantic segmentation, in
Conference on Computer Vision and Pattern
Recognition (CVPR)
. IEEE, Mar. 2015, pp. 34313440. [Online]. Available:
https://arxiv
:
org/abs/1411
:
4038v2
[LX17]
A. Y. Lingxi Xie, Genetic CNN,
arXiv preprint arXiv:1703.01513
, Mar.
2017. [Online]. Available: https://arxiv
:
org/abs/1703
:
01513
[Ma j17]
S. Ma jumdar, Densenet, GitHub, Feb. 2017. [Online]. Available:
https://github
:
com/titu1994/DenseNet
[Mar08]
M. Marszaªek,  INRIA annotations for Graz-02 (IG02), Oct. 2008. [Online].
Available: http://lear
:
inrialp es
:
fr/p eople/marszalek/data/ig02/
[MDA15]
D. Maclaurin, D. Duvenaud, and R. Adams, Gradient-based hyp erparameter
optimization through reversible learning, in
International Conference on
Machine Learning (ICML)
, 2015, pp. 21132122.
[MH08]
L. v. d. Maaten and G. Hinton, Visualizing data using t-SNE,
Journal of
Machine Learning Research
, vol. 9, no. Nov, pp. 25792605, 2008.
[MHN13]
A. L. Maas, A. Y. Hannun, and A. Y. Ng, Recti˝er nonlinearities
improve neural network acoustic mo dels, in
Proc. ICML
, vol. 30,
no. 1, 2013. [Online]. Available: https://web
:
stanford
:
edu/~awni/pap ers/
relu_hybrid_icml2013_˝nal
:
p df
[MM15]
D. Mishkin and J. Matas, All you need is a go o d init,
arXiv
111
preprint arXiv:1511.06422
, Nov. 2015. [Online]. Available: https:
//arxiv
:
org/abs/1511
:
06422
[MP43]
W. S. McCullo ch and W. Pitts, A logical calculus of the ideas immanent in
nervous activity,
The bul letin of mathematical biophysics
, vol. 5, no. 4, pp.
115133, 1943.
[MRM15]
N. McLaughlin, J . M. D. Rincon, and P. Miller, Data-augmentation for
reducing dataset bias in p erson re-identi˝cation, in
International Conference
on Advanced Video and Signal Based Surveil lance ( AVSS)
, no. 12, Aug. 2015,
pp. 16. [Online]. Available: http://ieeexplore
:
ieee
:
org/abstract/do cument/
7301739/
[MS07]
M. Marszalek and C. Schmid, Accurate ob ject lo calization with
shap e masks, in
Conference on Computer Vision and Pattern
Recognition (CVPR)
. IEEE, 2007, pp. 18. [Online]. Available: http:
//ieeexplore
:
ieee
:
org/do cument/4270110/
[MSM16]
D. Mishkin, N. Sergievskiy, and J. Matas, Systematic evaluation of CNN
advances on the ImageNet,
arXiv preprint arX iv:1606.02228
, Jun. 2016.
[Online]. Available: https://arxiv
:
org/abs/1606
:
02228
[MV16]
A. Mahendran and A. Vedaldi, Visualizing deep convolutional neural
networks using natural pre-images,
International J ournal of Computer Vision
,
pp. 123, Apr. 2016. [Online]. Available: https://arxiv
:
org/abs/1512
:
02017
[NDRT13]
N. Natara jan, I. S. Dhillon
et al.
, Learning with noisy lab els, in
Advances
in Neu ral Information Processing Systems 26 (NIPS)
, C. J. C. Burges,
L. Bottou
et al.
, Eds. Curran Asso ciates, Inc., 2013, pp. 11961204. [Online].
Available: http://pap ers
:
nips
:
cc/pap er/5073- learning- with- noisy- lab els
:
p df
[Nes83]
Y. Nesterov, A metho d of solving a convex programming problem with
convergence rate o (1/k2), in
Soviet Mathematics Doklady
, vol. 27, no. 2,
1983, pp. 372376.
[new00]
The training p erformed by qnstrn, Aug. 2000. [Online]. Available:
http://www1
:
icsi
:
b erkeley
:
edu/Sp eech/faq/nn- train
:
html
[Ng16]
A. Ng, Nuts and b olts of building ai applications using deep learning, NIPS
Talk, Dec. 2016.
[NH92]
S. J . Nowlan and G. E. Hinton, Simplifying neural networks by soft
weight-sharing,
Neural computation
, vol. 4, no. 4, pp. 473493, 1992.
[Online]. Available: https://www
:
cs
:
toronto
:
edu/~hinton/absps/sunsp ots
:
p df
[NH02]
R. T. Ng and J. Han,  CLARANS: A metho d for clustering ob jects for spatial
112
data mining,
IEEE transactions on know ledge and data engineering
, vol. 14,
no. 5, pp. 10031016, 2002.
[NWC
+
11a]
Y. Netzer, T. Wang
et al.
, Reading digits in natural images with
unsup ervised feature learning, in
NIPS workshop on deep learning and
unsupervised feature learning
, vol. 2011, no. 2, 2011, p. 5. [Online]. Available:
http://u˛dl
:
stanford
:
edu/housenumb ers/nips2011_housenumb ers
:
p df
[NWC
+
11b]
Y. Netzer, T. Wang
et al.
, The street view house numb ers (SVHN) dataset,
2011. [Online]. Available: http://u˛dl
:
stanford
:
edu/housenumb ers/
[NYC16]
A. Nguyen, J. Yosinski, and J. Clune, Multifaceted feature visualization:
Uncovering the di˙erent typ es of features learned by each neuron in deep
neural networks,
arXiv preprint arXiv:1602.03616
, May 2016. [Online].
Available: https://arxiv
:
org/abs/1602
:
03616
[OHIL16]
J. Ortigosa-Hernández, I. Inza, and J. A. Lozano, Towards comp etitive
classi˝ers for unbalanced classi˝cation problems: A study on the p erformance
scores,
arXiv preprint arXiv:1608.08984
, Aug. 2016. [Online]. Available:
https://arxiv
:
org/abs/1608
:
08984
[PMW
+
15]
N. Pap ernot, P. McDaniel
et al.
, Distillation as a defense to adversarial
p erturbations against deep neural networks,
arXiv preprint arXiv:1511.04508
,
Nov. 2015. [Online]. Available: https://arxiv
:
org/abs/1511
:
04508
[Pre98]
L. Prechelt,
Early Stopping - But When?
Berlin, Heidelb erg: Springer
Berlin Heidelb erg, 1998, pp. 5569. [Online]. Available: http://dx
:
doi
:
org/
10
:
1007/3- 540- 49430- 8_3
[RDS
+
14]
O. Russakovsky, J. Deng
et al.
, Imagenet large scale visual recognition
challenge,
arXiv preprint arX iv:1409.0575
, vol. 115, no. 3, pp. 211252, Sep.
2014. [Online]. Available: https://arxiv
:
org/abs/1409
:
0575
[RFB15]
O. Ronneb erger, P. Fischer, and T. Brox, U-net: Convolutional networks
for biomedical image segmentation, in
International Conference on Medical
Image Computing and Computer-Assisted Intervention
. Springer, 2015, pp.
234241. [Online]. Available: https://arxiv
:
org/abs/1505
:
04597
[RLS10]
S. Risi, J. Lehman, and K. O. Stanley, Evolving the placement and den-
sity of neurons in the hyp erneat substrate, in
Conference on Genetic and
evolutionary computation
, no. 12. ACM, 2010, pp. 563570.
[RSG16]
M. T. Rib eiro, S. Singh, and C. Guestrin, "why should i trust you?":
Explaining the predictions of any classi˝er,
arXiv preprint arXiv:1602.04938
,
Feb. 2016. [Online]. Available: https://arxiv
:
org/abs/1602
:
04938
113
[Rud16]
S. Ruder, An overview of gradient descent optimization algorithms,
arXiv preprint arXiv:1609.04747
, Sep. 2016. [Online]. Available: https:
//arxiv
:
org/abs/1609
:
04747
[SCL12]
P. Sermanet, S. Chintala, and Y. LeCun, Convolutional neural networks
applied to house numb ers digit classi˝cation, in
International Conference
on Pattern Recognition (ICPR)
, no. 21. IEEE, Apr. 2012, pp. 32883291.
[Online]. Available: https://arxiv
:
org/abs/1204
:
3968
[SDG09]
K. O. Stanley, D. B. D'Ambrosio, and J. Gauci, A hyp ercub e-based enco ding
for evolving large-scale neural networks,
Arti˝cial life
, vol. 15, no. 2, pp. 185
212, 2009. [Online]. Available: http://ieeexplore
:
ieee
:
org/do cument/6792316/
[SEZ
+
13]
P. Sermanet, D. Eigen
et al.
, Overfeat: Integrated recognition, lo calization
and detection using convolutional networks,
arXiv preprint arXiv:1312.6229
,
Feb. 2013. [Online]. Available: https://arxiv
:
org/abs/1312
:
6229v4
[SHK
+
14]
N. Srivastava, G. E . Hinton
et al.
, Drop out: a simple way to
prevent neural networks from over˝tting.
Journal of Machine Learning
Research
, vol. 15, no. 1, pp. 19291958, 2014. [Online]. Available:
https://www
:
cs
:
toronto
:
edu/~hinton/absps/JMLRdrop out
:
p df
[SHY
+
13]
A. Senior, G. Heigold
et al.
, An empirical study of learning rates in deep
neural networks for sp eech recognition, in
International Conference on
Acoustics, Speech and Signal Processing
. IEEE, 2013, pp. 67246728. [Online].
Available: http://ieeexplore
:
ieee
:
org/do cument/6638963/?arnumb er=6638963
[SIV16]
C. Szegedy, S. Io˙e, and V. Vanhoucke, Inception-v4, inception-resnet and the
impact of residual connections on learning,
arXiv preprint arXiv:1602.07261
,
Feb. 2016. [Online]. Available: https://arxiv
:
org/abs/1602
:
07261
[SKP15]
F. Schro˙, D. Kalenichenko, and J. Philbin, Facenet: A uni˝ed emb edding
for face recognition and clustering, in
Conference on Computer Vision
and Pattern Recognition (CVPR)
. IEEE, Mar. 2015, pp. 815823. [Online].
Available: https://arxiv
:
org/abs/1503
:
03832
[SL11]
P. Sermanet and Y. LeCun, Tra˚c sign recognition with multi-scale
convolutional networks, in
International Joint Conference on Neural
Networks (IJCNN)
, Jul. 2011, pp. 28092813. [Online]. Available:
http://ieeexplore
:
ieee
:
org/do cument/6033589/
[SLJ
+
15]
C. Szegedy, W. Liu
et al.
, Going deep er with convolutions, in
Conference
on Computer Vision and Pattern Recognition (CVPR)
. IEEE, Sep. 2015, pp.
19. [Online]. Available: https://arxiv
:
org/abs/1409
:
4842
[SM02]
K. O. Stanley and R. Miikkulainen, Evolving neural networks through
114
augmenting top ologies,
Evolutionary computation
, vol. 10, no. 2, pp. 99127,
2002. [Online]. Available: http://www
:
mitpressjournals
:
org/doi/abs/10
:
1162/
106365602320169811
[SMG13]
A. M. Saxe, J. L. McClelland, and S. Ganguli, Exact solutions to
the nonlinear dynamics of learning in deep linear neural networks,
arXiv preprint arXiv:1312.6120
, Dec. 2013. [Online]. Available: https:
//arxiv
:
org/abs/1312
:
6120
[SMGS14]
R. K. Srivastava, J. Masci
et al.
, Understanding lo cally comp etitive
networks,
arXiv preprint arXiv:1410.1165
, Oct. 2014. [Online]. Available:
https://arxiv
:
org/abs/1410
:
1165
[SSSI]
J. Stallkamp, M. Schlipsing
et al.
, The german tra˚c sign recognition
b enchmark. [Online]. Available: http://b enchmark
:
ini
:
rub
:
de/?section=
gtsrb&subsection=news
[SSSI12]
J. Stallkamp, M. Schlipsing
et al.
, Man vs. computer: Benchmarking
machine learning algorithms for tra˚c sign recognition,
Neural Networks
,
no. 0, pp. , 2012. [Online]. Available: http://www
:
sciencedirect
:
com/science/
article/pii/S0893608012000457
[SV16]
S. Saxena and J. Verb eek, Convolutional neural fabrics,
arXiv preprint
arXiv:1606.02492
, 2016. [Online]. Available: https://arxiv
:
org/abs/1606
:
02492
[SVI
+
15]
C. Szegedy, V. Vanhoucke
et al.
, Rethinking the inception architecture
for computer vision,
arXiv preprint arXiv:1512.00567
, Dec. 2015. [Online].
Available: https://arxiv
:
org/abs/1512
:
00567v3
[SVZ13]
K. Simonyan, A. Vedaldi, and A. Zisserman, D eep inside convolutional
networks: Visualising image classi˝cation mo dels and saliency maps,
arXiv preprint arXiv:1312.6034
, Dec. 2013. [Online]. Available: https:
//arxiv
:
org/abs/1312
:
6034
[SZ14]
K. Simonyan and A. Zisserman, Very deep convolutional networks for
large-scale image recognition,
arXiv preprint arXiv:1409.1556
, Sep. 2014.
[Online]. Available: https://arxiv
:
org/abs/1409
:
1556
[SZS
+
13]
C. Szegedy, W. Zaremba
et al.
, Intriguing prop erties of neural
networks,
arXiv preprint arXiv:1312.6199
, Dec. 2013. [Online]. Available:
https://arxiv
:
org/abs/1312
:
6199v4
[TF-16a]
 MNIST for ML b eginners, D ec. 2016. [Online]. Available: https:
//www
:
tensor˛ow
:
org/tutorials/mnist/b eginners/
115
[tf-16b]
tf.nn.drop out, Dec. 2016. [Online]. Available: https://www
:
tensor˛ow
:
org/
api_do cs/python/nn/activation_functions_#drop out
[TH12]
T. Tieleman and G. Hinton, Lecture 6.5-rmsprop: Divide the gradient
by a running average of its recent magnitude,
COURSERA: Neural
Networks for Machine Learning
, vol. 4, no. 2, 2012. [Online]. Available:
http://www
:
cs
:
toronto
:
edu/~tijmen/csc321/slides/lecture_slides_lec6
:
p df
[Tho14a]
M. Thoma, On-line recognition of handwritten mathematical symb ols,
Karlsruhe, Germany, Nov. 2014. [Online]. Available: http://martin-
thoma
:
com/write- math
[Tho14b]
M. Thoma, The Twiddle algorithm, Sep. 2014. [Online]. Available:
https://martin- thoma
:
com/twiddle/
[Tho16]
M. Thoma, A survey of semantic segmentation,
arXiv preprint
arXiv:1602.06541
, Feb. 2016. [Online]. Available: https://arxiv
:
org/abs/
1602
:
06541
[Tho17a]
M. Thoma, The HASYv2 dataset,
arXiv preprint arXiv:1701.08380
, Jan.
2017. [Online]. Available: https://arxiv
:
org/abs/1701
:
08380
[Tho17b]
M. Thoma, Master thesis (blog p ost), Apr. 2017. [Online]. Available:
https://martin- thoma
:
com/msthesis
[VH13]
P. Verbancsics and J. Harguess, Generative neuro evolution for deep
learning,
arXiv preprint arXiv:1312.5355
, Dec. 2013. [Online]. Available:
https://arxiv
:
org/abs/1312
:
5355
[vLA87]
P. J. M. van Laarhoven and E. H. L. Aarts,
Simulated annealing
.
Dordrecht: Springer Netherlands, 1987, pp. 715. [Online]. Available:
http://dx
:
doi
:
org/10
:
1007/978- 94- 015- 7744- 1_2
[VTKP17]
E. Vorontsov, C. Trab elsi
et al.
, On orthogonality and learning recurrent
networks with long term dep endencies,
arXiv preprint arX iv:1702.00071
,
Jan. 2017. [Online]. Available: https://arxiv
:
org/abs/1702
:
00071
[WHH
+
89]
A. Waib el, T. Hanazawa
et al.
, Phoneme recognition using time-delay
neural networks,
IEEE transactions on acoustics, speech, and signal
processing
, vol. 37, no. 3, pp. 328339, Aug. 1989. [Online]. Available:
http://ieeexplore
:
ieee
:
org/do cument/21701/
[Wil92]
R. J. Williams, Simple statistical gradient-following algorithms for connec-
tionist reinforcement learning,
Machine learning
, vol. 8, no. 3-4, pp. 229256,
1992.
116
[WWQ13]
X. Wang, L . Wang, and Y. Qiao,
A Comparative Study of Encoding, Pooling
and Normalization Methods for Action Recognition
. Berlin, Heidelb erg:
Springer Berlin Heidelb erg, Nov. 2013, no. 11, pp. 572585. [Online].
Available: http://dx
:
doi
:
org/10
:
1007/978- 3- 642- 37431- 9_44
[WYS
+
15]
R. Wu, S. Yan
et al.
, Deep image: Scaling up image recognition,
arXiv
preprint arXiv:1501.02876
, vol. 7, no. 8, Jul. 2015. [Online]. Available:
https://arxiv
:
org/abs/1501
:
02876v4
[WZZ
+
13]
L. Wan, M. Zeiler
et al.
, Regularization of neural networks using drop connect,
in
International Conference on Mac hine Learning (ICML)
, no. 30, 2013,
pp. 10581066. [Online]. Available: http://www
:
matthewzeiler
:
com/pubs/
icml2013/icml2013
:
p df
[XGD
+
16]
S. Xie, R. Girshick
et al.
, Aggregated residual transformations for deep
neural networks,
arXiv preprint arXiv:1611.05431
, Nov. 2016. [Online].
Available: https://arxiv
:
org/abs/1611
:
05431v1
[Xu11]
W. Xu, Towards optimal one pass large scale learning with averaged
sto chastic gradient descent,
arXiv preprint arX iv:1107.2490
, Jul. 2011.
[Online]. Available: https://arxiv
:
org/abs/1107
:
2490
[XWCL15]
B. Xu, N. Wang
et al.
, Empirical evaluation of recti˝ed activations in
convolutional network,
arXiv preprint arXiv:1505.00853
, May 2015. [Online].
Available: https://arxiv
:
org/abs/1505
:
00853
[XXE12]
H. Xiao, H. Xiao, and C. Eckert, Adversarial lab el ˛ips attack on
supp ort vector machines. in
ECAI
, 2012, pp. 870875. [Online]. Available:
https://www
:
sec
:
in
:
tum
:
de/assets/Uploads/ecai2
:
p df
[XZY
+
14]
T. Xiao, J. Zhang
et al.
, Error-driven incremental learning in deep convolu-
tional neural network for large-scale image classi˝cation, in
International
Conference on Multimedia
, no. 22. ACM , 2014, pp. 177186.
[YL98]
C. J. B. Yann LeCun, Corinna Cortes, The MNIST database of handwritten
digits, 1998. [Online]. Available: http://yann
:
lecun
:
com/exdb/mnist/
[ZBH
+
16]
C. Zhang, S. Bengio
et al.
, Understanding deep learning requires rethinking
generalization,
arXiv preprint arXiv:1611.03530
, Nov. 2016. [Online].
Available: https://arxiv
:
org/abs/1611
:
03530
[ZCZL16]
S. Zhai, Y. Cheng
et al.
, Doubly convolutional neural networks, in
Advances in Neu ral Information Processing Systems 29 (NIPS)
, D. D. Lee,
M. Sugiyama
et al.
, Eds. Curran Asso ciates, Inc., Oct. 2016, pp. 10821090.
[Online]. Available: http://pap ers
:
nips
:
cc/pap er/6340- doubly- convolutional-
neural- networks
:
p df
117
[ZDGD14]
N. Zhang, J. Donahue
et al.
, Part-based R-CNNs for ˝ne-grained category
detection, in
European Conference on Computer Vision (ECCV)
. Springer,
Jul. 2014, pp. 834849. [Online]. Available: https://arxiv
:
org/abs/1407
:
3867
[Zei12]
M. D . Zeiler, Adadelta: an adaptive learning rate metho d,
arXiv preprint
arXiv:1212.5701
, Dec. 2012. [Online]. Available: https://arxiv
:
org/abs/
1212
:
5701v1
[ZF13]
M. D. Zeiler and R. Fergus, Sto chastic p o oling for regularization of deep
convolutional neural networks,
arXiv preprint arXiv:1301.3557
, Jan. 2013.
[Online]. Available: https://arxiv
:
org/abs/1301
:
3557v1
[ZF14]
M. D. Zeiler and R. Fergus, Visualizing and understanding convolutional
networks, in
European Conference on Computer Vision (ECCV)
. Springer,
Nov. 2014, pp. 818833. [Online]. Available: https://arxiv
:
org/abs/1311
:
2901
[Zho16]
B. Zhou, Places2 download, 2016. [Online]. Available: http://
places2
:
csail
:
mit
:
edu/download
:
html
[ZK16]
S. Zagoruyko and N. Komo dakis, Wide residual networks,
arXiv
preprint arXiv:1605.07146
, May 2016. [Online]. Available: https:
//arxiv
:
org/abs/1605
:
07146
[ZKL
+
15]
B. Zhou, A. Khosla
et al.
, Learning deep features for discriminative
lo calization,
arXiv preprint arXiv:1512.04150
, Dec. 2015. [Online]. Available:
https://arxiv
:
org/abs/1512
:
04150
[ZKL
+
16]
B. Zhou, A. Khosla
et al.
, Places: An image database for deep scene
understanding,
arXiv preprint arXiv:1610.02055
, Oct. 2016. [Online].
Available: https://arxiv
:
org/abs/1610
:
02055
[ZL16]
B. Zoph and Q. V. Le, Neural architecture search with reinforcement
learning,
arXiv preprint arXiv:1611.01578
, Nov. 2016. [Online]. Available:
https://arxiv
:
org/abs/1611
:
01578
[ZMGL15]
J. Zhao, M. Mathieu
et al.
, Stacked what-where auto-enco ders,
arXiv preprint arXiv:1506.02351
, Jun. 2015. [Online]. Available: https:
//arxiv
:
org/abs/1506
:
02351v1
[ZYL
+
15]
H. Zheng, Z. Yang
et al.
, Improving deep neural networks using softplus
units, in
International Joint Conference on Neural Networks (IJCNN)
, Jul.
2015, pp. 14.
118
I. Glossar y
ANN
arti˝cial neural network. 4
ASO
Automatic Structure Optimization. 29
CMO
Confusion Matrix Ordering. 2, 35, 36, 51, 52, 71
CNN
Convolutional Neural Network. 1, 36, 11, 13, 15, 2123, 28, 29, 31, 33, 37, 54, 60,
71, 72, 79, 8284, 8891
ELU
Exp onential Linear Unit. 38, 57, 6064, 72, 73, 77, 78, 84
ES
early stopping. 68
FC
Fully Connected. 91, 93
FLOP
˛oating p oint op eration. 27, 29, 87, 88, 90, 91, 93
GA
genetic algorithm. 30
GAN
Generative Adverserial Network. 80
GPU
graphics pro cessing unit. 37, 40, 59, 63, 67, 88, 91
HSV
hue, saturation, value. 79
LCN
Lo cal Contrast Normalization. 91
LDA
linear discriminant analysis. 79
LReLU
leaky recti˝ed linear unit. 63, 72, 77, 78, 84
MLP
multilayer p erceptron. 36, 28
NAG
Nesterov Accellerated Momentum. 83
NEAT
NeuroEvolution of Augmenting Top ologies. 83
OBD
Optimal Brain Damage. 29
119
PCA
principal comp onent analysis. 79
PReLU
parametrized recti˝ed linear unit. 60, 61, 63, 64, 72, 77, 78, 84
ReLU
recti˝ed linear unit. 5, 13, 60, 61, 63, 64, 72, 77, 78, 84
SGD
sto chastic gradient descent. 5, 30, 45, 46, 82
ZCA
Zero Comp onents Analysis. 79
120
